[SOUND] Hi welcome to the course, Cluster Analysis in Data Mining. Before starting this course, I'm going to give you a general course overview. First, what is cluster analysis? Actually when you fly over a city, you can easily identify fields, forests, commercial area, and residential areas based on their features, without anybody's explicit training or labeling. This is the power of cluster analysis. In this course, I'm going to systematically introduce you the concepts and methods of cluster analysis. And help answering the following question. What are the different proximity measures for effective clustering? Can we cluster a massive number of data points efficiently? Can we find clusters of arbitrary shape and at multilevels of granularity? How can we judge the quality of the clusters discovered by our system? Cluster analysis bring a lot of value to data mining. What is the value of cluster analysis? Cluster analysis may help you partition massive data into groups based on its features. Cluster analysis also will help subsequent data mining processes, such a pattern discovery, classification and outlier analysis. What roles does cluster analysis play in the data mining specialization? You will learn various scalable methods to find clusters from massive data. You will learn how to mine different kinds of clusters effectively. You'll also learn how to evaluate the quality of the clusters you'll find. And you will see cluster analysis will help with classification, outlier analysis and other data mining tasks. Cluster analysis has its broad applications. For example, for data summarization, compression, and reduction, like image processing or vector quantization, you will need cluster analysis. For collaborative filtering, recommendation systems, or customer segmentation, you will find like-minded users or similar products by cluster analysis. For dynamic trend detection, you will find clustering stream data and detecting trends and patterns will be very effective. For multimedia analysis, biological data analysis, and social network analysis you'll find you may have effecting measures to group audio/video clips and finding clusters of gene protein sequences. Cluster analysis is a key intermediate step for many other data mining tasks. For example, you want to generate a compact summary of data for classification, pattern discovery, hypothesis generation and testing your neat cluster masses. You wana find outlier detection. Actually, outliers are those far away from any cluster. In this course, the major reference of the readings is my own textbook published in 2011, which is, Data Mining: Concepts and Techniques, the 3rd edition, published by Morgan Kaufmann. In this book, we will only used two chapters. The first is Chapter 2. [INAUDIBLE] just cover section 2.4: Measuring Data Similarity and dissimilarity. And then Chapter 4 is a major source related to this textbook. The textbook of Chapter 4 is a major source related to this seminar, this lecture, called Cluster Analysis: Basic Concepts and Methods. Other references will be listed at the end of each lecture video. For the course structure, we will have six lessons. Lesson 1 is, Cluster Analysis: An Introduction. Lesson 2 is, Similarity Measures for Cluster Analysis. These two lessons from the first module, Module 1. Then, Lesson 3: Partitioning-Based Clustering Methods. An, Lesson 4(Part I): Hierarchical Clustering Methods (I), from the contents of Module 2. Then the Part II of Lesson 2, Hierarchical Clustering Methods (II). And, Lesson 5: Density-Based and Grid-Based Clustering Methods, form the contents of Module 3. Finally, Lesson 6: Clustering Validation, forms the material for Module 4. For general information about this course, I'm Jiawei Han. I'm a professor in the Department of Computer Science, University of Illinois at Urbana-Champaign. I'm the instructor. We have some teaching assistants on the web to help you. The course prerequisites. Just as long as you are familiar with basic data structure and algorithms, you like to work hard, you're quite okay. For course assessments, we have in-video questions to help you understand the course materials. Then we need you to pass some minimum requirements for lesson quizzes and two programming assignments. Thank you. Hope you will enjoy this course. Thank you. [MUSIC]
The first session is on what is Cluster Analysis? To understand what is cluster analysis, we should know first what is a cluster? A cluster is actually a collection of data objects, those objects are similar within the same cluster. That means the objects are similar to one another within the same group. And they are rather different, or they are dissimilar, or unrelated, to the objects in other groups or in other clusters. Okay, then cluster analysis which is also called clustering or data segmentation, the essential is getting a set of tape data points. The cluster analysis is to partition them into a set of clusters, or set of groups. They are as similar as possible within the same group and as far apart as possible among different groups. Cluster analysis is unsupervised learning, in the sense there's no predefined classes. This is very different from classification which needs supervised learning or needs to give in the class labors then you can construct the classification models. There are many ways to use our apply cluster analysis, essentially cluster analysis can either provide as a stand alone tool to get insight into your data distribution like a summary. Or you can serve, you can use it to serve as pre-processing step or intermediate step for other algorithms, like a classification or a prediction or like many other tasks, including data mining and other applications. Thank you. [MUSIC]
[SOUND] Hi, in this session, we're going to discuss applications of cluster analysis. Cluster analysis has lots of applications. For example, it has been properly used as pre-processing step or intermediate step for other data mining tasks. For example, you can generate complex summary of data for classification, pattern discovery, hypothesis generation and testing and many others. And it also has been popularly used for outlier detection, because outliers can be considered those points that are far away from any cluster. Cluster analysis also has been used for data summarization, compression and reduction. For example, in im, image processing, vector quantization has been using cluster analysis quite a lot. Cluster analysis also can be used for collaborative filtering, recommendation systems or customer segmentation, because clusters can be used to find like-minded users or similar products. Cluster analysis also has been used for trend detection, for dynamic data. For example, we can cluster stream data or detecting trends and patterns in dynamic data strings. And cluster analysis also has been used for multimedia data analysis, biological data analysis and social network analysis. For example, we can use cluster clustering methods to cluster images or videos or audio clips or we can use cluster analysis on genes and protein sequences and many other interesting tasks. Thank you. [MUSIC]
[MUSIC] In this session we're going to study the requirements and challenges of cluster analysis. There are many things to considered for cluster analysis. For example, the first one people are often to consider is partitioning criteria. That means whether we want to get a single level or multiple-level hierarchical partitioning. In many cases, multi-level hierarchical partitioning or what we call hierarchical clustering could be quite desirable. The second thing we want to consider whether how the clusters can separate the objects. It can be hard or exclusive, or soft, non-exclusive. The exclusive means one object, like one customer, can belong to only one region, okay. Non-exclusive, for example, one document may belong to more than one cluster. Then for a similarity measure, there could be many different kinds of similarity measures. For example, distance-based using Euclidean space or a road network or vector space. Or we can use connectivity-based, for example, based on density or contiguity. The fourth one that we are going to discuss is clustering space. That means, whether we want to do full space clustering, or we want to do subspace clustering. Full space means especially in the low dimensional, for example in 2D, two dimensional space, in the area or in the region, whether we want to find the clusters in those region, in the full 2D space. Or only we want to project on the 1D subspace. However in many high dimensional clustering, it is hard to find meaningful clusters in a very high damage space. But it is possible to find interesting clusters in the subspaces. For example, in the corresponding lower dimensional space. So the subspace clustering also becomes very important. Then, for cluster analysis, there are many requirements and challenges. The first, most important thing is the quality of clustering. Means whether we are going to deal with different kinds of attributes, numerical one, categorical data, text, multimedia data, networks, or a mixture of multiple types. Whether we can discover clusters with arbitrary shape, like an oil spill, or whether we will be able to deal with noisy data. Then the second important issue people consider is scalability. That means whether we will be able in a very big amount of data environment, whether we can cluster all the data or we must draw some sample and cluster only on the sample rate. Whether we can handle high dimensional data, or we can only project them in the low dimension space. And whether we can do timely incremental clustering, especially for data streams. Whether we can do stream clustering. Especially whether the cluster results may not be sensitive to the input order of the data. Another important issue we need to handle is constraint-based clustering. That means users usually have their own thinking on the preference or constraints of the cluster they want to derive. They may have domain knowledge. They may know certain subspaces could be more interesting. They may even raise queries. So can we handle clustering given user preference or constraints? The final issue we are going to discuss is interpretability and usability. Simply says the clustering whether they can derive meaningful clusters which can be understood by people, can be used by many applications, can interpret data nicely. All these are a very important issue we are going to study in this lecture. [MUSIC]
[SOUND] In this session, we're going to provide a multi-dimensional categorization of cluster analysis. Cluster analysis can be viewed in many different way. We will provide a multi-dimensional categorization on all the different methods. The first categorization is based on techniques, based on different techniques employed in cluster analysis. We can consider these are density-based methods or they are distance-based methods or grid-based methods or probabilistic generative model based methods. Or we are essentially using dimensionality reduction measures to reduce the dimensions before we do effective clustering or we can directly perform high dimensional clustering or whether the measures are scalable for large data analysis. And the second categorization is based on data type-centered. So that means different kinds of data types likely were used, rather different clustering methods. For example, how to cluster numerical data, category data, text data, multimedia data, time-series data, sequence data, stream data, network data or uncertain data. The third categorization is whether we can provide additional insights for clustering. For example, we can provide a visual insights or we can incorporate some kind of semi-slightly supervision or we can use ensemble-based measures or we can use validation-based measures. All these, we're going to get into more detail in the next sessions. Thank you. [MUSIC]
[SOUND] Hi, in this session we are going to provide a brief overview of typical clustering methodologies. There are many clustering methods. They can be roughly categorized into a few clustering methodologies. The first method's called distance-based methods. Essentially there are two kinds of methods. One called partitioning algorithms, the other we call hierarchical algorithms. Partitioning algorithms essentially is, partitioning the data in a high dimension space, into multiple clusters. The typical methods include K-Means, K-Medoids, K-Medians, those methods we are going to introduce. For hierarchical algorithms, essentially we can either do an agglomerative, or we call bottom-up, merging many points into clusters, merging small clusters into bigger ones, then from hierarchical clusters. Or we can use divisive methods. Start with a single big, all the data is encased in one cluster, then we try to split them, divide them using the top-down splitting, into smaller and smaller clusters. And then the second methodology, called density-based methods. The third one called grid-based methods. Of course they have some linkages as well. Density-based method essentially is, we can think the database space, made at a high-level granularity, may you think about it with certain grade, or certain structures, or certain k nearest neighbors, we may find certain density. For the dense small regions, we can merge them into bigger regions. In this way, we will be able to find clusters, those events regions, with arbitrary shape. The grid-based method, essentially is partitioning the data space into grid-like structures. Each grid is a summary of the characteristics of the data, in the lower grid or lower cells. Then the third method is called probabilistic and generative models, that means we can model data from a generative process. We can assume underneath, there are certain distributions, for example, mixture of Gaussians, okay? Then with the data, we can think the current points are generated from these underlying generative model, or underlying generative mechanisms. Then we can model parameters, using a Expectation-Maximization algorithm we are going to introduce. Based on the available data sets, we may try to find the maximum likelihood fit. And based on this fit, we will be able to estimate the generative probability of the underlying data points. And based on this, we may find the models. Then, in many cases the data may be sitting in a high-dimensional space. We may need to study high-dimensional clustering methods. One influential method called subspace clustering, essentially you try to find the clusters on various subspaces. So, that method can be categorized into bottom-up methods, top-down high-dimensional clustering method, or correlation-based methods. We will also introduce some interesting method, a delta clustering. Then, for high-dimensional clustering, there are many methods developed for dimensionality reduction. Essentially is we can think the height dimension is in a vertical form there, containing lots of columns. And for those columns, when we perform clustering, essentially the columns are clustered, the rows or columns can be clustered together, we call co-clustering. There are several typical methods. One is probabilistic latent semantic indexing. Later, people develop another method called Latent Dirichlet Allocation. So PLSI or LDA, are typical topic modeling methods for text data. Essentially, we can think the text can be clustered into multiple topics. Each topic is a cluster, and each topic is associated with a set of words, or we can think of they are dimensions. And also a set of documents, you can think they are rows, simultaneously. And the second popular study method's called nonnegative matrix factorization, NMF. This is a kind of co-clustering method you can think as, you'll get a nonnegative matrix because the word, a word frequencies in documents are nonnegative, okay? They are zero or more, but they are nonnegative values in the matrix. For this nonnegative matrix, we can approximately factorize it into two nonnegative lower ranked matrices, type U and V, that will reduce the number of dimensions. Another very interesting method we're going to study is called spectral clustering. That means we use a spectrum of the similarity matrix of the data, to perform dimensionality reduction. The higher dimension reduced into the lower, fewer dimensions, then we can perform clustering in fewer dimensions. That's spectral clustering methods. In this course, we are going to study many low-dimension and high-dimension clustering methods, and study their different variations, and applications as well. [MUSIC]
[SOUND] Hi, in this session we are going to give a brief overview on clustering different types of data. We encounter various kinds of types of data. For example, the early clustering algorithm most times with the design was on numerical data. And the second type of data is category data, including the binary that most people consider as also can be handled in categorical data or category. For example, gender, race, zip code, or market-basket data, you would consider the data is discrete. There's no natural order, but certain kind of data is text data, this is very popular in social media, on the Web, or social networks. Usually if we consider a word as one dimension, then we are handling very high dimensional data, but they are very sparse. Their value usually corresponding to word frequencies. The methods to handle such data, include a combination of k-means and agglomarative method, topical modelling methods, or co-clustering methods. We are going to discuss this extensive later. Multimedia data is another kind of data needs clustering. Usually images, audio, video, those are Flickr and YouTube actually are multimedia data. They are multi-modal in a sense. They have image, audio, video, many cases they have text, as captions as well. We can consider data are contextual data, they contain both behavioral and the contextual attributes. For example for images, we can consider the position of a pixel represents its context, the value represents its behavior. For video and music data, we can consider temporal ordering of the records represents its meaning. Time-series data is another popularly encountered data like in sensors, stock markets, temporal tracking, or forecasting, those task are usually handled time-series. Time-series, we usually consider data are temporally dependent. They are consecutive, usually the interval is somehow equal. Like, we can consider time as contextual attributes, data value as behavioral attribute. Usually such analysis include correlation-based online analysis, like online clustering of stocks to find stock tickers. Or we use shape-based offline analysis, for example, we can cluster ECG based on overall shapes. Sequence data is another kind of data. Usually in weblog analysis, or biological sequence analysis, or analyze the system commands. These, we can think the placement rather than time, the contextual attribute. That means where they are. Then the similarity function, include Hamming distance, edit distance, longest common subsequences. The clustering method, we call sequence clustering may use suffix tree, may use generative model, like Hidden Markov Model. Stream data is another kind of data. Stream data can be considered as a real-time data, like water flowing in and out. It may evolve a long time. It may have concept shifts because stream coming and go. So, we need a single pass algorithm, rather you can see it again and again. And the typical method for stream clustering could be micro-clustering, that means we need to create efficient intermediate representation by some kind of a micro-clustering method. Graphs and homogeneous networks is another kind of data. This then kind of data usually, so-called homogeneous networks that means we consider networks as graphs, the nodes and edges actually are of one kind, one type. Actually any kind of data can be modeled as a graph with the nodes as different attributes and entries, and edge graph in their similarity values. The typical method for handling graph clustering could be generative models, combinatorial algorithm like graph cuts, spectral clustering method, non-negative matrix factorization methods. Then a little bit beyond homogeneous network are heterogeneous networks. This kind of network consists of multiple types of nodes and edges. Like a bibliographical data, hospital, handling disease, patients, doctors, and treatments to cluster different kinds of nodes and links together. There are some interesting algorithms, like the NetClus algorithm. Uncertain data, means the data may contain noise, may have approximate values, or multiple possible values. Usually we need to incorporate some probabilistic information, like distribution, or approximate values that were improve the quality of clustering. Finally, recently, there are lots of discussion on big data. That means, we can model systems. That may store and process very big data, like weblog analysis. Usually, like Google's MapReduce framework is you have Map function to distribute the computation across many machines. Then we have Reduce function to aggregate the results obtained from the Map step. So we can see, the data is very rich that's why the clustering is a rather sophisticated methodologies, trying to handle different kinds of data. [MUSIC]
[SOUND] Hi, in this session, I'm going to give you a brief overview of user insights and clustering. Usually, users may have some interesting insights or views. User may like to interact with the clustering process, to influence the final results of clustering. One interesting one is visual insights, because a picture is worth a thousand words. Human eyes are actually powerful, high-speed processors. Because they are also linked with rich knowledge base. So, a human can provide a lot of intuitive insights whether this clustering result is good or something they desired is not achieved based on the current clustering result. Okay, there's usually user's good to one or two dimensional, at most 3D, they may be able to find clusters. Their algorithms, for example, High D eyes, how to visualize high dimensional clusters. The second one is semi-supervised insights, that means user may have certain insights or intention on the clustering process. They may like to pass their insights to the system to influence the results of the clustering. For example, user may provide some seeding. That means you can provide a number of labeled examples or some approximate representation of the categories of interest. Then you may like the clustering process to take your insight to do some desired clustering. The third one is multi-view and ensemble-based insights. So-called multi-view clustering is different clustering may generate different perspectives. The multiple clustering, you can ensemble them together to provide more robust solution. Finally, a very important one is validation-based insight. That means, you may want to evaluate the quality of the clusters generated based on certain validation methods. For example, you may use case studies or you may provide specific measures, or you may provide some preexisting labels. All of these, we're going to discuss in detail in some chapter. Finally, I provide a few reference books. And especially the chapter By Charu Aggarwal. An Introduction to Clustering Analysis is a very nice introduction on general ideas of clustering analysis. Thank you. [MUSIC]
[MUSIC] Basic concepts means how to measure similarity between objects. Since clustering is an unsupervised learning process, so we do not have predefined levels to judge whether it's right or wrong. So how can we judge a clustering process is good? That means whether it will produce high-quality clusters. Usually, we keep two measures, one called high intra-class similarity, the other called low inter-class similarity. What is high intra-class similarity? That means you want to see the objects within the same cluster, they are rather similar, they are cohesive. You also want to see inter-class similarity is low in the sense you want to see the objects in different clusters. They are rather dissimilar, or they are distinctive. So to measure this, we need some separate quality function to measure how good a cluster it is. However, it is hard to define what is similar enough or good enough and the answer usually could be quite subjective. That's the reason we want to look at different similarity measures or the similarity functions for different applications, but they are critical for cluster analysis. For terminologies, people actually want to distinguish a quite interesting similar things called similarity measure, dissimilarity measure, or proximity measure. What is similarity measure or similarity function? Usually, similarity function is a real value function to quantify the similarity between two objects. Usually the higher value, it means more similar. So the value range is 0 to 1. 0 means there's no similarity. 1 means they are completely similar, or they are identical. Then similar to similarity, many people want to use dissimilarity, or distance measure. Distance measure or distance function is a new micro measure to measure how different these two objects are. To some extent, dissimilarities, inverse of similarities, that means the lower or the shorter the distance, the more alike. The minimum dissimilarity is 0, means there are completely similar or identical. The value range could be either 0 to 1 or 0 to infinity based on the definition. Another term called proximity, this is usually referred to either similarity or dissimilarity. [MUSIC]
[SOUND] Now we examine Session 2: Distance on Numerical Data: Minkowski Distance. So we first introduced data matrix and dissimilarity matrix, or distance matrix. Data matrix is referenced in the typical matrix form is we have n data points, we use n rows. We have l dimensions, we use l columns to reference this data set. The distance matrix or dissimilar matrix usually is referenced in triangular matrix because you have n data points, we register only the distance between like objects one versus one or two versus one to look at their distance. But the distance function usually could be quite different, but different kinds of variables, like a real, boolean, categorical, ordinal, ratio, and vector variables. Their distance definition could be different. Moreover, sometimes they may have weights associated with those different variables based on the applications and data semantics. We are going to introduce this more later. Then, we look at an example. Suppose we have four points, four objects in two dimensional space. Then the data matrix is rampant in this typical form, okay. Then for dissimilarity matrix, or distance matrix, for Euclidean distance, you can see the matrix is in this way, like x1, x1, they are identical. Their distance is 0. x2, x1, their computation is based on the distance. This part is two, this distance is three, you take the sum of the square area. You take square root, you get this value. Then in general, we define the Minkowski distance of this formula. It means if we have area dimensions for object i and object j. Then their distance is defined by taking every dimension to look at their absolute value of their distance, then to the power of p, then you sum them up, get the root of p. Then we get the Minkowski distance. Such distance that P is the order we usually also call this distance this LP norm. The Minkowski distance in general have these properties. The first property is called positivity. It means, the distance be equal zero when they are identical otherwise they are greater in there. The second property called symmetry means the distance between I and J, distance between J and I should be identical. Then the third one called triangular inequality means for the distance between i and j. If you go through k, that means you go first to k, then from k to j. That distance should be no less than directly go from i to j. Should be greater than or equal to d(i,j). The distance measure that satisfies these three properties are metric distance. Notice not all the distance are metric for example set difference is not metric. Of course in this lecture we mainly discuss metric distance. Then we look at some special cases of Minkowski distance. If p = 1, we call L1 norm, they also call Manhattan or city block distance define this formula. In particular, if we are dealing with binary vectors we call these Hamming distance is the number of bits that are different. Then if p equals two, you can find also L2 norm. As Euclidean distance defined the typical way like this, and this vary for the middle one I think for all people, okay? Then if p goes to infinity means we're dealing with L infinity norm or L max norm, this is also caused supernum distance. Is that you find, is limit for p goes for infinity. In that case, actually the distance is really the maximum difference between any attribute of the vectors. For example you can see for F, from 1 to L. The maximum such absolute value of the distance, is the distance of L infinity norm or supremum distance. Let's look at some examples, for the same data sets, we get a four points. We get two dimensions. Then we look at the Manhattan distance is just a city block distance. For example from x2 to x1 you will go three blocks down then two blocks left. So the Manhattan distance is 3 plus 2, we get 5, okay. Of course x1 to x1 itself, is 0. And you Euclidean distance, as we discussed already, that's the measure. For Supremum distance, L infinity norm. You probably can see x2 to x1. So probably you can see the difference is you first go down 3 blocks for Manhattan distance, you can see you get a 3 plus 2. But for Supremum distance you could just see which attribute would give the maximum distance or maximum difference so you probably can see the maximum amount is 3 instead of 2 that's why x2 to x1 is 3. So it's pretty easy to compute, thank you. [MUSIC]
[MUSIC] In this session, we are going to discus proximity measure for symmetric vs asymmetric binary variables. For binary variables, we usually report their occurrences using contingency tables. Okay, suppose we have two objects, i and j. The number of times they both appear could be q, they both missing could be t cases. I appears and j does not, there are r cases. And i does not appear, and j appears s times, then for symmetric binary variables that means the chance they appear or they not appear actually have equivalent chances, or approximately same chances w call this as symmetric binary variables. In our case they are distance, like r and s. These two cases, they are different, so their distance measure is r+s divided by all the cases. For asymmetric binary variables, usually we assume they both appear, the case is much rarer than they both not appearing, okay. So for these asymmetric variables, they are different as r + s, but they both not appearing. In that case, actually is t is not so important. The reason For example has only the parts of y where attract attention. Then for that distance measure it we look at r + s divide by q + r + s. That means all the cases with t cases removed, okay. Then for their similarity measure, that means, how many times they are the same? Actually it's a q cases. For the q cases, we probably can't see that's a same denominator. Actually, Jaccard coefficient was somehow rediscovered in the Pattern Discovery here, they call this one coherence. And here, this coherence definition, if you really map them into this contingence table, they have the same definition as Jaccard coefficient. So then we look at the real cases, suppose we have some medical tests. Then we have Jack, Mary, and Jim, three cases, three people. Their tests actually is represented in this table. We can map them into table, since gender, the chance to be male or female are roughly equivalent, so this is a symmetric cases, so for us we are only interested in asymmetric cases, that is the remaining attributes becomes more important. Then we try to examine how they are different. Suppose we say Y and P, is positive case, and the value N, no, are not the case, will be 0, okay. In our case, we look at our distance special for asymmetric attributes, we can work out these [INAUDIBLE] for example, if a Jack and Mary, would proceed, Jack and Mary, they are the same parts of the case, like those have fever.both the test one actually is positive. There are two such cases. They are both negative, like a cough. Test two and test four, actually there are three cases they are both negative. But they do have one case they are different. Similarly, we can work the table for Jack and Jim, Jim and Mary. In this case we can calculate their difference. Probably we can easily see Jack and Mary actually are most similar, Jim and Mary are most different. We may conclude that Jack and Mary may have similar a disease in this case. [MUSIC]
[SOUND] Now we examine distance between categorical attributes, ordinal attributes, and mixed types. What are categorical attributes? Categorical attributes, also called nominal attributes because their value references by names. Take color as a example, we may have yellow, red, orange, blue, green. If we get their positions in color spectrum in physics, they are not ordered. In the sense either the two values are the same or they are different. The same as profession and many other things. Then how to calculate their distance. We can use simple matching method. For example, suppose there are total p variables, and their m matches. Then the number of mismatch is p minus m. So their distance between i and j will be the number of mismatches versus the total number of variables. Another measure is we can mapthem into binary variables. That means each value like red if they are existing as a red we write down as one. If they are not red, we write down as zero. Then we can change the categorical attributes into a set of binary variables. Then we can use the previous binary attribute evaluation function to evaluate them. Another kind of variable called ordinal variables. Ordinal variable means they do have order. They can be discrete like a rank, like a military rank, or even the rank for undergraduate students in the university, okay. Or they could be continuous, like time, okay? Then order becomes important. For example, in the University, a freshman is a first year, a senior is a fourth year student. In that sense, we can replace ordinal variable value by its rank. Then we can map any particular variable using this formula map onto this either 0 or 1, okay. Just give you an example. For example, we can map freshman into 0 because their position is 1- 1 0. And the same for senior it is four so by the total range is four. That's why they map into one. Then the distance for example between freshman and senior would be one minus zero, their distance is one. But between junior and senior their distance is only one third because with this one, now we can compute the other dissimilarity using the interval scale variables. What about we get a dataset may contain all attribute types, nominal symmetric binary, asymmetric binary, numerical or ordinal. No one can use a weighted formula to combine the facts. Then, if they are numerical data, we can use normalize the distance, like [INAUDIBLE]. If they are binary, or nominal data, we can use this formula as we just discussed. If they are ordinal data we can compute their distance using this formula. Then we can combine all their effects to compute their overall distance. [MUSIC]
[MUSIC] In this session, we're going to introduce cosine similarity as approximate measure between two vectors, how we look at the cosine similarity between two vectors, how they are defined. So we can take a text document as example. A text document can be represented by a bag of words or more precise a bag of terms. Each document can be represented as a long vector, each attribute recording the frequency of a particular term. The term can be a word or it can be a phrase. 5 means the term team, actually occurring in Document 1, five times, okay. So, we want to compare the similarity between Document 1 and Document 2. So how similar they are? We could use cosine similarity to do that. Other vector objects like gene features in micro-arrays can be represented in the similar way as a long vector, 'kay. For information retrieval, biological taxonomy, gene feature mapping, like a micro-array analysis, these are good applications to compare similarity between two vectors. The cosine measure is defined as follows. For example, we can consider the term-frequency vector to look at their similarity. They are defined by dot product of these two vectors divided by the product of their lengths. So we look at the, the cosine similarity definition, and take as an example. Suppose d sub 1 and d sub 2 are the vectors of these two documents, okay, then we can calculate their vectors dot product as follows, 'kay. Then we can calculate the length of each one of d sub 1's lengths, calculate using this formula, okay. D sub 2's two lengths eh, can be calculated also using the square root of sum of their product. Then the cosine similarity can be calculated using the formula given above. We can see their cosine similarity is 0.94, simply says these two documents are quite similar. [MUSIC]
[SOUND]. In this session, we are going to discuss correlation measures between two variables, especially we will introduce covariance and a correlation coefficient. Before we introduce covariance between two variables, we will first examine, or review the variance for single variable. What is variance? I think you all know what is average. For example the average salary of employees in a company. The average also core mean mathematically, as mu. So mu is actually the expected value of X, however, if we just use mean, we may not be sufficiently represent the trend or the spread of the value in the variable X. Okay. For example, we may not only like to know the average salary in a company, but we also like to know how this value spreads. That means whether they are very close to the middle, to the mean or they are widespread. They have many very high salaries and many very low salaries. In that case, we introduce the concept of variance. Which actually, is to measure how much the value of X deviate from the mean, or the expected value of X. Essentially we'll use sigma square to wrap in the variance of X. Sigma is called a standard deviation. Variance of X actually is expected value of X deviates from the mean if we take the square simply says, no matter if it's positive or negative, we all change them into positive. Then if X is a discrete variable, okay, then the formula is written like this. It simply says we use some of this function. This function is actually X deviate from the meal, from the mean value. Take the square times X density function. If X is a continuous variable, so we will take integral, where the range is from minus infinity to infinity. To that stand we see variance is the expected value of the square deviation from the mean. For this formula, we can also do a little transformation. For example, we can transform for the expect value of X square deviation from the mean. We can write down it's the expected value of X square minus means square. This transformation actually has been introduced in many textbooks. It's quite simple when I continue to introduce here. In many cases, if we write in this form, it may lead to more efficient computation, especially when you want to do incremental computation. If we take a sample, then the sample variance is actually the average square deviation of the data value xi from the sample mean mu hat. So the sample variance is written as sigma hat square. So we often use this formula, I'll use a similar transform formula to compute it. Now we introduce a covariance for two variables. Once we get two variables, X1 and X2, we want to see how these two variable, they change together, whether they're going up together or going down together Which would be the positive covariance. Let's look at the definition. The definition, actually, original definition is X1 minus mu 1 square. Now we see, actually these two variable we want to see X1, the difference from it's mean value of X1. X2 the difference from X2's mean value, or expected value. And then we look at their expectation okay. Mathematically we also can transform this into this form. If we get a sample covariance, we look at the sample covariance between X1 and X2. So the sample covariance is calculated by their difference from the sample mean. So that's also popular to use. Actually, the sample covariance can be considered as a generalization of sample variance. For example, originally we'll want to look at the two variables X1 and the X2, their covariance. But if we think this 2, X1 and X2, we replace it by X1. That means we just look at the two variable X1, X1, what is their covariance? Then we can represent these two by one. Then in that case this sample covariance of formula., we'll look at this formula, we change the variable from i2 to i1 and mu2 to mu1 hat. So then we'll derive this formula, and this formula essentially is sigma1 hat, it's square. So we probably can easily see, the sample variance is just a special case of sample covariance when the two variables are just the same. When the covariance value is greater than 0, we say it is a positive covariance. If it's this value is less than 0 it is negative covariance. If these two variables are independent, then their covariance is 0. However, the converse is not true. That means not when the covariance is 0, then that mean X1 and X2 are always independent. Only under certain additional assumptions, for example, if the data follows multivariate normal distributions. In that case the covariance of 0 implies independence. Now we will look at a concrete example. Suppose we have two stocks, X1 and X2. They have the following values in one week, like these five pairs of values. Then the question is, whether the stock effected by the same industry trends, term is, whether their price will rise or fall together. Then we calculate their covariance. We were be able to know whether they are possibly correlate on that one. So if we look at a covariance formula especially, we use more simplified computation formula. Then we can carry the expect value of X1 which is the mean value of X1. Expect value of X2, which is the mean value of X2, then we look at their covarience actually is, we use this formula. We look at their product, their dot product, then divide by sum of them, divide by the number of variable pairs. Then we minus, this is expected value of X1 and expected value of X2. Then we get the finer value is 4. That is, the covariance is greater than 0, that means X1 and X2, they rise or fall together. Then if want to normalize them, we will introduce correlation coefficient. That means for two numerical variables, we want to study their correlation which essentially is the standard covariance. That mean we want o normalize the covariance value where it is the standard deviation of each variable. So it is defined as correlation coefficient as the covariance divided by the product of their standards deviation. Or you can say, the covariance is divided by the product of variance, get their square root. So, if we look at sample correlation for 2 attributes X1 and X2, then essentially we get a row 1, 2 hat is equal to the sigma 1, 1's hat is essentially their sample covariance divided by the sample standard deviation. In a concrete formula we can write in this way. Then, if this correlation coefficient is greater than 0, that means A and B are positively correlated. That means X1's values increase as X2's. The higher value greater than 0, the stronger correlation. If rho 1, 2 equals 0, that implies they are independent under the same assumption as discussed in the co-variance. If they are less than 0, they are negatively correlated. Then we can look at the a set of variables. We can see for example, for 2 variables when they're perfectly negative correlated, they line up like this, their correlation coefficient is -1. Then if they become not so perfectly negative correlated, you will see their trend. When this value is 0, that's, you could not see anything like a positive correlated or negative correlated. But when you gradually grow these correlation coefficient, you will see their value become more and more correlated. When they are perfect correlated, then their correlation coefficient is 1. That simply says the correlation coefficient value range is from -1 to 1. Okay. Then if we draw this in the scatter plot, we'll see the set of points, their correlation coefficient changes from -1 to 1 in this shape. In many cases, we may want to write for 2 variables, we may want to write their variance and the correlation information into the 2 by 2 covariance matrix form. For example, you may say the variable 1 is self correlation there, essentially is their variance Is this 1. And for their coverence between 1 and 2 is defined here between 2 and 1 is defined here and then that's variable 2's variance. So this is a typical 2 by 2 covariance matrix. In general if we have t, d numerical attributes, that means suppose we find a data sets it has [INAUDIBLE] rows and d columns that means we really have d numerical attributes. Then their covariance matrix essentially written in this form. We can see this is the variance of variable 1, this is variable of the second dimentions and this is the variance of the d dimentions. And the covariance for each one would be lining up like this. And we give a few interesting additional reading. These are the several books, they contain interesting chapters discussing the different measures. Thank you. [MUSIC]
[SOUND] So first, we will introduce basic concepts of partitioning algorithms. The partitioning method is essentially to discover the groupings in the data. That means you get K groups if you want to partition, I mean, 2K groups by optimizing a specific object function, for example, sum of the square distance. And then we iteratively improve the quality of such partitioning. So the K-partitioning method, we partition a dataset D of n objects into a set of K clusters. Then we definitely can iteratively improve it, so that an object function is optimized, for example, the object function could be the sum of the square distance is minimized, where C sub k is the centroid or medoid of cluster capital C sub k. So a typical objective function is Sum of Squared Errors is often written as SSE. So a clustering, okay, for the SSE, is sum of K clusters, k is from 1 to K, okay. Then for each such cluster if an object's i is in this cluster, then the square error that simply says the distance of sum of these squares. These such distance, the whole function, is a K clusters, and each cluster get its points to its center the sum of such a square, distance or square errors. We want to minimize this objective function. Then in general what we can think, conceptual we can think of this, if you get a very nice K cluster, then, all the objects to the center is somehow shorter, okay. It is shorter than the sum of the square distance or the square errors. Actually, it will be pretty small. That's why we can't get within the cluster center, the sum of the square error is pretty small. Then if we get a K such thing, we add them together, the whole thing will be smaller. For the K-partitioning method, essentially the problem is given a K, the number of clusters, we want to find a partition of these K clusters that optimize the chosen partitioning criterion, for example, the sum of square distance. However, to find a global optimal, actually we need to exhaustively enumerate all partitioning because the potential number of partitioning actually is exponential. So more realistically, we should get a heuristic method, that means the greedy algorithms. For example, the typically used like the K-Means, K-Medians, K-Medoids and all the other methods are all greedy algorithms or heuristic methods to find such a nice partition. [MUSIC]
In this session we're going to introduce the most popular clustering methods, the K-Means clustering method. So you may wonder, who first proposed this K-Means clustering method? Some people refer to MacQueen because he published a paper on the K-Means Clustering algorithm in 1967. But, some other people said, Lloyd actually internally, in a company, published the, the one in the internal report in 1957. Since the publication was internally published mo, most people may not even know it. So, he republished this one in a journal in 1982. That's the reason some people say Lloyd first got this algorithm. But no matter what, actually, the general common thing for this algorithm is, they consider every center is represented by the center of the cluster, or we can say, centroid. The K-Means Algorithm essentially can be outlined as below. It means, given K, the number of clusters, first we can select the K points as initial centroid. Of course, you can randomly select it, that's what our original K means, that, or you may have some small way to select it we'll introduce later. Then, we can put this one on repeat and here, look, there is once we select the K centroid, we can form K clusters by assigning each point to it's closest centroid. Once assigning this, we may need to recompute the centroid because the initial randomly selected centroid may not be a very good one. So we recompute the centroid that means the mean point of each cluster. Once you recompute the centroid, some object initially assigned to say centroid A now because of all the centroid changed this point may be even closer to centroid B. Okay? That means we need to go back to form K clusters by assigning or re-assigning each point with its closest centroid now. Okay? Then we need to recompute the new centroid. That's why we need to get into this repeat until loop, until some convergence criterion is met. There are different kinds of measures that can be used for calculating the distance and assigning the, to its closest centroid. We can use Manhattan distance, L1 norm, or Euclidean distance, L2 norm, but often, our initial algorithm was using this Euclidean distance. We can even use Cosine similarity. Now let's look example. How we can execute this K-Means Clustering Algorithm. Suppose we got the into 2-D space. Initially there were black points on the initial data point. Then we can based on the first line of the algorithm, we can select the K points as initial centroid. Now, K is two. We can select the two initial centroid. Once we select this initial centroid we can form K clusters by assigning each point to it's closest centroid. Now you probably can see we can assign these, these black points to either red centroid or to the blue centroid, you can see that's a current assignment. Once you assign these to form two clusters, okay, then we need to recompute the centroid, okay, where you see, you can recompute the centroid once you recompute this red centroid mode down here, okay? Then this, blue one actually is also moved, okay. Once we recompute the centroid, we may need to go back to reassign these point to its closest centroid. In this case you probably can see, the closest centroid, like this, blue points assigned to the red ones, and these red, this red one assigned to the blue side of the cluster. Okay? Then we can calculate the centroid again. Okay? So this repeat the loop here until it becomes stable or until the, the difference the different assignment becomes really, really small. Now the first important thing we should know is, this actually is a pretty efficient algorithm for clustering. Because if we're told we'll have n objects, we will want to find a K clusters. Suppose we get a t iteration it becomes stable. You probably can see the computational complexity is, every time every object that you need to see which cluster is it belongs to, so this time, you're assigning to the corresponding K centroid, so that's why it's n times K. In total, you have t iterations, that's why the computational complexity is big O of t times K times n. usually K and t, number of cluster and number of iterations, is far smaller than number of objects. That's why if you give me n, supposed quite big number of objects, this complexity essentially is a linear to the size of n. The number of objects, that's efficient algorithms. However, K-means clustering often may terminate at a local optimal. Then, the initialization becomes important if you want to find a high quality Clusters. We need to have a smart initialization or we need to randomly initialize this many times, try to find a good clustering green dots. Another important thing is how to specify K, the number of clusters. We need to specify this in a, advance for the K means algorithm. There are many ways to automatically determine the best K. There are some research papers, there are lots of efforts contribute to this. In practice you also can run a range of values as a K then select which one is the best. Then for K-Means messrs it's quite sensitive to the noise data and outliers. So there are variations like K-medians, or K-medoids algorithm we try to overcome this outlier noise data problem. Another thing is K-means actually works only to objects in the continuous. That means numerical data in the, in the, in dimensional space. How to handle clusters on the categorical data? Actually, people invented something called K-modes algorithms. Another important thing we should know is, because we use the sum of the square distance, it then, it is not a good idea to try to find a clusters with non-convex shapes or non-circular shapes. Okay. What you find is something closer to a ball. So we can use density based clustering or kernel K-means algorithm to do if we get into a non-convex shapes. We're going to introduce this algorithms later. So, because K-means came quite early and very popular used, there are lots of studies to work out the different variations of the K-means method. For example, how to choose initial centroid. There are methods called K-means++, Intelligent K-Means, or Genetic K-Means. We are going to introduce K-Means++ in the next discussion. Another aspect is how to choose different representative prototypes for the clusters. That means we may not use the mean point, then we may use K-Medoids or K-Medians or K-Modes. We're going to introduce this in the subsequent discussion. Another thing is how to apply feature transformation techniques. So, we may be able to cluster things even if they are not in convex shape. For example, they are massive developed card, weighted K-Means or Kernel K-Means. We are going to introduce the Kernel K-Means method [MUSIC]
[SOUND] As we just had discussed, the quality of K-means clustering is quite sensitive to the initial relation status. Then we need to see how to do good in initialization so we find a good quality clustering using K-means method. I just gave you a simple example everybody can see. Different initializations may generate rather different clustering results. Some may not be optimal at all. Just give you example, suppose we have only four points, we will find two clusters. If we initialize in the solid circle. That means these two points get into one cluster like if we give you the center like this two, they will attract these two and these two as in one cluster. Then actually it becomes stabilized, because you calculate the center, the center is still here. They will not move. So then you find a pretty ugly cluster, because you perceive these, if you vertically group things into two clusters. The sum of the square distance or SSE actually become rather small. From this point of view we know the quality of a clustering could be very sensitive to the initialization. More recently, MacQueen in 1967 said we should select the k seeds randomly. Now we need to run this algorithm multiple times using different seeds. In some software package, they may even say, run these algorithms 200 times. Finally find which one you derive the best K-Means cluster simply says you want to find the SSE, the sum of the square arrow is minimized. There are many other methods also proposed for better initialization. For example, there's one called K-Means++, proposed in 2007. Essentially this ++ initialization is as follows. The first centroid that you can select randomly, but then the next centroid to be selected, you try to find the farthest point from the currently selected points. That means a selection based on weighted probability score based on a different probability you select which is best for the next one. Then this selection continues until all the k centroids are obtained. That initialization is done. Then we can run the K-Means algorithm. I'll give you a simple example of protein C, poor initialization may lead to some poor clustering. For example we take the same data sets, those black points as we've shown before, now we give you two same choice initialized as the red lines here, the blue lines here. With this initialization, we can get clusters, we can assign these objects into two different clusters in different colors. One, the upper part, those parts assigned to the red cluster. The lower part assigned to the blue cluster. Then we can recalculate the center again. You can probably see the center actually moved. And with this recalculation we can re-assign those points. You can run this if you want. You probably can see finally they're stable like this, but actually, this is not a very good cluster at all. That means that this run of the K-Means generates a poor quality clustering. That is simply giving us a simple show. We do need some smart interrelations or multiple random initialization is the best ones. So this is the initialization of K-Means clustering. [MUSIC]
[MUSIC] Now, I'm going to introduce you another interesting K partitioning clustering method called the K-Medoids Clustering Method. Why do we need to study K-Medoids Clustering Method? Just because the K-Means algorithm is sensitive to outliers. Because a mean is sensitive to the outliers. Just give you a simple example, if you look at a company's salary, if you adding another very high salary, the average salary of the whole company shifts quite a lot. So, let's look at the K-Medoids, what is K-Medoids? That means instead of taking the mean value of object in a cluster, as our centroid, we actually can use the most centrally located object in the cluster or we call medoids. That means the K-Medoids clustering algorithm can go in a similar way, as we first select the K points as initial representative objects, that means initial K-Medoids. The difference between K-Means is K-Means can select the K virtual centroid. But this one should be the K representative of real objects. Then we put this one into repeat loop. We can assign, similarly, we assign each point to the cluster with the closest medoid. Then, we can randomly select a non-representative object, suppose it's o sub i, or see whether we use o sub i to replace one medoid, m. Whether it will improve the quality of the class ring, that means the total cost of swapping is negative. Simply says, it was sloppy, we can reduce some of the square arrows. Then, we are going to swap m with object oi to form the new set of medoids. Then we need to redo the assignment. And here, this process goes, and here, the convergence criteria is satisfied. Now, we'll see a small example how a typical K-Medoids Algorithm is exacted. We look at the PAM, as an example. Suppose we are given ten small number of points in this small graph. In this 2D space, we want to find the two clusters. At the very beginning, we arbitrarily choose k objects here. We choose two objects as initial medoids. Then we will find the clusters of these medoids, as follows. Okay. Then, we will see whether we can randomly choose another object like O random. Say these non-medoic object. We want to see whether it could become a medoid. If it would reduce the total cost. Always say we get a better SSE. And in this case, suppose we choose one here, but we found it does not really reduce any, the total SSE. Then we actually can get another one. Like we get this orange one. Then we look at the cluster we can form. We know this one will reduce the total SSE. That simply said the quality of the cluster is improved. Then we will do the swapping. So this essentially is listed here as we initially, we select initial K medoids randomly. Then, we will do object reassignment. Then we try to swap medoid, m, with the random non-medoid object, o sub i, if it improves the clustering quality. Then we'll do it again and again until the convergence criterion is satisfied. So this is just a simple execution to illustrate the ideas of this K-Medoids, how it is executing. Now we see these K-Medoids clustering essentially is try to find the k representative objects, so medoids in the clusters. And, the typical arrow is in PAM, called Partitioning Around the Medoids, was developed in 1987 by Kaufmann & Rousseeuw, starting from initial sets of medoids. Then we iteratively replace one of the medoids by one of the non-medoids, if such a swapping improve the total sum of the squared errors. That is the total quality of the cluster. This method works effectively for small data sets. Because we can keep trying different swapping, but it cannot scale well, because the computational complexity is quite high. If we look at it into detail, actually, this computational complexity for every swapping, actually, it's to the square of the number of points. That's quite expensive. So, how to improve its efficiency. There's one proposal by same authors in 1990 called CLARA. Essentially, it's PAM on samples. That means, instead of using the whole points, we choose a sample, s. S is the sample size. Then, the computational complexity, this square, actually, comes down to the size of the sample. However, if the sample, initial sample selection, is no good, the final classroom quality could be poor. Then in 1994, there's another algorithm called CLARANS proposed, as every iteration we do randomized re-sampling. That means, we do not keep exact the same sample. We do randomized re-sampling, that, or ensure the efficiency and the quality of clustering. [MUSIC]
[MUSIC] In this session, I'm going to introduce the K-median and the K-modes clustering methods as two interesting alternatives to the K-means clustering method. Why we want to do K-Medians. Because medians are better than means when we encounter outliers. Medians usually less sensitive to outliers comparing to means. For example, in a large firm, okay, there could be many employees. We want to find the median salary, even when you're adding, when you add a few top executives, the mean salary may change quite a lot. The median could still be very stable. So that's the reason, instead of computing the mean value of the object in the cluster, so we take the medians of the centroid. And we use L1 norm. That means the distance as our distance measure. The criterion function for the K-Medians algorithm is written as this. The center is sum, the total sum should be K from one to the number of cluster K, and for each cluster the object in the cluster you just look at the difference. The difference take the absolute value of their distance to the median. The K-Medians clustering algorithm essentially is written as follows. The first, at the very beginning we selected K points as the initial representative objects. That means as initial K medians. Then we get into this loop, we assign every point to its nearest median. Then we re-compute the median using the median of each individual feature. Then this process repeats until the convergence criterion is satisfied. Then we look at k-modes as another interesting alternative to k-means. K-modes essentially is to handle categorical data. Because K-Means cannot handle non-numerical, categorical, data. Of course we can map categorical value to 1 or 0. However, this mapping cannot generate the quality clusters for high-dimensional data. Then people propose K-Modes method which is an extension to K-Means by replacing the means of the clusters with modes. The Similarity measure between object X and the center of cluster Z is written as follows, okay. This is the similarity, we can see distance measured, distance function, okay. For the jth field of object x, that means x sub j, and look at the jth field of the cluster center z, what's a distance? It essentially is if this object is not equal to the center on the same attribute, essentially say they are different, then we just say the distance is one because it's very dissimilar, or the distance is largest. However, when this value is equal to this attribute center here then we use this formula. This formula actually means if there are many many objects occur the same in this cluster, okay. Then, this value actual be very small. For example if the attribute cluster, there's only one value. But if very frequent, then this guy have to equal to this value. Then, this division where it lead to one, and the distance is smallest. In most cases, the more frequent for this j value, and this one would be closer to one, so the distance is smaller. And if this one is very rare you go to the very rare value and the number of distinct value is quite big. So this your distance is closer to l. But this is a quite reasonable definition. That means this dc measure, distance function is essentially frequency-based. The more frequented the closer the last frequent and there are far away evan it may equal to this value. The whole algorithm that were not list here but is still based on the iterative one we first will object cluster assignment then to centroid update then we take this one into loop and tear the criterion reached. Then there is another iterative method called fuzzy K-Modes method. The fuzzy K-Modes method essentially is to calculate a fuzzy cluster membership value for each object to it's cluster. Simply says, you give a fuzzy cluster value, if it's very close to this cluster, the fuzzy value is closer to 1. It's far away from this cluster, and the fuzzy value is somewhat closer to zero. Okay, and if we have the data some attributes are categorical attributes, some attributes are numerical attributes we can use K-Prototype method essentially for numerical one we can use a method the function similar to the K-Means. And the categorical one we can use similar to K-Mode and finally we integrate this, we can get a K-Prototype method. [MUSIC]
[SOUND] Hi, in the last session of this lecture, we're going to introduce Kernel K-Means Clustering method. What is Kernel K-Means? Essentially is we know K-Means can only detect clusters that are linearly separable, they will have difficulty to handle non-convex clusters. For example, if you look at this set up data points if we say, k equals 2 we want to find these two clusters of different color. For example, the red one is a core part right in the center, the blue one is a big ring surrounding this circle. However, for K-Means, you can only find something linearly separable. Likely, you will chop every cluster in two half and you find something quite ugly. Then our idea is if we can project data onto a high-dimensional kernel space and then perform K-Means clustering on this space. We may be able to solve this problem of the clustering problem. That means we'll remark data plans in the input space on to a high damage in our feature space using the corner function. Then we perform a K-means on the map feature space. Of course, by doing so their computation or complexity could be higher. Because we need to compute and store n by n kernel matrix generated from the kernel function on the original data. If the original data contain n objects, if this n is large, then n by n kernel matrix could be very large. Actually the widely studied the spectral clustering can be considered as a variant of Kernel K-Means clustering, that's this Kernel K-Means. It's a pretty interesting. Let's look at kernel functions and Kernel K-Means clustering. The typical Kernel functions, for example, we may have polynomial kernel of degree h, you use this formula. If we have Gaussian radial basis function, RBF, the RBF Kernel is a typical Gaussian function. Sigmoid kernel is defined in this way, and the formula for kernel matrix X that means for any two points, Xi sub i and X sub j. Within cluster C sub k then we can map into this kernel function in this way then if we want to compute sum of the square errors. Okay so a Kernel K-Means the formula is as follows whether you can see is we want to find the number of clusters from one to K. K is a number of clusters then from each cluster, each point in cluster C sub K, this part where we just need to use some of the squared distance phi Xi, and the cluster center C sub k. Then the formula for the cluster centroid is very similar to the K-means. As we use C sub k, the center essentially is defined by the sum of this function divided by the size of this cluster. The clustering can be performed even without actual individual projection of this one for the data points with what we needed just computing those formulas. Here I give you the Kernel function to see how we can do the mapping. Suppose we want to check how to map the real points into this RBF corner. Okay we are given five original points like this. X sub 1, X sub 2, X sub 3 to X sub 5. For these five points this is original space. If we set sigma equals to 4 actually you can set sigma to other values as well, okay. Equals to 4, the calculation just a little nicer and simpler. So you probably can see then if we want to calculate this distance, okay, this formula, this formula that is on the top of this part. What we can see is x1 is zero zero, so there are two zeroes, and x two, x is four four, these two fours. So when you compute this distance you basically compute the sum of their square distance, equals 32, then based on these formulas or we get a e to the power minus 32 divide by 2. Times 4 square, then you get to without a e, to the power of minus 1. Then you can see for this mapping suppose sigma equals 4, what we can get is for k is when i equals one, you'll get x1, x1. You look at x1, x1 they're the same so this part is essentially you derive as zero okay because of this mapping. Then if you get x1, x2, what you can see here is you get this value e to the power of -1. Similarly you can derive other values. What you see is originally you have five points,once you map to this RPF kernel space we get a 5 by 5 matrix. That's why the computation could be more expensive because you map into n by n matrix. Now we want to calculation the Kernel K-Means clustering using this example. For this example what you can see is these are the original data points for this data set. If we want to use K-Means to generate the two clusters, you will be able to generate like this. This is pretty ugly, because you can see this very dense kernel will be split, and this ring will be split as well. You generate something really very ugly, not like a quality cluster. However, if we are to Gaussian RBF Kernel transformation mapped it into a Kernel Matrix K for any two points. Using this function and the Gaussian Kernel, we'll be able to generate pretty nice clusters. That means the K-Means clustering actually is conducted on a mapped data and then we can generate the quality clusters. That's why the Gaussian K-Means Clustering could be rather powerful. Here are a set of interesting references, you want to look at it. The first on is MacQueen's paper, Lloyd paper as you can see is published in 1982. Actually in 1957, there was a Bell Lab papers collections, essentially the material was first appear in 1957 in Bell Lab internal report, okay. Then the other interesting papers and books you may like to read, as well, thank you. [MUSIC]
[SOUND] In the first section we will introduce, Basic Concepts of Hierarchical Algorithms. So what is hierarchical clustering? The hierarchical clustering means we can start from either singleton cluster, iteratively merge them into higher level clusters, okay. Or we can start from one big macro cluster, iteratively split the bigger cluster into smaller ones. Then in this way we will generate a clustering hierarchy, drawn as a dendrogram like this. There is no requirement to specify K, the number of clusters. And it is more deterministic. There is no iterative refinement process. Hierarchical clustering methods contain two categories of algorithms. In general, the hierarchical clustering methods have two categories of algorithms, one called agglomerative. It start with singleton clusters, continuously merge two clusters at a time to build bottom-up hierarchy of clusters. The second category is called divisive cluster. Essentially, it starts with a huge macro-cluster, then split them continuous into two groups. It will generate a top-down hierarchy of clusters. [MUSIC]
[MUSIC] >> In this session, we're going to examine agglomerative clustering algorithms. We already introduced the general concepts of, you know, agglomerative and divideditive clustering algorithms. Now we look, from the computer science point of view, we can think agglomerative clustering essentially is a bottom up clustering. That means we start from the single den clusters, that means every single element treated as one cluster. Then we try to merge their nearest neighbor into bigger, bigger clusters and eventually merge them into one cluster. This one is a bottom up, or agglomerative clustering. In Kaufmann and Rousseeuw's 1990 book they describe one algorithm they call AGNES, or AGglomerative NESting. This algorithm uses a single-link method, or we call nearest neighbor method. And the dissimilarity matrix try to continuously merge nodes that have the least dissimilarity, or the nearest neighbor. Eventually, all the nodes merged into one cluster, okay. That means you start from the singleton, you try to find its, everyone's nearest neighbor. Then you check the, the, the closest nearest neighbor, you try to merge them. Okay. Then once you merge into bigger cluster you're looking at cluster, cluster their nearest neighbor they're single-link. Then decide which one to merge eventually you'll merge everything into one cluster. Actually in the rear agglomerative clustering algorithms, they vary different similarity measures. For example, we just discussed AGNES. It uses nearest neighbor we called single-link measure. You also can use the farthest neighbor, or diameter. That's complete link. Then you can use group average. That means you average of everything based on their distance. Every pair of elements, you average them. Then you calculate the distance based on the average link. Or we can use central link, that means we look at the distance between the centroids of those clusters. Let's examine a little detail on these different links. We first look at the single link, or we call nearest neighbor. Okay. So the similarity or the distance between the two clusters is defined based on the most similar or the closest elements in these two clusters. Okay. Just to give you an example, suppose one is in U.S., one is in Cuba, you want to find their closest points. Likely in the U.S., it will be the Key West. So such kind of merging essentially is you examine its neighbors, or the close regions. So, you know the overall structure of the cluster. So, in that context, you will be able to find irregular shaped, or non-elliptical shaped, group of objects. This link is also sensitive to noise and outliers. Because obviously if you get outlier which is very close to the other one, you may, you may decide to merge them. Another approach called complete link is you check the diameter of the cluster. The idea is you try to define the similarity between the two clusters based on their farthest neighbors. That means between the most, the de-similar elements in these two objects. For example, if you define the distance between U.S. and Cuba, you probably, for the complete link, you may pick up the farthest point in Alaska. So that's pretty far apart. So that means you actually try to merge the two clusters to form one with the smallest diameter, because when you merge them together, you want the final one is pretty compact. You examine non-local behavior, you obtain compact shaped clusters. But this measure also sensitive to outliers, okay? Because you probably do not even want to think about Alaska. You may think about Hawaii or Guam, you know, you try to merge them. That's pretty far apart. Then we will look at the average link. The average link between two clusters actually is rather expensive to compute. Because what you want to calculate is the average distance between all the elements in one cluster and all the elements in the other cluster. That means you calculate all the pairs of the two clusters, then you try to average them. For example, if you have a number of cluster in C sub a is N sub a, and a number of elements in cluster C sub b is N sub b. So what you want to care, the distance, essentially you will get total number of pairs is N sub a times N sub b. That's pretty big, you know, average time. So an easy way, or a more efficient way to calculate the the distance between two clusters, actually use centroid link. The centroid link means you calculate the centroid of cluster C sub a, the centroid of cluster C sub b, then the distance between the two clusters is defined by the distance between these two centroids. And you may even want to put a weight there for example, if the cluster C sub a has many, many points versus C cluster C sub b, you want to take this into consideration. Then we introduce GAAC, or group averaged agglomerative clustering. That means, if you assume the two clusters C sub a and C sub b be merged into one cluster, C sub a unit b, then the centroid will be defined by, you get a centroid of C sub a. But you'll time the number of elements in the cluster, then you get a centroid of C C sub b of cluster C sub b, bigger C sub b. Then the, you want also times their number of elements. Then you finally normalize them, okay? So this is the GAAC measure. Another interesting Y is people define one called Ward's criterion. This Y's centering is to measure the increase, once you do this merge. Remember, once you merge the two clusters, C sub a and C sub b into C, the sub union b the SSE, the sum of the squared distance were increased because the cluster becomes bigger. Okay. Then once it's increased you want to see the original clustering. What's the difference? What's the increase you've got on this? Okay, this why is defined using this criteria and essentially is you look at the distance between the two centroids and then you, based on this weight, you calculate the increase. [MUSIC]
[SOUND] In this session, we examine more detail on divisive clustering algorithms. We already introduced a general concept of divisive clustering. Essentially is, we start from a big single macro-cluster, and we try to find how to split them into two smaller clusters. We continue doing this, finally, every single node become a singleton cluster. Okay. So, this method, actually described in Kaufmann and Rousseeuew's 1990 book, called DIANA or Divisive Analysis, is also implemented in some statistic packages, such as Splus. This is essentially inverse of AGNES. That means you start from bigger cluster, you start splitting them continuously, recursively, finally you'll find that each one is a single, singleton cluster. Divisive clustering is a top down approach, because you start from all the points as one cluster, then you'll recursively split the high level cluster to build the dendogram, okay? It can be considered as a global approach. It is more efficient, when compared to agglomerative clustering. Because you don't have much choice. You just decide every point. You just decide how to split, and you do it recursively. But we will see how which cluster to split. If you have multiple clusters now, you can check the sum of the squared errors of the clusters, and see which one is the largest one. That you, then simply say, this one is not a very good, you, you want to split them. You want to reduce the sum of the squared error, overall. Then, once you decide to split this cluster, then, how do you split it? Okay. Actually, you may try to split, in many ways. For that you can use Ward's criterion we just introduced, to see which one, which split, give you the greatest reduction in the difference of the SSE criterion. Then you choose that one to split. For categoric data we can use Gini-index in a similar way. Then, the third issue is how to handle the noise. Essentially, when you decide to split, you don't want to split into too small cluster, mainly contain noises. That means you need to use a threshold to determine the termination criterion, okay? So, these are the major issues, that design divisive clustering. [MUSIC]
[MUSIC] Hi, in this and subsequent sessions, we are going to discuss several Extensions to Hierarchical Clustering. Hierarchical clustering methods look simple, but on the other hand, there are some weaknesses of hierarchical clustering method. The first weakness is the master can never undo what was done previously. For dividing methods, you first try to figure out how to divide one cluster into two subclusters. But once you divide them into two subclusters, you work on each subcluster and try to do further splitting. You will never be able to merge them back again and try to adjust to some particular elements. Even for agglomerative clustering, the philosophy is similar in a sense. Once you try to merge two clusters into one, the subsequent you know, analysis will be treating this one as one unit. You will never split it again, and you try to see whether other subcluster obtained so far can be further merged. This, you know, require you for every split or merge must be final. That's too high requirement to generate high-quality clusters. The second problem is the masters may not scale well just because every time when you try to merge, you try to check all the possible pairs. So the complexity is at least n square, when you want to split, you try many different possible choices, to try to find the best to split but complexity is also high. There are some developments other hierarchical clustering algorithms. In this lecture, we are going to introduce three of them. One is a BIRCH, developed in 1996, you use micro-clustering, macro-clustering idea, so using clustering feature tree and incrementally adjust the quality of subclusters. The second one we are going to introduce is called CURE, developed in 1998. That method essentially is to, representing the cluster using a set of well-scattered representative points. The third one, CHAMELEON, it was developed in 1999, it used graph partitioning methods on K-nearest neighbor graph of the data. We will introduce all these three, one by one. [MUSIC]
[SOUND] In this session we are going to introduce you an interesting extension of hierarchical clustering method, called BIRCH: A Micro-Clustering Based Approach. BIRCH is an abbreviation of Balance Iterative Reducing and Clustering Using Hierarchies. It was developed by a group of researchers in University of Wisconsin in 1996. The general philosophy is, it incrementally constructs a CF tree, or called a Clustering Feature tree, which is a hierarchical data structure for multiphase clustering. For phase 1, essentially, it scans the database to construct an initial in-memory CF tree. Which is a multi-level compression of the data that tries to preserve the inherent clustering structure of the data. Then at Phase 2, it uses an arbitrary clustering algorithm to cluster the leaf nodes of the CF-tree. The key idea is multilevel clustering. The low level it does micro-clustering, therefore reduces the complexity, and increases scalability. At the high level, it does macro-clustering. It leaves enough flexibility for high level clustering using different clustering methodologies. The BIRCH methods scales linearly. That means you, it finds a good clustering with a single scan, and then improves the quality with a few additional scans. The clustering feature we first introduce is the CF vector in BIRCH. The BIRCH Clustering Feature essentially is suppose you get these five points into one cluster. okay? Then suppose these are the five points, their positions, okay? Then the CF vector contains three components. One is the number of data points. The second is a linear sum of the points in the cluster. The third one is square sum of the N points. Okay. So you probably can see the, the first one, 5 means there are 5 points. The second one acts as a linear sum of each dimension. Okay. The third one actually is the squared sum of each dimension. So the Clustering Feature essentially is the summary of the statistics of a given sub-cluster, which you can consider the number is the zeroth, the first one is the linear, the second one is the second moments of the sub-cluster from the statistic point of view. That means it will register the crucial measurements of the, for computing cluster and utilizes storage quite efficiently. So we can look at the, the general concepts of centroid, radius, and diameter. Okay. The centroid essentially is the center of the cluster, okay? Then, suppose we have a vector of N dimensions, x sub i. Okay. Then, the centroid is essentially computed by the sum of all the points in this cluster divided by the number of points in the cluster, so that what we get is a centroid of the cluster. Okay. Then the radius actually is the average distance from the member objects to the centroid. That essentially is every one you get a difference with the centroid, then we use the sum of their square distance. Divide by the number of points in the cluster. Take their square root. Essentially it's the square root of the average distance from any point of the cluster to its centroid. What is diameter? Diameter essentially is average pairwise distance within the cluster. That means if x i and x j is within the same cluster, so essentially what we want, we will find is there are total n times n minus 1 pairs and we sum up all these pairwise distance then you get the square root. That's the diameter. Then we look at CF Tree structure in BIRCH. The CF Tree Structure essentially is very much like a, B+-tree. We can do incremental insertion of the new points. That means when the new points come, okay, we can find the closest leaf entry. Start from the root. Okay, then we try, traverse we find the closest entry, we can add points to the leaf entry and update the Clustering Feature, CF. If this entry diameter is greater than the maximum diameter, then we'll split the leaf and if it's possibly we even will be able to split parents based on the B+-tree algorithm. A CF tree has two parameters, one called branching factor. That means the maximum number of children. Another is maximum diameter of sub-clusters stored at the leaf nodes. Then a CF tree essentially is height-balanced tree that stores the clustering features. The non-leaf nodes store the sums of clustering features of their children. So we can see BIRCH is an interesting algorithm, because it, it is an integration of agglomerative clustering with other flexible clustering methods. The low level we do micro-clustering. We explore the CF feature and BIRCH tree structure. It preserves the inherent clustering structure of the data. At the high level we do macro-clustering. It provides sufficient flexibility for integration with other clustering methods. So this method act, impact to many other clustering methods and applications for large data sets. There are some concerns. One is the, the BIRCH tree is still sensitive to the insertion order of the data points. Another is, since the leaf nodes has a fixed size, the clustering obtained may not be as natural. And also, the clusters tend to be spherical given the radius and diameter measure as the major parameters. Still, it is pretty interesting algorithm and it can generate quite effective clusters. [MUSIC]
[SOUND] Hi, in this session we are going to introduce another expansion to hierarchical clustering method called CURE: clustering using well-scattered representative points. And it was done by a group of researchers at Bell Labs in 1998. CURE actually represents a cluster using a set of well scattered representative points. Let's look at the details of the CURE. Actually, for CURE, the first interesting thing is redefining the clustering distance to be the minimum distance between the representative points chosen. But how to choose representative points? Actually is you can consider each group, you may have some points which is closer to the center of this group of data. You can consider this as a representative point of this group. Okay? Then for this group of data, their distance between the clusters actually is their minimum distance between the represented points. This actually incorporate the ideas of both single link and average link. Single link in the s, in the sense that you still look at the minimum distance of, between clusters, but you do not look at every point. You look at the representative. This is the literal, like, average link, or centroid link. Because we choose those scatter points, it will help CURE capture the cluster of arbitrary shapes. Because you can see if you get different shapes, you look at the representative points some particular shape, actually the, the representative point, it will reduce the complexity. Okay. Another interesting point is, if you use a shrinking factor, alpha. That means that the points actually are shrink towards a centroid by a factor alpha. Okay. For, for example you can see the, left side, okay, this group you can find these red points, a set of representative points. Then, we look at the centroid of these points. You actually shrink this group of points toward the centroid, so you probably can see these red points actually shrank towards the center. Once it's shrank, it has a greater effect to outliers than to the normal points. Because of the outlier, even you have a chance to find outliers pretty far away from this and you take it as a rate, representative point. Because it is far away from the center with this factor alpha, it will shrink dramatically towards the center, okay? That's why the outlier, effect will be minimized, okay? So, you probably can see, with this, map, transformation, okay? It improves the efficiency and, and it moreover improved the effectiveness. Actually the paper also taking a set of the points you know form into different shapes. You'll probably see if you use a typical like a k-mean space approach you're going to find the cluster in the center. You'll find a very ugly shape. And, these kind of clusters we're, actually getting into you know, these ugly shaped cluster, we're painting those connected objects into different clusters, different colors. However, using, using CURE, we will be able to find very nice, you know, group the points you perceive all these shapes, those points, closer together even it is not a spherical shape. You'll still find very nice clusters. That's a power of CURE. [MUSIC]
[SOUND] Hi, in this session we are going to introduce an interesting hierarchical clustering algorithm extension called CHAMELEON which is a graph partitioning on the KNN, means Kenya's neighbor graph of the data. So, this graph partitioning based approach was developed in 1999 by a group of researchers at the University of Minnesota. CHAMELEON actually has some interesting points. It measures the similarity based on a dynamic model. Essentially two clusters are merged. Only if the interconnectivity and the closeness between the two clusters are high relative to their internal interconnectivity of the clusters. And a closeness of items within the clusters. CHAMELEON is a graph-based, two-phase algorithm. In the first phase, it uses a graph partitioning algorithm. Essentially, it cluster objects into a large number of relatively small sub-clusters. Also, we call it graphlets. Then, in the second phase, it uses an agglomerative hierarchical clustering algorithm to find the genuine clusters by repeatedly combining these sub-clusters or these graphlets. Let's look at the overall framework of CHAMELEON. Suppose we have a large data set. Then we can use K-NN graph to merge them into sparse graph like this. What is K-NN graph? Essentially we can see that two points, P and Q, are connected. If Q is among the top k closest neighbors of p, then based on this k nearest neighbor, you construct this graph. Then we'll repartition this graph into a good number of smaller subclusters, or called graphlets. After this partitioning, we are going to merge them into high quality clusters. How we merge those graphlets back to the bigger clusters? Essentially the, the merge is based on these two measures. One we call relative interconnectivity, another we call relative closeness. Relatively interconnectivity means the connectivity of C sub 1 and C sub 2 over their respective internal connectivity. Okay. Then the relative closeness is defined as closeness between clusters C sub 1 and C sub 2 over their respective internal closeness. Let's look at the, the messert in some detail. The first is KNN graph. What is KNN graph? Let's look at the example. This is the original two dimensional data set. So the first nearest neighbor, one nearest neighbor graph is for each point, each you know, object. You just find the top one closest neighbor. For example, this ones closest neighbor could be here, this ones close neighbor could be here. Then you construct a subgraph based on these you get a one, you get a one, one nearest neighbor graph. For two nearest neighbor graph is you, for each point, you look their two, top two nearest neighbor, construct a graph like this. Three nearest neighbor graph is for each point, you look at their top three nearest neighbors in a constructed graph like this. Okay? Then, we look at their absolute interconnectivity between C sub i and C sub j. Okay. These absolute in, interconnectivity is, essentially, the sum of the weight of the edges. That connects vertices in C sub i to vertices in C sub j. Essentially you look at the weighted sum of these edges. Then the internal connectivity of the cluster C sub i essentially the size of its min-cut bisector, the weighted sum of these edges partition the graph into two roughly equal parts. Then you look at the weighted sum of the edges of these partition. Then the relative interconnectivity, RI, essentially is for C sub i and C sub j is defined by their ab, absolute interconnectivity divided by or you can say normalized by the average of their respective interconnectivity. Then we look at the relative closeness and the merge of sub-clusters. Relative closeness between a pair of clusters C sub i and C sub j is defined as follows. The first is what is absolute closeness between C sub i and C sub j. The absolute closeness essentially is the average weight of the edges connected these, the vertices between these two clusters. Then their internal essentially the internal closeness like this internal closeness of C sub i internal closeness of C sub j, is the average weight of the edges that belong to the min-cut bisector of clusters C sub i and C sub j, respectively. That means you look at what is min-cut bisector cut this cluster, or cutting this cluster. Then you'll find, their average weights of the edges. Okay. Then you probably can see the relativeness is, is the absolute closeness normalized with their inproportionally defined internal closeness. Then how do we merge sub-clusters? Essentially is we will only merge those pairs of clusters whose relative interconnectivity and relative closeness are both above some user specified threshold. That means we only merge those maximizing the function that combines relative interconnectivity and relative closeness. That means even we find those pair are mergeable, we still want to find these merge, we're maximizing the function. Then based on the implementation and experiments we properly can see, CHAMELEON actually can cluster complex objects like this. Okay. This actually, these are all the different points with different density different shapes, as well as lots of noise points. Even with different shapes, different density and different shapes and different noises, we still can see CHAMELEON actually generates pretty high quality clusters. Okay. That simply says CHAMELEON using this graph partioning and emerging methods is pretty robust and generate a pretty high quality clusters. Even it is in the spirit of hierarchical clustering, it does do a lot of good with this graph-based method. [MUSIC]
In the last session of this lecture, we are going to introduce probabilistic hierarchical clustering. What is probabilistic hierarchical clustering? Let's first examine the problems of algorithmic hierarchical clustering. The first problem for algorithmic hierarchical clustering is it is nontrivial to choose a good distance measure. For example, we already see the single link, the complete link, the average link and the central link. They're all different measures, but it's hard to choose a good distance measure based on the application problems. The second problem is it is hard to handle missing attribute values because when the attribute value is missing, it may impact on the quality of clustering. The third one is the optimization goal of algorithmic hierarchical clustering is not so clear because it is a heuristic algorithm, it more relies on local search. Now, there is another approach called probabilistic hierarchical clustering. This method essentially uses probabilistic models to measure distance between clusters. It is largely a generative model which means it regards the set of data objects to be clustered as a sample of the underlying data generation mechanism to be analyzed. And then you finally find the parameter, find out how they generate this data set. It is easy to understand. [COUGH] It has the same efficiency as algorithmic agglomerative clustering method and, also, it can handle partially observed data. In practice, we can assume the generative models adopt common distribution functions. For example, we can assume it's a Gaussian distribution or Bernoulli distribution, and governed by a set of parameters. Let's look at the generative model. In the generative model, we can assume, given a set of one-dimensional points, X, we can assume they are generated by a Gaussian distribution like this. That means you're essentially using different mu and sigma. You may generate the distribution according to the typical, normal distribution or Gaussian distributing formula. Then, the probability a single point, x sub i, which is one point in the set of X, is generated by the model. Essentially, it's what's the probability to generate x sub i under the condition of mu and sigma is based on this formula. Then for the likelihood that this whole set of X generated, essentially, is the likelihood for the normal distribution to generate this set of x points. Essentially, it's the probability of a set of points, x, under the condition of mu and sigma squared. For i from 1 to N, all of these probability multiplying together we get the probability generated at the data set, x. Then the task of learning this generative model, essentially, we try to find the parameters, a mu and a sigma. So for this particular distribution, we try to find the maximum likelihood this set of x has generated, according to these two parameters. So the Gaussian distribution usually, I think most people know, for example, for a bean machine, if you drop a ball with pins, if you use many, many balls, finally you will find the distribution is a Gaussian distribution. Then if we have may points, we can assume these points are actually generated by the Gaussian distribution like this. Actually, for a one-dimensional Gaussian, this different mu, different in mean and different of variance, you probably can see that it generate different distribution. For a two-dimensional Gaussian, an image generated in 3-D space, you probably can clearly see the distribution of this 2-D Gaussian. Then a probabilistic hierarchical clustering algorithm, essentially, is suppose we regard the partition, the whole set of points, into m clusters, C sub one to C sub m. Then the quality of the clustering can be measured using this quality function that's the product of all those probabilities. Then if we want to merge two clusters, C sub j1 and C sub j2, then the change in quality of the overall clustering is one criterion. So you probably can see this is the merged cluster, your generating new cluster, and this originally two cluster disappear from the original set of clusters. So that's the new clustering, that's the original clustering, because if you merge this, essentially, the sum of the square arrows will be increasing. So that's the reason you're trying to minimize that increasing. That's the probability of P(C sub i). Now, you get the probability of these two merged together, okay. The you have to minus the original probability, i from 1 to m. Then the distance between clusters C1 and C2 is, this is the probability of C1 and C2 together as one cluster, these are the independent clusters. If this distance is less than zero, we should merge these two clusters. So that's the idea, or the essence, of the probabilistic hierarchical clustering algorithm. [MUSIC]
[MUSIC] We first discussed basic concepts of density-based clustering. What is density-based clustering? Essentially, we do clustering-based on density. Which is a local clustering criterion, that means just to look at surroundings to see how dense this reading could be. We may introduce density-connected points. Its major features are we can discover clusters of arbitrary shape rather than just compact or something like a sphere. It can handle noise nicely. You may just use one scan because it only examines the local region to justify its density. But it needs density parameters as a termination condition. There are several interesting studies. For example, DBSCAN, we are going to cover in this lecture. Optics, we are going to cover also in this lecture. Denclue, we may not have time to cover it, but it's in the textbook. CLIQUE, developed by Rakesh Agrawal's group, we will cover it in the grid-based lecture. [MUSIC]
[SOUND] In this session, we are going to introduce a density-based clustering algorithm called DBSCAN. DBSCAN is a density-based spatial clustering algorithm introduced by Martin Ester, Hanz-Peter Kriegel's group in KDD 1996. This paper received the highest impact paper award in the conference of KDD of 2014. This paper developed an interesting algorithms that can discover clusters of arbitrary shape. Actually, DBSCAN itself is acronym of density-based spatial clustering of applications with noise. The method introduced a new notion called density-based notion of cluster. That means a cluster is defined as a maximal set of density-connected points. It defines two parameters. One called Epsom or EPS, it's the maximum radius of the neighborhood. That simply says, if you have a point q using the radius epsom, supposedly is 1 cm. You want to find out how many points include in q's disk. Then minimum points, a minimum number of points in the Epsom neighborhood of a point. For example, you can define minimum point as p then this q for sure is quite dense because it contains like 11 points within its radius. Then we can define the Epsom neighborhood of a point q which is defined as follows. For any point p is distance to q if it's decimal equal to Epsom then this p actually belonged to this Epsom neighborhood of q. For example, if you look at this point you probably can see. If using this one as q this neighborhood is including like 5 points so this one we actually can't call this one as a call. But if a does not have that many neighborhood but it can be reached by the cluster, we call is a spotter. But if it's isolated cannot form the cluster cannot be reachable by other clusters, we call this one outlier or noise. Now we look at an, a concept called directly density-reachable. Directly density-reachable means, for example, p is directly reachable by q. The reasoning is p actually is within q's Epsom neighborhood on the other hand, q itself is a core point because q's disk contain a number of points that's greater than the minimum number of points. Then we define another notion called density-reachable. Density-reachable means a point p is density-reachable from a point q with respect to the radius Epsom and minimum number of points, if there is a chain. There's a transitive closure, okay? You get the point p1 to pn. P1 actually is q then you can go to p2, then p2 is density reachable to p, eventually. Then we will say these p, i plus 1 is directly density reachable from p i and finally p stands a reachable from upon a q because there is a transitive closure of directly density-reachable. Then we define another notion called density-connected. Then we can say point of p it's density connected to point q if there exists a point o, which is density-reachable to p and also density reachable to q. That means p and q both can be at the border but as long as there is point o which can be directly density-reachable to these two points. Then these two points are density-connected or we say they are in the same cluster. So we introduce then the DBSCAN algorithm. The algorithm actually is saying arbitrarily select upon a p that means at the very beginning you can select at any point starting point. Then you retrieve all the points density-reachable from p with respect to Epsom and the minimum number of points. That means you try to find from this p how far it can density-reachable. That means you first find directly density-reachable then find its transitive closure or the reachable y, okay? That means you find a core and then you find the border. If p is a core point then the cluster is formed because it will reach from anywhere, you'll find you can't reach it. But if p is a border point, like this one is a border point, this one is a border point, then no point at density- reachable from p for example, like this point, okay? No point it can't be density-reachable, then you were think to this one is, noise [INAUDIBLE]. Then the DBSCAN where the, the, the next point in the database, it means you try to find a new point to start. You will continue this process until all the points have been processed so this is the output. This algorithm is quite efficient in the sense if you spatial index, so it accesses neighbors very fast then the computational complexity of DBSCAN is O(nlogn), where n is a number of database objects. Otherwise, the complexity is O n square because you have to check all the points every time but it comparatively this one is still pretty efficient algorithm. However, DBSCAN is sensitive to the setting of parameters, this one was published by Karypis group in 1999 in computer magazine. They show the DBSCAN, for example, if you said the minimum number point is full but Epsom you said this disc size, the radius size is 0.5 you actually find pretty nice clusters. However, if you said 0.4 you can see the clusters found many but they are pretty ugly. For the same experiments, you could see for these number of points, if you set minimum number of points for the Epsom is 5.0 you find a cross like this. If principally, probably even should find two clusters. On the other hand, if you select point 3.5 you find a bunch of clusters but they break nice cluster into many. If you set up upon sweep on zero, you will find it pretty ugly scenes because there are many many small scattered points you found many small clusters. So we may need to invent another message, try to fix this one. That means to make the master less sensitive to the setting of parameters. This is the one we're going to discuss in the next session. [MUSIC]
Starting this session, we are going to introduce grid-based clustering methods. What is grid-based clustering method? Essentially, it is you consider the whole space. Can be partitioned into multi-resolution grid structure. Then you work on the cells in this grid structure to perform multi-resolution clustering. That means we can partition the data space into a finite number of cells to form a grid structure. For example, on the plane you may be able to, to partition this plane into a 10 by 10 or 100 by 100, these kind of grid structure. Then you may find a cluster, so dense regions from the cells in the grid structure. That means you have a higher or lower resolution, or refined resolution or cross resolution in your clustering. So, a typical clustering algorithm have the following features and challenges. The first thing is very obvious. It is efficient and scalable in the sense, once you partition the number of cells, usually number of cells, even you say 10 by 10, you only have 100 cells. It is much smaller the number of data points, it could be minutes. The second one is its uniformity. It means it is uniform, because you get a ten by ten. It's a 3D uniform structure, but it, it's hard and are highly irregular data distribution. The third feature is locality. That means it's limited by the predefined cell size and borders and the predefined density threshold. The first one is a curse of dimensionality, means it's hard to cluster high dimensional data. We're going to introduce two methods in this lecture, one called STING, called a Statistic Information Grid Approach, developed by Wei Wang, Joe Yang, and Dick Muntz at UCLA, published in 1997, VLDB Conference. Another one, CLIQUE, which is both grid-based and a subspace clustering, an interesting methodology for subpace clustering developed by Rakesh Agrawal, Johannes Gehrke, Gunopulos, and Raghavan at IBM, published in 1998 SIGMOD Conference. [MUSIC]
[MUSIC] We first introduced STING, the first grid based method which is statistic information in the grid structure. This method called statistic information grid approach. Essentially you can think the gray structure looks like this. You got multiple layers. That means the spacial area's defined into rectangular cells at the different levels of revolution. Then these cells actually form a tree structure. That means each cell at a higher layer, each cell actually store a summary of the lower level set of cells. That means the cell at a higher level contains a number of small cells of the next lower level. To that extent, it forms a hierarchical tree structure. What the cell will store, statistical information of each cell that is calculated and stored beforehand and then can be used to answer queries. That means you can calculate the statistical information that is corresponding set of cells at the lower layer. Then the parameter of the higher level cells, the calculation contains a bunch of statistical information including number of points in the cell which is a count. Then their mean, their standard deviation, the minimum value, the maximum value and also the type of distribution. For example whether these points form a normal distribution, a uniform distribution or other kinds of distributions. So if we store the cell in a hierarchical way, to process a region query including finding clusters, we can start at the root of this grid structure. And proceed to the next lower level using this STING indexing structure. That means we can calculate the likelihood a cell is relevant to the query at some confidence level using the statistical information stored in the cell. Once we are confident, we need to go down, only children of the likely relevant cells are recursively explored. So it is quite localized, it could be quite efficient. We can repeat this process until the bottom layer is reached. The advantage is this structure is query-independent. And the process can be easily paralyzed and we can do incremental update as well. Also the efficiency is very good. That means the complexity is quite low because if the K is number of grid cells at the lowest level. Then the complexity is big O linear to K, where K is much less than N, the number of data points. However, it's disadvantages, because you formed a grid structure, you may lose a lot of details. The probabilistic nature of the higher level may imply a loss of accuracy in the lower level for query processing. [MUSIC]
[SOUND] In this session, we are going to introduce CLIQUE, a grid-based subspace clustering algorithm. This algorithm, CLIQUE, actually is an abbreviation of Clustering In QUEst. QUEst is an IBM data mining system. It was developed by a group of researchers at IBM. It was published in SIGMOD, 1998 conference. CLIQUE is a density-based, grid-based subspace clustering algorithm. Why it is grid-based? Because it discretizes the data space through a grid structure, and estimates the density by counting the number of points in a grid cell. Why it is density-based? Because a cluster actually is maximal set of connected dense units in a subspace. That means a unit is dense in the fraction of the total data points contained in the units, it exceeds certain parameters. Then you try to connect those dense units into a structure, into a cluster. That means it is density-based. But why it is subspace clustering? Because it starts from a low dimension, like a single dimension. For this particular subspace, you try to find neighboring dense cells in arbitrary subspace, and you can grow into 2-D, 3-D and find the maximum number of dimensions in this subspace, it contains clusters. It also discovers a minimum description of the clusters. That means for this particular algorithm, it automatically identifies the subspaces of a high dimensional data space that allow a better clustering than the original space using the Apriori principle. Let's look at an example. Suppose we want to find the clusters based on salary, age and number of vacation weeks. And we can first start from one dimension, to find out where are the dense points. For example, if you just look at salary, you may find the salary at the dense points is 20K to 50K. That part is pretty dense, especially around 40K. You find that the age is pretty dense between 30 to 50, and is very few, probably, lower ones. Then, based on number of vacation, you probably can see the vacation is clustered around two to four weeks. That means we find a dense region in each subspace, then generate their minimum descriptions. Then we use this dense region to find the promising candidates in 2-D space based on the Apriori principle. It simply says, if you find it's dense in 1-D on salary and on 1-D on h, then you probably can find that they are combined at one, that is, you try to find clusters within this candidate's space. Similarly, you can find the clusters within the candidates for the space in salary and vacation, okay? Then we can repeat this process in the level-wise manner in higher dimensional subspaces using the Apriori principle. It simply says, if you find a dense in this 2-D part and this 2-D part, then you may find the candidates in this 3-D region based on their intersections. So the major step of the CLIQUE algorithm is first try to identify subspace that contains clusters. That means we can partition the database space into the grid structure, then find the number of points that lie inside each cell of the grid partition. Then try to identify the subspaces that contain the clusters using the Apriori principle. That is, we try to identify clusters, and as a second step determine the dense units in all subspace of the interests. Then we determine the connected, dense units in all the subspaces of interests. Then, we will generate the minimal descriptions of the cluster. That means determine the maximal regions that cover a cluster of the connected dense units, then determine the minimal cover of each cluster. So this master, the interesting points is it automatically finds subspaces of the highest dimensionality as long as a high density cluster exists in those subspaces. Because if you find a 2-D, you'll try to find their intersection. If the 2-D form a cluster, the intersection is 3-D space, likely you may be able to find clusters. It is an insensitive to the order of records, the input, and also does not assume a particular data distribution. And it scales linearly with the size of the input, and has good scalability as the number of dimensions when the data increases. But this is a quite efficient algorithm. The weakness of the method, essentially, is because we use the grid-based clustering approach. So the quality of the results will depend on how to choose the number and width of the partitions and the grid cells. Nevertheless, it's a very interesting subspace clustering method. Finally, I should say all the original papers on this density-based and grid-based clustering are listed here. Also, for Sheryl Aggarwal and Reddy's book there are two chapters. One is called Density-Based Clustering by Martin Ester, another is called Grid-Based Clustering by Cheng, Wang and Batista. They have a very good summary and also introduce many more methods on density-based clustering and grid-based clustering methods. [MUSIC]
[MUSIC] We first introduce some basic concepts of clustering validation. We know clustering is unsupervised method, in the sense we do not have any expert to judge whether this clustering is good or what should be clustered. So it is important to evaluate clustering quality. That means evaluating the goodness of the clustering. Also we want evaluate clustering stability. That means to understand the sensitivity of the clustering results to various algorithm parameters, for example, the number of clusters. That means, what number of clusters is really good based on our evaluation. The third one is clustering tendency. That means whether it's suitable to do clustering. That means whether the data has inherent to grouping structure, then we should, for some kind of clustering, to discover it. [MUSIC]
[SOUND] In this session, we're going to study another important issue called clustering tendency. What is clustering tendency? That means we want to study whether the data sets we have really contain clusters. Whether data contains some inherent grouping structures. Otherwise, even use random generated data, you may find a certain number of clusters. However, it does not really make sense. That means we want to assess whether the data is suitable for clustering. However, to determine the clustering tendency, or clusterability, is a hard task. Just because there are so many definitions of clusters. For example, we studied partitioning methods, hierarchical methods, density-based methods, graph-based methods. All these different methods may have different definitions of clusters. That's why to study whether the data is clusterable or not is pretty hard. However, even we just fix the cluster type. For example, we just study the partitioning methods, it is still hard to define an appropriate null model for a data set. However, there are still some studies on clusterability assessment. For example, there are methods like spatial histogram method, distant distribution method, a Hopkins statistic. The general philosophies, they try to compare their measure with the measure generate from random samples, to see whether they are rather different. For example, for spatial histogram method is try to contrast the histogram of data with the histogram generated from the random sample to see whether they are rather different. For distance distribution is try to compare the pairwise point distance from the data, and the pairwise distance from the randomly generated samples. Hopkins Statistic is to use a sparse sampling test for spatial randomness. And since they carry quite similar spirit we will only introduce one spatial histogram method. For spatial histogram approach, the general philosophy is we try to construct a d-dimensional histogram of the input. For example, this is the two d we may constructed two dimensional histogram for the input dataset D. With the histogram generated from random samples. That means we want compare the figure generated from the left and that generated from the right figure. Okay, the data is clusterable if these two histogram distributions are rather different. That means they really generate very different distributions, then likely this dataset is clusterable, okay? The concrete method is we try to divide each dimension into equal waste bins. Then we try to see each cell here, how many points are in the cell. Now we can generate an empirical joint probability mass function, okay? Then for the random generate data sets, we'll do the same. We'll generate empirical joint probability mass function. Then what we will do is we try to compare whether these two, they differ quite a lot. That means we're going to compare how much they differ. The typical method is we use KL divergence. That means we calculate the KL divergence value. The KL divergence is Kullback-Leibler divergence is defined based on some formulas. I'm now going to introduce the detail of KL divergence. If you want to learn more, you can check any textbook in statistics or mission learning or data mining. In general they introduce this measure. Then you'll find if they are very different. Then, from the random samples, then you will say this data set is clusterable. Finally, I'm going to introduce a few textbooks, chapters, or general papers, that discuss the clustering validation measures. Especially what he shows this way is a summary of many research papers. And Zaki's book gives a lot of details on other measures we did not cover in this lecture. Thank you. [MUSIC]
[SOUND] Now we get into clustering evaluation, measuring clustering quality. Measuring clustering quality is an important issue just because clustering is unsupervised measure. We want to evaluate the goodness of clustering results by either some internal or external measures. Unfortunately there's no commonly recognized best suitable measure in practice. But there are three categories of measures we call external measures, internal measures and relative measures. For external measures, we can consider they are supervised, employ criteria not inherent to the datasets itself. That means we may have some prior or expert knowledge. For example, some ground truth. Then we can comparing the clustering results against the prior or expert specified knowledge, using certain clustering quality measure. Then the second kinds of measure are called internal measure, which is unsupervised. That means the criteria derived from the data itself. In that case, we will evaluate the goodness of clustering by considering how well the clusters are separated and how compact the clusters are. For example, we can use silhouette coefficient. The third one is a relative measure. That means we can directly compare different class rings using those obtained via different parameter setting for the same algorithm. For example, For the same algorithm, we use different number of clusters. We may generate different clustering results. We may want to compare those results to, for example, we can use silhouette coefficient to see how nicely, by setting a certain number of clusters, the clusters are better separated and you, within each cluster they are more compact so this is a relative measure to comparing different parameter setting. [MUSIC]
[SOUND] In this session, we will give a general introduction on External Measures for Clustering Validation. We know clustering is unsupervised. In this sense, we really want to use some good judgement. External methods is to give some expert knowledge or some prior truths. So to that extent, we call these as ground truth T. So given ground truth T, then we want to measure the quality of clustering C, we use the quality measure Q(C, T). The function Q(CT) is good if it satisfies the following four essential criteria. The first one called cluster homogeneity. That means we want the cluster formed to be pure. That means they're in the same cluster. So, the purer the better, of course. The second one, called clustering completeness. That means we want to assign objects belonging to the same category in the ground truth to the same cluster. That means you may get all these object are a category a, but you assign them to two, to three clusters. That's no good. We want to be complete. The third on is rag bag better than alien. Simply says, you may have some heterogeneous object. 'Kay. If you put them into a pure cluster, you mix the other category up. That's no good. You want to penalize this. It's better even to put them into a rag bag. Rag bag means it's in the miscellaneous or other category. It's better than you mix up with some pure cluster. Then the fourth measure of course, small cluster preservation, simply says, if you got a pretty small cluster already, you do want to further split them into pieces because those pieces likely represent noises. In that case, you better split a larger category into smaller pieces. The reason is, the larger category likely, they may belong to different clusters, or different other smaller categories. That's small cluster preservation. Then, we will see what we are going to examine on those external measures. We often use this graph, this color graph. Those brown points, this dark brown points, suppose they are ground truth partitioning T1. That means expert label those points as T1, okay. Then the ground truth, the other ground truth may be they labeled as blue one. That means T2 dark blue one, actually is another category. 'Kay. That means for ground truth, for expert, they know that different colors belong to different truths. 'Kay. However your clustering algorithm may generate things circled by the light brown one or orange one, or the red one. That means they are not ideal, they may not be good enough for the matching what expert judgement. So, we want to measure whether your clustering method is good. 'Kay. So, the measure one we call matching-based measure. We, we have sev, several measures, like purity, maximum matching, or F-measure. Another kind of measure called entropy-based measures. There are conditional entropy, normalized mutual information, NMI, or variation of information. The certain kind of measures called pairwise measures. For example, we may have four possibilities, true positive, false negative. False positive, true negative. Okay. Then, we actually have Jaccard coefficient, Rand statistic, Fowlkes-Mallow measure. The fourth category called correlation measures. Essentially, they are discretized Huber static and normalized discretized Huber static. And those marked as to be covered. We will discuss them in more detail in the subsequent sessions. [MUSIC]
[MUSIC] Hi! In this session we are going to discuss the first group of external measures called matching-based measures. For matching-based measures we will first introduce purity versus maximum matching. We still use a similar notation, suppose we have ground truth we get three categories. T1 actually marked as the brown points. T2 marked as a green points. T3 marked as black points. These are the ground truths. Then the clustering we finally got is, cluster C sub 1, actually it's marked as the ellipse of the orange color, okay. C2 marked as surrounded by the red color. And C3 is surrounded by the black color. So what is a purity? Actually quantifies the extent that cluster C sub i contains the points only from one ground truth's partition. Suppose we have R clusters the finer we get. So for purity sub i actually is you first count how many points in this cluster then you try to see the maximum number of the ground truths here. The maximum number of ground truths actually is the black points. You will see the max number is one, two, three, four, five, six, seven. So you get this one seven, but the total number is nine, so you get a purity of seven over nine, okay. Then what is the total purity of the whole clustering, C? Essentially this total purity is, each one's purity. Suppose you have R clusters. Each one's purity get their corresponding proportion. So you add them together, so that's the total purity. Or you can transform this formula easily into this one, okay. Of course, the perfect clustering is in the case when the purity is one, that means everything is very pure, you add them together, it's one. And the number of clusters you obtained is the same as the number of groups in the ground truths. Then we look at these two tables. Let's first look at the green table. The green table, the purity 1. That means the first cluster, you get a maximum of 30. So the purity is 30 over the total number of points. It's 50 so you get 30 / 50. Purity 2, obviously you get 20 / 25. Then purity 3 is 25 / 25, it's really 1. Then the total purity for the whole clustering is, according to this formula, we'll get a maximum y is (30 + 20 + 25) over the total number of points, you get 0.75. Interestingly, if you look at the orange table, orange colored table, you will get the exact number of the same thing is 30 over 50, 20 over 25, 25 over 25 and finally to get the purity is exactly the same formula. But you may find one undesirable thing for this orange colored table because if you can see, the ground truth T2 total we get 50, the actual was partitioned into two clusters, C1 and C2. That means both C1 and C2 would say, we belong to ground truth T2. But on the green table you probably can see, it's different, because C1 more pair with T3 and C2 more pair with T2. So that motivates people proposed maximum matching. That means one cluster can only match to one partition. If we have this rule then we have to find a pairwise matching. That means when we look at this we look at the weight, we will allow these element only belong to one cluster. The ij and ij. The ij must be exactly the same then if this element belonged to M then it only belonged to one. So then we need to look at the what is maximum weight matching. The maximum weight matching simply says you look at the whole group and then you look at all over n points you try to find the final matching together should be maximized. For example if you look at a green one the max matching is 30 parting into T3, T2, pairwise with C2, and T1 pairwise with C3, okay. That's why green's maximum matching is equal to purity is 0.75. However, for this orange color table, you probably can see is if you assign T2 to C1. Then you can assign T2 to C2 then you have to assign T3 to C2 then finally you get this matching 0.6. However if you assign, T3 to C1, T2 to C2, T1 to C3 you will get matching as 0.65 to that extent this assignment is better because the maximum matching, the value is bigger. So that's the reason for the maximum matching, you do have restriction, you have the pairwise assignment. Then, another popular use matching-based measure called F-measure. F-measure has been very popularly used in information retrieval. F-measure is essentially computed by precision and recall, let's look at how the precisioning is defined, okay. The precision define is for this particular matching what you want to see is for precision T sub i what you want to find is what's a maximum y matching this particular cluster then divide by the number of total number of points in this cluster so in that sense your precision actually is the same as purity. That means if you get a fraction of C sub i from the majority partition of C sub j. Then the j's partition contains a max number of points from C sub i then you get this formula. For this formula, you get this the precision and the purity, actually this one is the same. You will get 30/50, 20/25 and 25/25. Okay, that's the precision, what about recall? Recall actually is the fraction of the points in the partition shared in common with cluster C sub i. Simply says, if you assign T3 to C1, what you want to see is you've got a 30 but T3 actually got 35 granted you only capture 30. That means you're recall is 30 over 35. That's exactly, you probably can see the formula. Similarly, for the second partition if you assign ground truths T2 to C2, what you can see is, you only get 20/40 so your recall is 20/40. Similarly for the third one when you assign the ground truth of T1 to C3 you get 25/25. That means the fraction of the point in this partition share in common with the cluster C sub i, then you get this recall. Now what about an F measure? F measure actually is a harmonic means of precision and recall. So F measure for a cluster C sub i is the harmonic means of precision sub i and recall sub i. If you transform the formula will become like this one. Then for F measure for the whole clustering. What you need is you just need to average all the clusters the F measures, that means supposed you get r clusters then you will get F measure from 1 to r. You add them together divided by r, okay. So for this green table what you can see is, where you get F1 is because you got 2 times 30 divided by total is these two add together you get 85. Then for F2 you get 2 times 20 divided by these two adding together, okay. F3 of course is 1, no matter what you calculate it. Then you average all these three. You get F-measure this number. [MUSIC]
[SOUND]. In this session we are going to introduce another group of external measures called entropy-based measures. We know entropy is a very useful information theory measure. Not only is information theory also used in data mining, mission learning quite a lot. Entropy essentially is representing the amount of otherness of the, of the information in all the partitions. So we still use this graph, this figure to represent conceptually we have ground truths represented by different color points, we have clusters represented by different ellipses. For entropy of clustering C. Suppose we got R clusters. It's, it's essentially this is the entropy of every particular cluster. Then we add all these clusters together we get entropy for the clustering C. Then for each cluster, actually the entropy is based on the probability of C sub i. The probability of clustering C sub i is based on the number of points in this cluster divided by the total number of points. Then entropy of partitioning T is essentially defined a similar way. Suppose we have ground truths j from 1 to k, these k groups. Okay? So for each ground truth is, PT sub i we actually can get a, the probability of the ground truth as defined by these P sub t, sub i, okay. Then we add all their entropies together for all the K ground truths we get a ground truth for the whole partition. Then what we're interested in is the entropy of T with respect to clustering C sub i. That means we want to see how the ground truth is distributed within each cluster. So you probably can see this j represent the ground truth, and the i represents the really clusters. So we probably want to see such distribution so we can work out the entropy of T with respect to the cluster C sub i. Then if we want to get to the conditional entropy of T with respect to whole clustering C, then we just add all these together. Because this one is just for cluster C sub i, but we total have r clusters, we add all these up proportionally, then we will get the whole conditional entropy of T with respect to the whole clustering of C. Conceptually, we can see the more of a cluster's membership has split into different partitions, the higher the conditional entropy. That's the less desirable. You probably can see if, if you're partitions, wide spread into different clusters it is no good. Okay. For perfect clustering the conditional entropy value should be 0. The worse conditional entropy value, is log k. We can use a transformation formula like this. We can transfer on this conditional entropy To be the joint-entropy, minus the clustering's entropy. So, you probably can see, that's the conditional formula transformation. We're not getting to detail, but you can check it. Another very useful measure used for external measure is Normalized Mutual Information. We use a similar figure to sketch the idea. The Mutual Information is also defined in, information's theory, introduced there. But it's also very useful in machine learning and data mining. Okay. Essentially, the mutual information quantifies the amount of shared information, between the clustering C and the partitioning T. So you can probably see the formula, we have r clusters, we have k ground truths. So they are mutual information essentially so adding all these up together. This is a similar formula as entropy, but it's different because you can probably see this part is really, this ij's probability divided by this cross rings property and a partitioning's property. That means we want to measure the dependency between the observed joint probability p sub ij of C and T, and the expected joint probability of PC sub i and PT sub j. Under the independence assumption. Of course, if c and t are really independent that means they really want equal distance to this. Then you probably can see this is one actually when you take log this becomes 0, okay. Of course in this case, this is no good because it implies these ground truths actually scatter r around different clusters. However there's no upper bound on the mutual information, which is less desirable. That's why we need to introduce a normalized mutual information. That means we want to normalize the range from 0 to the highest wines one. Then the value close to 1 actually indicates a good clustering, a value close to 0 means they are almost accompanied to random independent assignment. Okay, then this normalized mutual information essentially is you take the mutual information. Divide by the entropy of clustering and divided by entropy of partitioning. You product them together, take their square root, or you can transfer in this way. So this is, quite useful, because you will know once you're clustering based on your external measure. Your cluster becomes perfect if their value is very close to one. [MUSIC]
In this session, we're going to introduce a certain kind of external measures called pairwise measures. What are pairwise measures? That means we take any two points. We see whether they agree between their cluster label and a partition label. So there are four possible cases for any pairs. We have true positive, false negative, false positive and true negative. Then we look at the cases. For example, you get any two points x sub i and x sub j. If they belong to the same partition T and they also in the same cluster C, in that case, this is a true positive. For example, we look at this case. The definition is the true positive is a number of such cases, okay? For example, for any two points x sub i and x sub j, if they have the same true partition label and they also have the same cluster label, that means they belong to the same cluster, then that's the case of true positive. For example, we just look at this case. For these two blue points, they belong to the same ground truth T2 and also they belong to the same cluster C2. That's true positive. Okay. Then what is false negative? False negative means they have the same ground-truth partition label, but on the other hand they are not in the same cluster. For example, you just look at these two brown points. They have the same ground-truth T1, but they belong to different, clusters. So that's the false negative case. Well, what is false positive? False positive means they actually have different partition label, but they are in the same cluster. For example, just look at this blue one and this brown one. They actually do not have the same partition label but they are in the same cluster, okay. What is true negative? True negative pairs actually means that they do not have the same partition label but they are also not in the same cluster. For example, if you look at this black one and this blue one. These two points, they do not have the same partition label, but they are also not in the same cluster. So that's a good case. Then we see how we can calculate the four measures. First, given n points in the data sets, the possible pairs you need to examine, actually the n chooses two, so that's the reason you have this formula. Then for the true positive case is you get all the, the partitions and the clusters, if they agree to each other, that's the case you have n i j, you choose any, these case you choose any two, you get that many cases. Then what is false negative? False negative means you get that many partition cases, but they do not belong to the true positive, okay. What is false positive? That means you have so many clustering cases, but they do not belong to the true positive. Then what is true negative cases? That means for all the cases, they do not belong to any one of the above three cases, then they are true negative. Okay, so that computation we can just simply use those formulas. With the introduction of true positive, false negative, these four measures, then we can calculate other measures like the Jaccard coefficient and Rand statistic. Then, we still take this figure as our illustrative example. Then for Jaccard coefficient, remember we define the Jaccard coefficient before. And this one is the definition for the pairwise measures. And this Jaccard coefficient have the similar kind of flavor. You probably can see. You have true positives divide by all the cases except that you ignore the true negative case, okay. That means the fraction of the true positive points, but after ignoring the true negative cases. Therefore, this computation, positive and negative are different to the, our asymmetric measure. However, for perfect clustering Jaccard coefficient should be one because, you know, all the cases you actually cover. You don't have those false ca, cases. For a Rand statistic is you take all the true cases, true positive plus true negative divided by all the possible pairs. Okay? That means you don't cover anything like a negative. Okay? That's why, if it's perfect clustering, Rand statistic should be 1. It is symmetrical because you take a positive and a negative case just, equally. Then there is another interesting measure called Fowlkes-Mallow measure. This measure is a geometric mean of precision and recall. Remember we just studied F measure, which is a harmonic mean of precision and recall. For geometric mean Fowlkes-Mallow measure, essentially is precision times recall, you get their square root. And once we introduce all these measures, if we wanted to calculate for example any table like this one this green table, we can use this measure to calculate all the numbers. We will leave this calculation as an exercise, instead of spending time lecturing here. [MUSIC]
[SOUND] In this session, we were introduce Internal Measures for Clustering Validation. Remember, we just discussed several kinds of external measures. The lucky thing for external measure is we do have [INAUDIBLE]. We have expert knowledge, we have a priors. Unfortunately, in many cases, we do not have [INAUDIBLE]. That's the reason we have to rely on internal measures. The internal measures based on the concept of clustering. It means we want the points within the same cluster. That means intra-cluster, they are very compact. They are close to each other and we wanted points in different clusters that means inter-cluster distance. We want them as far apart as possible. That means we want them to be far separated. That's the reason we want to maximizing the intra-cluster compactness and also maximizing the inter-cluster separation, but we want a tradeoff between these things. So let's look at how we define this. Suppose we give a clustering C with k clusters, C sub 1to C sub k. For each cluster C sub i, it contains n sub i number of points. Then we introduce the function W of S, R, which is the sum of the weights of all the edges with one verdicts in S, the other in R. For example, you'd get a two points. You say, this point is in this cluster. The other point is in the other cluster. You want to look at their weight. The weight usually can be defined like a euclidean distance or whatever you are using for clustering. Then based on this, we can say, the sum of all the intra-cluster weights over all clusters should be defined this way. The reason is you can see this is a C sub i, this is a C sub i. That means both vertices are in the same cluster, so that's why it's inter-cluster. But at one point A and the other point B, you actually use it will calculate twice, because A to B, B to A, you're actually calculating twice. That's why you have to divide this by two. Okay. For all the clusters sum up together is i from one to the k's clusters, then we define the sum of all the inter cluster weights should be calculated this way. There, which is very obvious, because one point, one vertex is in the cluster C sub i and the other one is not in the C sub i. Simply say, in the other clusters, that's why it's intro cluster. But since the two points, you calculate it twice, that's why you have the divide by 2. Then we sum up all these essentially, it's i from one to k, all the clusters. We get a Wout, okay? That's the inter-cluster weights sum them together. Now we also can see the number of distinct intra-cluster edges, because for i's cluster if n sub i number of points there intra-cluster link, you have end points choose two. That's why you get this formula. Then get all the clusters one to k, you sum up, that's why you get n sub m. Then for the inter-cluster N sub out is notated by one point is in cluster i, the other point is cluster j. They never are in the same cluster. So you want to calculate all the edges, among different clusters. So that's the number of distinct inter-cluster edges. Then for BetaCV measure is defined as Wi divided by N and you can think this is the mean intra-cluster distance and you get Wout divided by number of out, you can think is a mean inter-cluster distance. So the BetaCV essentially the ratio of mean inter-cluster distance versus the mean intra-cluster distance. Of course, for this measure is the smaller, the better clustering. Because a smaller means, this is pretty small is quite a compact. And this is the bigger, it means it's more separated. That's a BetaCV measure defined, then we look at some other definition. One popular use one called normalized cut. Normalized cut is defined as follows. The normalized cut of this ratio is defined by sum up of all the k clusters. For every cluster, you calculate in this way or more explicitly, we can interpret in this way. This is ways of the inter-cluster. This is a weights to sum up of, of the intra and inter-cluster weights. And for this ratio, you actually sum from i from one to k, you'll get these normalized cut ratio. Of course, based on this, you want to the inter-cluster to be far separated, comparing to the intro one. That's why the bigger normalized cut value, the better the clustering. Then there's another definition called modularity. This is especially popular for graft clustering. It is defined in this way. You proceed, this part is the same as normalized cut, cut formula. Okay. Like this, but this part is the square of this. Okay. So that simply says, this one is the observed the fraction of the weights within clusters. This is the expected one. So modularity, actually measured the difference between the observed one and the expected one. So the smaller the value, the better the clustering, because the inter-cluster distance are lower than the expected one. So this is the modularity definition used for graph clustering. [MUSIC]
[SOUND] In this session, we are going to introduce another kind of measures for clustering validation called the Relative Measures. We already introduced external measure when we have [INAUDIBLE]. We also introduced internal measure when we try to measure the intro cluster, they are very compact, and inter cluster, they are far apart. Now, we want to look at what is relative measure. Relative measure actually is trying to direct compare different clusters. Especially for those obtained using the same algorithm but different parameter setting. For example, like if we all use k-means to cluster a group of things but in that case, we want to measure for different cluster instantiation. Which one gave you the best clustering or a different case for k from one to certain number, which k gives you the best clustering. In that case, we were measured as using relative measure. And, one interesting measure called silhouette coefficient can be used for relative measure, okay? But, actually, silhouette coefficient also popularly used as internal measure. That means we will be able to use it to check cluster cohesion and separation. So, I first introduce it as an internal measure before I introduce it as an external measure. So for every point, x sub i, we can calculate its silhouette coefficient, s sub i, okay? How we calculate it? We use this formula this formula actually, mu sub n, is the mean distance to the point of its own cluster. Simply says, if we find this point in the blue cluster, okay? We're going to see this x sub i, actually to all the points in it's own cluster that distance we want to reach them. We get a mean distance that's mu sub n. Then what is mu sub out's mean, okay? That means we want to see this blue point to its closest cluster which is this orange one, to its closest cluster. All the points its closest cluster, we calculate their mean distance, that's this value, mu sub out mean, okay? Then what we want to see is their distance, you probably can clearly see for any point x sub i. You want that the internal distance, its rhythm is small but even to its closest cluster, this average reason be big. So, to that extent, this value usually should be positive and the more positive the better, okay? Then we, we normalize it, we divide by the maximum value between this mu out mean. And the mu n, and if we pick up the two, so it simply says that this division must be smaller than one. But on the other hand, if this cluster to one, simply says, it's better because the internal is very compact to the outside is the reasonably big. This is only for one point, what is for the whole clustering? Okay, so silhouette coefficient for the whole clustering is you take all the end points. You calculate every one's silhouette coefficient s sub i. I from 1 to n you calculate all the s sub i, you normalize it, you divide it by the number of points n. You get the whole clusterings silhouette coefficient. Of course, these silhouette coefficient, if it's closer to plus 1, it, it implies good clustering. Because the internal one to its own points is rather close to the other cluster, even to the closest one, is really far. So we also can use this silhouette coefficient as a relative measure. For example, we can use it to estimate the number of clusters in the data, say, what is the best of k. We can pick a value that yield the best at clustering, that means yielding the high value for SC and SC sub i, okay? For example, you can, thinking about, you did a lot of clustering, okay? For all the i's, you want to see which one give you the best clustering? That means which one gives you the highest silhouette coefficient? Then you can pick up that instantiation, pick that k. So that's the usage of silhouette coefficient as a relative measure. [MUSIC]
[SOUND] In this session, we are going to study another issue for clustering assessment called Cluster Stability. Cluster stability basically say, whether you got the right parameter, you get the very stable clustering results. That means you can assume you get the data set D, you take a sample get several datasets from the same data set D, you do the clustering. And you should assume if you're clustering finally get a very stable clustering results, that's a good clustering. Usually, you can use this to find a good parameter values for a given clustering algorithm. For example, you can find a good value of k, which is a good number of clusters. So how do you find this? You probably just look at this graph, you probably can see. If you set k as three or even as two, you probably can find it quite reasonable clusters. But if you set as four or five, no matter how smart you are, probably this cluster you find may not be quite stable. So to that extent, you may try to test the cluster's stability, we introduce one method called Bootstrapping approach to find the best value of k, judged based on stability. How can we do this? This Bootstrapping basically says, from D, you take tt samples of size n, but the, the sampling is sampling with replacement. Then it's every time you take almost from the same D, you take it t times. Then you get to the sample D sub 1, D sub to D sub j to D sub t. Okay. Then when you run the same clustering algorithm with k value from two to the maximum of the k you like. Okay. Then you can compare the distance between all the pairs of clustering. For example, for each k, you may try to see whether the D sub i and D sub j, these two different sample datasets. You will see whether finer you get the, the sample, you want to compute the expected pairwise distance for each value of k. That means suppose k you get a ten, you get at least ten clusters, you want to see their pairwise distance. Okay. Then what you can see is this value k, if it exhibits the least deviation between clustering. That means you do clustering those sample D sub i or sample D sub j, they have the least deviation. Okay. That k should be the most stable one. Thus, you may want to have this k value. That's one application to test the cluster stability to find the best k. Actually, there are many methods to find the k, the appropriate number of clusters. One method called empirical method, actually people give you this empirical formula. For example, for total data sets of n points, you may take the square root of half of this n points. Okay. For example, if n is 200, the expected value you will need to get number of clusters should be 10. Of course, this may work out for a small number of points. If you get a really big number of points, for example, you get a, too many points, then you get this value, you'll get a k as a solvent, probably you may not want to get a solvent clusters even for too many points. Another method, people use called elbow method. Elbow method means you tried to based on the number of clusters that go one, two, three, four. You get number of clusters, then you get it, the sum of the within square variance. That means you just to look at the average of within to cluster variance, you try to look at this to see, know what is the best, the number of k that you want to see the elbow point, the turning point. So that's one method. Another method of finding the best number k is use cross validation. That means you divide a given dataset into m parts. So you take one part as a test data, the remaining part to do the clustering. Okay. You can do this m times, so you can check their overall quality of the clustering. Then how we test the, the quality of the examples? Okay. What we do is, is, supposed we use this m minus 1 parts to do the clustering, we find the k clusters. Then for each point in the testing set, you try to find the, it's closest centroid. Then you, based on this, you try to find for all the points in the datasets, you try to find the sum of the square distance. That means you try to find some of the square error SSE, as we introduced. Then usually, you try to see whether this, you get the best fit of the test datasets. That means you get the smallest sum of square distance. Since for cross validation you repeat this m times, so then what you can do is you can compare the overall quality measure with respect to different k's, then you'll find what is the best number to fit the data best? That means you get the overall quality measure, you get the lowest SSE for this particular k. Usually, this is the right number of clusters. [MUSIC]
