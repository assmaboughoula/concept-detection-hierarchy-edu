[SOUND]
So, let's first discuss frequent
pattern mining in Data Streams. We know in current big data era,
besides we get a huge amount of data stored in database
systems, in file system, on the web, but also we have internet of things or
internet of sensors. So, in those kind of scenarios,
there are lots of stream data. The stream data, the feature usually
is the come in and go continuously. They are ordered in the sequence but
they are changing dynamically. They come and go very fast but
in very huge volume. This is very different from traditional
finite persistent data sets stored in the file systems, in database
management system or on the web. So, now let's look at the major
characteristics of data streams. First, data streams are usually repeated
huge volumes of continuous data. They could be potentially
infinite like the online sensors, they have no ending at this part. So they are fast changing,
they may also require faster rear time response like
anomalies or emergencies. And then for data stream usually
captured nicely of our data processing needs of today because even in
data systems, the data could be so huge. You may only want to scan them in
a sequential manner live data streams or you don't want to keep on repeating,
getting them back again. Okay, so to that extent that even in the large
data stored you may also process them. In the data stream manner, not to say you really have to
enter large amount of sensor data. So, in general, we say for processing
data streams, random access is expensive. Because of the data come and go,
you don't want to fetch back, or you can not fast forward as well. So we usually call these algorithms
developing data streams, called single scan algorithm. That means for any particular data set, particular page,
you can only have one look. So another interesting thing is
the data stream in many cases you may like store some kind of sketch or
store a summary of the data seen so far. Instead of storing the detailed data
because you don't have such volume or such processing power. Then another important
challenge is most data streams actually are coming at the very low
level like the particular sensors. And they also come in
the multi-dimensional feature, for example, even for the data they may come. Some part would be the temperature,
some part it could be video, some part it could be audio,
some could be the text data. It's multi-dimensional in nature but for the information processing needs,
usually you want summarize or process the data and pattern in a
multi-level and in multi-dimensional way. So we can see there lots of research
challenges data stream processing. If we look at the Architecture of data stream processing system usually we called
this as Stream Data Management System. What we have is we have
a stream query processor. You'll have multiple data stream
streaming into this processor at the same time in a continuous way. But even for User/Application programs they usually post Continuous Query
rather than ad hop words. It's like a [INAUDIBLE]. They won't see anything abnormal. They want to summarize the data
in a multi-dimensional space. Then for this Stream Query Processor
usually you have some scratch space that could be a big main memory,
or even plus some disks. These markable streams coming down here,
where be summarized the processed, especially answer your continuous query. The results will be streaming back
to the user's application programs. So this is a typical way
to process streaming data online in the real time, okay. The problem becomes for
frequent pattern mining. How can we find the frequent
patterns in these data streams? Actually, it's pretty challenging. When we look at the major
differences between stream mining versus stream querying. The first thing is stream mining and steam querying,
they share many common difficulties. For example, you can only do single-scan,
you need a fast response, you have to handle dynamic, noisy data. But for stream mining, usually you
want to see the global picture, you want to see patterns,
you want to find the clusters. It also requires less precision
than the stream querying because stream querying one the ping
pong to a particular point. You may need to perform join,
grouping, sorting which actually in streams join and sorting those
algorithm is actually pretty difficult. Because you cannot see the pass data,
you cannot predicted the future of incoming data
as well but patterns are hidden. They are more general than querying,
so their processing is different. Actually, for stream data mining,
there are lots of research and development activities already. One branch actually studied
pattern mining in data streams. Another is doing the multi-dimensional
on-line summary of data streams and also data stream can be
clustered dynamically. Can do dynamic classification,
can find outliers and anomalies. So there are lots of research in
data mining, all these frontiers. So what we are studying in this lecture, we will only touch pattern
mining in data streams. So one important thing is for
mining frequent patterns. In stream data it is unrealistic to try
to find a precise frequent patterns. Simply says the past precising
may already been gone, it's pretty hard to capture them. The future one may not be coming yet. So it's pretty hard to predict them and even further why you can hold
just because they are so big you cannot store them even in
a compressed form like FPtree. So what we expect is trying
to find approximate answers but in many cases, approximizer may be
sufficient for our analysis purpose. For example, you may find a router could
be interested in finding the flows. Those flow in the network you
may find whose frequency is at least 1% of the entire
traffic stream seen so far. If you identify those patterns, those frequent occurring flows
you could be pretty happy. On the other hand you may say for this sigma if you give me the count
is probably a little less like over 1/10 or
you would say the error rate is 0.1%. You probably feel is very comfortable,
you think you're doing good thing already. In that case, we may develop
some very efficient algorithm to mine such patterns with
good approximation like this. Then in this lecture I'm going to only
introduce one algorithm around this which is called Lossy Counting Algorithm
developed by Manku and Motwani in 2002. The major idea is not
to keep all the items especially not to keep the items
with very low support count. That means if they are very low they unlikely will reach the frequency
[INAUDIBLE] you do not even keep them. Okay, the advantage is you can
guarantee some error bound. That means I [INAUDIBLE] find all
the frequent items but of course you need still to keep really large set of traces,
your buffer size should be really big. Let's look at Lossy Counting Algorithm but
we only introduce frequency single items. They did study multiple item sets for
the simple explanation of the idea refers to the frequent
single item counting, okay. Suppose you get a very,
very huge data stream, you may divide this stream into buckets. But the bucket size
could be one over epsom, because if your epsom is 0.1% error bound. Then the bucket size could be 1 over
epsilon means, 1 over 0.1% is 1,000, means each bucket should have 1,000 items,
okay. Then this one item we
assume that the main memory is size is big enough,
you can easily hold this one bucket. Then at the very beginning,
when the first bucket comes, the main memory, at the very beginning
you can assume for this part is empty. This summary is empty but then you get
the first bucket you get 1,000 items. Then you start counting them for
example, there are four red ones, there are two yellow ones, one green one,
one blue one, black and so on. So at the end of this bucket boundary we will decrease all the counters by 1,
you probably can't see. There are many if they only appear
once like the green one, a blue one, the black one. They all gone, because it decrease
a counter by one, a counter is zero, they all gone. Even for red ones you get four,
now the counter becomes three. You get two yellow ones,
the counter actually is only one. So that means you your counter
actually get a less than the real one. When the next bucket of the stream coming,
you'll probably empty a lot of space because the counter
was not existing there too. There are too few items in this color. Then, you may get additional red ones and
yellow ones or even black ones. So you can see at the end of this bucket,
then you will do the same. You decrease all the counters by one. So those, not frequent,
they are gone again. So the yellow why you can see,
originally you got a one. You get one more but
you decrease by one and so you keep one. The black one is one, the red one you get a little more
because the red one is so frequent. After you know youâ€™re thinking bucket by
bucket you get many, many buckets you still keep this counter 1,000 counters. Because you get 1,000 buckets
at most you have 1,000 counters you only get a number so
the space is actually quite limited. Then finally we want to
output the frequent patterns. The interesting thing is whether
we will be able to output all the frequent patterns if they
are frequent in these data streams, but maybe we have some reduced count. So, actually,
suppose the support threshold is sigma, the error threshold is epsilon,
and the stream length is N so far. Then, the output are those items
with frequency counts exceeding sigma- epsilon times N. Simply says if for
any item you find it is conquered, was this number or
bigger your output is a frequent item. So the key is because
end of each bucket we decreased the counter by 1, so
we were undercount something. So the question becomes,
how much do we undercount? So if the stream length seen so
far as N, the bucket size is 1/epsilon. We can easily determine
the frequency count error should be no more than number of buckets,
why? Because for each item for every bucket you decrease
the count by what? If some item come, even later,
at the very beginning they did not come, you did not decrease them. So to that extent you'd decrease
less than number of buckets, so if they come even at the very
beginning, the first bucket is they are on the way to create the count
by number of buckets. What is the number of buckets you see? Because you have seen N elements and the bucket size is one over it's epsilon. So N atom is over bucket size is number of
buckets which is N over 1 over epsilon. So you get epsilon times N,
that means that the frequency count error
at most is epsilon times N. We know epsilon is pretty small, so
the error count error is not that big. Then we look at the counting. They have some interesting property
we call approximation guarantee. The first thing is there's
no false negatives. Simply says if the item is frequent, it will be captured an output, why? If we can see,
is if your item is frequent, that means your total support threshold
should be sigma times N or more. Then, because we output a frequent
item exceeding sigma-epsilon times N, we know we already undercount
this part at most. That means actually since you output
all these you will output every item whose support count is no less than sigma
times N so there's no false negatives. But there could be false positives
where the false positive comes just because probably in
sometimes they come later like you may have one
color okay suppose orange. They come at a very late stage. Okay so they actually
the previous increment by one actually it did not tax them. So now they really have this support, sigma minus epsilon time N. You actually offer this,
say this one could be a frequent item. Yep, okay but this orange one. Actually, you, the real support is this. They do not suffer the decrement or
only suffer one. So then your support count, this one, actually in principle is
not frequent in rear. But you think they are frequently
just output as a frequent item. So the false positive have this
two frequency a list of this so that means the orange wire have
the two frequency of this one. You still count them as a frequent item so
you do have some false positive. But a frequency count underestimated
by at most epsilon times N, because we already have this. So, you have some nice guarantee for, finally you have just this
bucket size as your main memory. You actually can return
all the frequent items and it may contain some false positives. It may have some underestimate
of your frequency count. But still it is a nice, elegant algorithm with very limited
resource you can handle data streams. Of course, after this there
are lots of studies by improving, for example,
improving lossy counting algorithm. One interesting study
is by Metwally in 2005, they do space-saving computation
of frequent and top-k elements. The general philosophy is,
they may tried to use more memory space if you
really have bigger memory. You don't have to restricted down to one
over epsilon as your bucket size but that we are not going to
discuss this in more detail. Another interesting thing is in this lecture we only discuss
frequent one item sets. That means frequent item. We discuss how to mine
the approximate one. Actually in Manku and Motwani paper, they also discussed frequent k-itemsets. How to maintain counter,
how to do Lossy counting. But due to limited time we will
not introduce this algorithm. You may like to read
the paper by yourself. Then there are also some
subsequent studies on how to mine sequential
patterns in data streams. It is a pretty challenging task as well. So I'm going to only
introduce you two papers, one is Manku and Motwani's paper,
another is Metwally's paper. You may want to see how to my streams streams in a very interesting way. Thank you. [MUSIC]