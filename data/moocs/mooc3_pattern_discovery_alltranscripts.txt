Hi, welcome to the course Pattern Discovery in Data Mining. Here I'm giving you a general overview of this course. This course is one of six courses being offered by the University of Illinois at Urbana-Champaign, as a data mining specialization series. Data mining specialization consists of six courses. Data Visualization, Text Retrieval and Search Engines, Text Mining and Analytics, Pattern Discovery in Data Mining, Cluster Analysis in Data Mining, and finally, Data Mining Capstone. This final Data Mining Capstone is a project oriented courses using the knowledge you learned in the first five, six courses. So first you may wonder, what is pattern discovery? So I'd better first give you example. Considering massive shopping transaction data, pattern discovery may help you answer the following questions. What groups of items are frequently bought together? If a person buys diapers at night, what is possibility of this person buying beer as well? If a customer buys an iPhone 5 or iPhone 7, what other electronic products will the customer be most likely to buy in the next three months? So you probably can see pattern discovery is quite interesting. What is the value of pattern discovery? Pattern discovery may help you find the hidden and inherent data patterns in massive data. Pattern mining will play a unique and critical role in mining massive data. What roles does pattern discovery play in this particular data mining specialization series? In this course, you will learn scalable methods to find patterns from massive data. The patterns maybe the set of data items strongly correlated to each other. You will learn how to mine a large variety of patterns. You will also learn how to evaluate the patterns. And finally, you will find the pattern discovery may help classification, clustering, and many other data mining tasks. Pattern mining has very broad applications. We already see it may predict shopping transaction data. For example, for a customer who buys products A and B, what is the likelihood the customer will buy product C as well? It may predict a web click streams for example, based on the current data you may want to see which webpage is most likely to be clicked the next. It may help mining software bugs. Where is the likely bug in this program? It may identify objects or sub-structures in images, videos, and social media. It may help finding quality phrases, entities, and attributes in massive text data. It may help repeating DNA and protein sequences in genomes. It may help find hidden communities in massive social network. There are several major reference readings for this course. We will have a textbook, the textbook is written by me, Micheline Kamber and Jian Pei, published in year 2011, called Data Mining Concepts and Techniques. This is the third edition published by Morgan Kaufmann. The following three chapters are most related to the course. Chapter 1, Introduction. Chapter 6, Mining Frequent Patterns, Associations, and Correlations, Basic Concepts and Methods. Chapter 7, Advanced Pattern Mining. Other references will be listed at the end of each lecture video. The course has the following structures. We have Lesson 1: Pattern Discovery: Basic Concepts. Lesson 2: Efficient Pattern Mining Methods. These two lessons were from Module 1, which usually you can finish in one week. Then we have lesson 3, Pattern Evaluation. Lesson 4, Mining Diverse Frequent Patterns. These two lessons were from Module 2. We have lesson 5, Sequential Pattern Mining. Lesson 6, Pattern Mining Applications, Mining Spatiotemporal and Trajectory Patterns. These two lessons form Module 3. We have lesson 7, Pattern Mining Applications, Mining Quality Phrases from Text Data. And lesson 8, Advanced Topics in Pattern Discovery. These two lessons from Module 4. For the general information about this course, I am the instructor. I'm a Professor in Department of Computer Science, University of Illinois at Urbana-Champaign. We will have some teaching assistants who will help me editing this courses and in the meantime, giving some quizzes and program assignments. The course prerequisite is as long as you are familiar with basic data structures and algorithms, you will have no problem to take this course. The course will be assessed by the following three measures. We have in-video questions. At the end of the lesson, we have lesson quizzes. We also will be giving two programming assignments. We have one required programming assignment. The second one is an optional one, it is not required for passing the course. Hope you will enjoy this interesting course. Welcome. [MUSIC] [SOUND]
[SOUND] The first session we're going to discuss, what is Pattern Discovery. Why it is important? To know why pattern discovery is important, we need to know what are the patterns. Okay. Patterns, actually are a set of items subsequences or substructures that occur frequently together in a data set we call this strongly correlated. Patterns usually represent intrinsic and important properties of data. That's why Pattern Discovery is the process try to uncover, try to mind patterns from massive data sets, it has many interesting applications. For example, in transaction database you may want to find what kind of products are often purchased together? Now you can do some target marketing. And for customers, you may want to study if a person buying an iPad, what are likely other products he or she is going to buy in the future? And even for software engineering, you may want, based on a co second analysis, to try to find copy-and-paste bugs. For text analysis, you may want to see what are likely the keywords were from phrases, finding phrases may need pattern mining as well. We can see Pattern Discovery could be very important because it is uncover inherent regularities in the data sets. It forms the foundation for many things. For example, association, correlation, and causality analysis, mining sequential structure patterns, pattern analysis is spatiotemporal data, multimedia data and stream data. Even for classification, ff we use discriminative native pattern-based analysis, the classification could be more accurate. And for cluster analysis, pattern-based subspace clustering could be an important direction for cluster analysis. So there's no wonder why we want to study Pattern Discovery. [MUSIC]
Let's first introduce some basic concepts: frequent patterns and association rules. We first look at this simple transaction example. There are five transactions, 10 to 50 are the transaction IDs, and these are the sets of items they bought. For example, transaction ID 10 contains beer, nuts and diaper, which form an itemset because it is a set of items. And, for this particular line, it is 3-itemset because it contains three items. And for each itemset, you may have a concept of support. Support means, in this transaction dataset, how many times beer happens. In this particular case, there are three occurrences of beers in this transaction data, so this support count of beer is three. But you also can use relative support; that means the fraction of the transactions. For example, there are a total of five transactions, three of them contains beers, so the relative support is 3/5 or you can say 60 percent. So, we may see whether an itemset X is frequent or not if X, the support of X, pass a minimum support threshold. For example, if we set the minimum support threshold is 50 percent, then we can see the frequent 1-itemset in this datasets, you will find they're four. Like beer, you can see they're three cases, this absolute support is three, the relative support is 3/5, is 60 percent. OK. But for frequent 2-itemsets, you may check it. There's only one. OK. Because it's a beer and diaper, they happen together 3/5, that's why we get this. But none of the other, you know, itemsets they pass this 50 percent threshold, so there's only one. And from the frequent itemsets, we can introduce an interesting rule, association rule that implies, for example, X implies Y simply says if people are buying X, what is the support and confidence people will buy the itemset Y? OK, then S is the support, which is a probability X and Y contained together in this rule set. OK. Then c is a confidence, which is conditional probability. That means if the transaction contains X, what is the probability it also contains Y? So, for this probabilistic computation, you can use support X union Y divided by support X. That means take the whole rule support, divide by the left-hand side. You may see this notion X union Y. This is a set union. But if you look at the Venn diagram, actually transactions containing X could be this part; transaction containing diaper, like beer, is this part of diaper, is this part; containing both actually is their intersection, OK, the intersection of the events. OK. But from the itemset point of view, X is the transactions containing both X and Y, both beer and diaper. You will say this one were count, that's why we use not X intersects Y but X union Y. OK. If you think X is beer, Y is diaper, X intersects Y will be empty, X union Y means it contains both. Then, for association rule mining is actually try to find all such rules which pass a minimum support and a confidence threshold. We already know if we set minimum support is 0.5, we're find these are the frequent 1-itemsets, these are the frequent 2-itemsets. From here, if we set minimum confidence is 50 percent, we're going to derive two association rules. Because these two rules, if you use this computation, beer and diaper getting together is three, in here, then it's 60 percent because 3/5. Then, if every time beer occurs, you can see diaper also occur. That's why the confidence is 100 percent. But for diaper implies beer, you can see the diaper support is four, beer support is three, so that means there's only 75 percent of probability the customer buying a diaper likely will buy beer. Are there more rules in this one? Actually if you check this, because this is only frequent 2-itemset, these two are the only associated rules that can be generated from this transaction data.
Now, we are going to introduce another set of important concepts, closed patterns and max-patterns. Actually, in association rule mining or frequent pattern mining, there is one challenge is, in many cases, we may generate too many frequent patterns. Usually, a long pattern will contain a combinatorial number of sub-patterns. Just give you one simple example. Suppose we get a transaction database. It contains only two transactions. T_sub_1 and T_sub_2. For T_sub_1, it contains only 50 items, a_sub_1 to a_sub_50. For transaction t_2, it contains 100 itemsets, a_sub_1 up to a_sub_100. Suppose we set the minimum support, the absolute minimum support is only one. That means everything is frequent. Then, how many frequent items it contains? Let's have a try. So, for frequent 1-itemsets, we have a_1 occurs twice, a_2 occurs twice, up to a_50 occurs twice, then a_51 occurs only once up to a_100 occurs only once. Then what about 2-itemsets? Actually, 2-itemsets is all the possible combinations of a_1 to a_2, a_1 to a_3, up to a_99 to a_100. You probably can see some are support two, some support is only one. OK. Then we can go on and on up to 99-itemsets, we again get a_1 to a_sub_99, the support is one. We can chop one by one, so finally you get a_2, a_3 to a_sub_100, support is one. Finally, we're get a 100-itemset, where support is only one. If we add all these together, that means 100 choose 1, 100 choose 2, up to 100 choose 100, we would get a huge number, 2^100 -1, that many sub-patterns. This is a huge set. For any computer, it's too huge to compute or store. Then, how can we handle such a challenge? One interesting proposal is introduced a concept of close patterns. What is close pattern? We say X is closed if X is frequent, and there exists no super-pattern Y with the same support as X. Just give you a simpler example. We look at the same transaction database, like T_1 and T_2. We still say minimum support is one. So how many close pattern does this transaction contain? So instead of 2^100 - 1, we actually find only two, a_1 to a_50, support is two; a_1 to a_100, support is one. You may say, wait, you may lose something. Let's look at it. Do you know a_1 to a_49? Actually, it's contained here; it must be support of two. Do you know a_1 to a_51? It contains here, support must be one. So you can see you do not lose anything because you got the concrete patterns which are the maximum close to one. In the meantime, you do not lose the support information for any sub-patterns. So to that extent, we can say closed pattern is a lossless compression of frequent patterns. It does reduce the number of patents but does not lose the support information. For example, you will still be able to say a_sub_2 to a_sub_40, the support is two. a_5, a_51, this itemset support is one. Then, let's look at another possible compression called max-patterns. Max-pattern, the definition is very similar except it does not care the support anymore. That means X is a max-pattern if X is frequent and there exists no frequent super-pattern Y which is a super-pattern of X, and we don't care support anymore. So the difference from the closed pattern is we do not care the real support of sub-patterns of a max-pattern. Let's look at the transaction database like this. OK. We still say minimum support is one, then how many max-patterns does this transaction database contain? So we will find only one. The reason is, even a_sub_1 to a_sub_50, the support is two, but remember, we do not care support anymore; for this a_1 to a_sub_50, it does exist, a super-pattern like this, OK, which is also frequent. To that extent, this is a further compression, so you only get a one pattern, but it is a lossy compression because we only know a_sub_1 to a_sub_40 is frequent, but we will not know the real support of a_sub_1 to a_sub_40 and many more. So, you do lose a support information of many frequent itemsets. OK. To that extent, in many applications, mining closed patterns is more desirable than mining max-patterns. We're going to see this in our future discussions. So for this lecture, we mainly give you four recommended readings. The first one, actually, is the first time introduced, frequent patterns and association rules. The second one is the first time introduced, max-patterns. The third one is the first time introduced, the closed patterns. And the fourth one actually is an overview, give you overall, you know, this field, how the frequent pattern mining came and how it evolves up to this stage. Thank you.
[SOUND] We are going to introduce a very important property of frequent patterns, which is called the Downward Closure Property of Frequent Patterns. Let's look at this simple transaction database TDB 1, it contains only two transactions, T sub 1 and T sub 2. Suppose we get frequent itemsets a1 to a50, then we actually can clearly see all its subsets like a1, a2, or a1, a2 as a item set, they're all frequent. Then you may wonder, there must be some interesting hidden relationships among different frequent itemsets. Actually there is a one called downward closure property of frequent patterns which is also called the Apriori property. Okay, now let's look at this. Suppose we know {beer, diaper, nuts}, this itemset is frequent. Obviously, beer and diapers should be frequent as well, because any transaction which contains beer, diaper, and nuts must also contain beer and diaper as a itemset. That's why the beer, diaper as an itemset should be at least as frequent as beer, diaper, and nuts. So we can easily derive this property so that any subset of a frequent itemset must be frequent, if we keep the minimum support ratio as the same. So in that context, we can derive a efficient mining methodology. The general philosophy is, if you find an itemset S, any of its subset is infrequent. Then, there's no chance for S to become frequent because based on this Apriori property, then we do not even have to consider to mine S. This actually turns out to be a sharp knife for pruning. So, this Apriori Pruning, based on this, it generates quite a lot of Scalable Pattern Mining Methods. So the first Apriori Pruning principle was discovered by Rakesh Agrawal and Srikant in VLDB 1994. [INAUDIBLE] Mannila in KDD'94 workshop also generates a similar methodology. The methodology generally says if there's any subset, any itemset which is infrequent, then its superset should not even being considered and not even being generated. Based on this, there are three major approaches develop in subsequent studies. One essentially is Apriori, the first representative work was published in VLDB 1994 called level-wise join-based approach. Another method was developed by Zaki and [INAUDIBLE] and what they got called Eclat is based on vertical data format. Then the cert approach is essentially pattern based. It's frequent pattern projection growth is pattern growth approach. Called api growth, developed by us in year 2000. [MUSIC]
[SOUND] Hi, let's introduce the very famous Apriori Algorithm. This algorithm is the first candidate generation and test approach for frequent pattern mining. It is a level-wise candidate generation test approach. Initially, the first time you just scan the database once to get frequent 1-itemset. Then taking this frequent 1-itemsets, you are going to generate lens two candidate itemsets. In the case iteration, you're going to take a length-k frequent itemsets to generate landscape plus one candidate itemset. Then you go against the database to test these candidates generated, and to find the real frequent k+1 itemsets. Every iteration you set k to k+1, so you can go and tier the no frequent itemsets will be generate, or no candidate itemsets can be generated. Then after you exit from loop, you just return all the frequent itemsets derived, that's the algorithm. Let's look at the pseudo-code. We set C sub k to be candidate itemset of size k, F sub k to be frequent itemset of size k. Initially, we just get a frequent 1-itemsets. Then we get into the loop. Suppose the frequent k itemset is not empty, then we get in. We use ks frequent itemset to generate k+1's candidate itemsets. Then we go against the database using the minimum support to see which k+1 candidate itemset or frequent. We derive the frequent k+1 itemsets. Then we reset k to k+1, and here we get out of this loop. We just return all the frequent k itemsets for all the ks derived. Let's look at the concrete example. You look at this transaction database. It contains only four transactions. The first scan, you just try to find their support for every single itemset. Then we find d, the support is only one, so it's not a frequent. So then we just remove it, we get a frequent 1-itemsets and their support. Then we use this to derive the candidate 2-itemsets, C sub 2. Then we scan the database again, we find the real support count. Then we find these two blue marker lines are infrequent, we derive frequent 2-itemsets. Then using frequent 2-itemsets, we derive the frequent three candidate itemsets. Remember, this one you probably can't see a big cut. Why? Because you probably see A, C, B, C are frequent, you may think maybe A, B, C could be candidate itemset. But A, B is not frequent here, so A, B, C will not be derived. So we only derive B, C, E. With another scan, we find it's support is two, so it's frequent as well. Then we find all the frequent one, two, three itemsets. The concrete implementation actually involve self-joining and the pruning. Self-joining goes like this. We get abc, abd, the first two are the same. The third one is different, so we generate this self-join, generate this candidate set. A similar thing we get acd, ace, the first two are just the same. The third one we get them together, we get this candidate sets. But we may need a pruning process. The pruning is pretty simple. Before you count against a database, you proceed for abcd, this bcd is there. So this is a candidate one. But for acde, cde is not in the frequent three itemset. So acd can not be the candidate, so it's pruned. So that's the simple way, you bring in the self-joining and pruning, we can solve this problem. You'll find out we get only one candidate sets, abcd. Let's look at the SQL implementation of this candidate set generation and test. So we proceed the candidate generation essentially this. You proceed the self-joining. How to do self-joining? You'll see the first k-2. They are all identical. Then the last one, k-1 item. One is smaller than the other. We get one candidate set. Then for this candidate set, we still need the pruning. The pruning essentially is a check whether these subsets, the k-1 subset containing this one. If s is not in F k-1, then we just delete this candidate from the candidate set C sub k. So this is a candidate generation. The key step of SQL implementation of Apriori. [MUSIC]
[MUSIC] Since the proposal of the famous Apriori algorithm, there have been many interesting extensions or improvements in this method. Let's just examine some of them. So the first line of work is how to reduce the number of passes of transaction database scans. We know that database scanning involve disk accessing which could be quite expensive. But if you want to find size 10 frequent item sets. Likely, you will have to scan this database ten times. That's quite expensive. To reduce a number of scans, so there's several publications. One is called partitioning method, we're going to introduce. Another one called dynamic itemset counting method. You will probably notice the first author, Serge Brin who was a PC student of Stanford university. The second year, 1998 he actually proposed one of the most famous algorithm PageRank and set up a Google company. Okay. So this another line of the work is how to shrink the number of candidates. You will generate very large number of candidate sets in many cases. How to shrink the number of candidate sets to be generated could be a big saving. Hashing method is interesting one for mining max patterns. Bayardo also proposed pruning by support lower bounding. Another interesting method is Toivonen proposed sampling method. The third item's work is to explore some special data structures. Some tree projection method using H-tree to do H-miner. And hypercube decomposition is another interesting method. We're going to introduce a new tree structure called FP-tree. Let's look at the first one, partitioning method. Partitioning method guarantee you only need to scan data twice before you generate all the frequent item sets no matter for how many case that is. So they observed a very interesting property called any itemset potentially frequent in transaction database must be frequent in at least one of its partitions. That means suppose you get a big transaction database, you partition them into k partitions. If items at X is not frequenting any one of these k partitions, it cannot be frequent in this global transaction database. Why? Before you can see, if you got items at x which is not frequent in the first one. Not frequent in the second one. Even not frequent in the case one. So you add them together. You add all these support together, you get a global support. You add all these database together, you get a global database. You frequently can see if every one is smaller then sigma more times its size then the finally the global hour also smaller than 6, sigma times the size of global database. That's why at least one of this database contain TDB frequent. So with this operation, you can see this partition method will need only two scans. The first scan is you partition database into K partition database. How do you partition them? Each partition you want them to fit into the main memory. Why? Because no matter how you scan them, you will not involve any database I/Os. So that's a reason you can scan once you put them the database, you can work on it to derive k frequent items that for no matter for how many case you can get. Okay, that means you can derive all the local frequent item sets. Okay. Then based on this property you can do the second scan like this. You can say, since any frequent item set in one of these k transactions can be potentially frequent. So the global candidate sets is which is frequenting any one of them, it will be a global candidate item set. Then, the second scan, just count against each database kind of those counts of the global candidate sets. Then you will get their account, you add them together, you will be able to derive global frequent patterns. That's why this method is guaranteed to scan the database only twice. So this is pretty interesting. Another method I'm going to introduce to you called Direct Hashing and Pruning. This method is trying to reduce a number of candidate sets based on hashing and pruning techniques. The general philosophy can be like this, okay. The observation is, if the k-itemset is frequent, then in this hashing bucket, it contains several potential itemsets that must be frequent. So if the hash count is not frequent, than any one of them cannot be frequent. The general philosophy like this. Okay. Why are you going to derive the frequent one item? In the mean time you set of the hash entry for the potential two itemsets but why you set a hash entry rather than count each one because there could be many, many distinct items. So the frequent tool items as before, you get a frequent 1, the candidate 2 could be really, really big. But if you set a hash entry to get several items, share 1 hash entry, the whole hash table will not be too big. Then the interesting thing is, while you derive frequent one itemset, you also can cut many two itemsets that can not be frequent. For example suppose our minimum support count is 70. Then this one has only 35, this one has only 58. That implies those items, none of those items can be frequent. Because they share this entry, their support count is still below the threshold. So only the second one, this bd, be, de, the support is quite big. The past support threshold, these could be poked. This may contain the real candidates sets. Okay. That's a reading you can reduce the number of candidates substantially using this hashing and pruning method. [MUSIC]
[SOUND] Now we are going to look at another interesting pattern mining method. It's Mining Frequent Patters by Exploring Vertical Data Format. This method is called ECLAT, or equivalence class transformation method, which in our philosophy looked like this, okay? In original transaction database, it's horizontal data format in the sense you get every row you get transaction IDs and a set of items in this item entry. Then you can transform this horizontal data format into vertical data format like this. For every item, a, you will see which transaction IDs is associated with this item a. That means a bot, you reach transactions. What's the benefit of this? The first thing is, you transform the Itemset into TidList. The total size is approximately the same if every entry or every ID, they have the similar number of bytes, but the way to compute this will be different with this TidList. For example, if you say what is TidList of e? You get 10, 20, and 30. What is TidList of a? You get a 10 and 20. Then how to derive ae? You just intersect them together, you can see you intersect this set with this set, intersect two TidList, you derive the TidList of this ae, this Itemset. If this one contains sufficient number of transactions, then this is frequent like two, then ae or be frequent. If this one's infrequent, then you don't need ae to go further, that's a similar thing as Apriori principle. Then the properties of TidLists basically say if these two TidList is equivalent, that means they have the same set of transactions. If this one is the subset of the other one, that simply says the transaction containing X must always containing Y, because this one is this one's subset. So you probably can see, if we try to derive frequent patterns based on these vertical intersection, we just need to see the size of these transaction list. But there's one interesting method called diffset to accelerate the mining. The reason is, when you get very large number of transactions, each item may be associated with a very long TidList. Then,their intersection, like this e and the ce, their intersection could be small, but their difference could be small. So you look at the intersection, it's large, the difference could be small. For example, you intersect these two, the intersection is 10 and 20. But the difference is only one. So you only keep this, you don't have to keep t(ce). That may save a lot of space, okay? This is the general idea. Using diffset you can further improve this efficiency. [MUSIC]
[SOUND] Hi, I'm going to introduce you another interesting pattern mining approach called pattern growth approach. This approach is represented by interesting algorithm called FPGrowth. Let's look at how this algorithm works. The general idea is first we find the frequent single items and then we partition the database based on each such item. Then we recursively grow frequent patterns by doing the above iteratively or recursively for each partitioned database, also called conditional database. To facilitate efficient processing, we use a new data structure called FP-tree. The whole mining can be summarized as follows. We recursively construct and a mine conditional FP-trees. Until the result FP-tree is empty or until it contains only one path, the single path will generate all the combinations of its sub-paths each of which is a frequent pattern. Let's look at a simple example. For this transaction database, it has only five transactions. Each contains a set of items. So we can scan the database once. Find the single of item frequent pattern like the following. Suppose minimum support is three. We will be able to find the following frequent single item. Then we can sort the frequent items based on it's support frequency in descending order like this. Base on this, we can construct a tree by first construct the header, following this order, then scan the database, we can construct this tree as follows. For example, okay, if you look at the first transaction, it's f, c, a, m, p, so we construct it f, c, a, m, p with only support as 1. Then we get f, c, a, b, m, we constructed this f, c, a, b, m. And this power support becomes 2, this maintains to be 1. You can go one by one working on the following tree. Then to mine this tree, we can use the divide and conquer and do it recursively. We can do it like this, okay, for each tree we can construct this pattern base. We call it p's conditional pattern base, m's conditional pattern base. Let's look at how p's conditional pattern base is constructed. So if you can see, p is the leaf of the node. You only look up. You get f, c, a, m that supports 2 because p occurs together with this branch only twice. That's why you get f, c, a, m 2. Then for the same reason, you look at the other p, it's cb happens only once together with p, that's why we get cb1, okay. Then if you want to look at m's conditional database, the philosophy you were thinking the patterns having m but not having p because of p already being taken care of by the p's conditional pattern base. So you look at m's conditional database, actually you also look up and you get f, c, a, so for this branch they occur twice, that's why you get f, c, a 2. For the same reason, you look at this m and you get an f, c, a, b 1. So you get f, c, a, b 1. You can do this on and on. And with this, you can get into transform the prefix paths of p to get the following. Then, the remaining task which is, mind this, conditional pattern base. For this conditional pattern base, we mine single item patterns, we construct the FP-tree and mine it recursively. For example, for p's conditional pattern base, you will get only c: 3, the reason is f, a, m, and b they never occur more than three times, so they are out, they are not frequent so you only get 3. But for m-conditional pattern-base, you per can see f, c, a really occurs three times, the b only occurred once, then you'll get a f, c, a 3. And b-conditional pattern-base gives you where you can see none of them, actually pass the support threshold of 3, so it's empty. So let's just look at of course, you'll get a and c's pattern-base like f: 3 and fc:3. Actually for the single branch, you can dump all the possible combinations there, support our tree. But we just look in the recursive way how we can mine this pattern-base. For this pattern-base, you probably can see, you can say for a's conditional pattern-base you'll get this. You'll get fc: 3, and for c you get f: 3, you can see that's the same thing. Then for this particular am conditional FP-tree you might, you just take a c, you say c's that means a, m, c or c, a, m their conditional pattern-base is only f: 3, you get this. Then you can dump all the patterns like this. For single branch essentially that's the same thing, you will dump all the patterns, like all the possible combinations, they are all three, okay. You can see if you get a single prefix-path, you actually can partition this into two parts like this path you can mine it, and this path you can mine it and then you can just concatenate the path and results. What about if tree cannot fit in the main memory? If you cannot fit in the memory, we can use database projection. That means, we project the database based on the patterns, based on single items set, okay. So, then we can construct and mine this tree for each projected database. So we can have parallel projection or partition projection, two different methods. The parallel projection means for this one, you will get f4 predict database and f3 predict database. For example, for this first stream you will see gh suppose is non frequent to only f. F's a frequent. You will get an f4 predict database, you'll get an f2, f3. Okay, then, f3's predicted base, you get f2. That's one, and every one of these projected databases is independent of the others. So, you can mine them in parallel. But then, you also can have partition projection. Partition projection the general philosophy is for each string. You only put into one place. For example, this one country is therefore you only put f2, f3, but you don't put this f2 to f3 projected database. When you're finished mine this f4, when you do this projection then you think this one migrates to f3, you put this one into f3, okay. So that's just a different choices for different partition of parallel projections, it's just a different way of implemented, thank you. [MUSIC]
Hi! In this final session of this lecture we're going to discuss mining closed pattern. As we already know before, closed pattern is a compact form but it's a last less compression of frequent patterns. So mining this closed itemsets, it's very interesting and useful. And with pattern-growth approach there's one interesting method developed called 'Closet+'. Let's look at this 'Closet+'. How to develop efficient directly mining of closed itemsets. So let's look at this transaction database, it contains only four transactions and these are the items in these transactions. Suppose the minimum support is two, we'll be able to get these as frequent itemsets. And based on this we can work out the F-list like the following. Now, we look at an interesting method developed called 'itemset merging'. The philosophy can be represented using this example. Let's look at these projective database. For these projective database, we will have ACF, EF, and ACF based on this. As you can see this project database will get ACEF and ACF. But the interesting thing is ACF happens in every transaction project in this database. ACF have the same support as D. In that case, we can grab ACF out form ACFD project database which contains only one item E, it is not frequent. Therefore, we will be able to get a ACFD support is two, it's the final result. This method called 'itemset merging' simply says if Y appears in every occurrence of X, then items of Y is merged with X. Now, the X is D and a Y is ACF. ECF occurs in every occurrence of X which is D, then we will merge ACFD together to form a more compressed form. That means, you can mine all these immediately. So this is more efficient. Actually, there are many tricks developed in Closet+. For example, hybrid tree projection, we use bottom-up physical tree projection, top-down pseudo tree projection. There's one technical sub-itemset pruning, itemset skipping, efficient subset testing. But I'm not going to get into the details. For details, you can read this paper. So finally I'll summarize the recommended readings. These are all classical papers. Apriori mining and the further improvement of Apriori mining. Then we have vertical methods, FP-growth methods, and we have Closet+ methods. So finally, there is an interesting survey article called 'Frequent Pattern Mining Algorithms', which contain many more algorithms covered in this lecture. If you're interested in, go ahead and read this chapter.
[SOUND] The first thing we want to discuss is, The Limitation of the Support Confidence Framework. As we know, pattern mining may generate a large number of rules but not all of the patterns and rules generated are interesting. In general we can classify the interesting measures into two classes, objective versus subjective. For objective measure like support, confidence, correlation are defined by mathematical formulas, not change from person to to person. But the subjective measure may change from person to person, because one man's trash could be another man's treasure. So the first thing, is we may want a user to say, what do you like to see? So it's query based. Another thing is we may base on user's knowledge base, try to mine something unexpected, fresh or recent. Or we can map patterns and rules into two-dimensional space, let user to interactively pick some interesting things. So we know we have support and confidence as two interestingness measures in association rules. So we may be careful about this, because not all the strong support and confidence rules are interesting. For example, take the following table, a 2-way contingency table, to look at this to see how to interpret things we found. For example, in this table, 400 out of 1000 students play basketball and eating cereal, but 200 students, they play basketball but they may not eat cereal. In this one, you may derive association rule like this. Playing basketball implies eating cereal. You probably will get a 40% support because it's 400 over 1,000. You will get two thirds of the confidence because you get 400 over 600. So this is pretty high support and confidence. Is this really interesting? Let me try another rule regenerated. Actually you will see, if we say not playing basketball, eating cereal. You will see not playing basketball is 35%, with the confidence even higher, because they have 350 eating cereal out of 400 people. So this one is even higher. So, if you recommend these two rules to the cereal company, they will get confused. They'll say, the first rule say that I better give some free basketball because if they play basketball, eating cereal. The second rule say I better take their basketball away, because if they do not play basketball they actually eat cereal even more. Which one is right? Let's examine it. [MUSIC]
We have learned support and confidence. These two measures are not sufficient to describe association. So the problem becomes what additional interesting measures are good enough to describe their relationships? So that's the reason we want to examine a little more like lift and chi square whether they are good enough to describe additional interesting measures. So lift has been properly used in statistics as well. We look at the same table, the same table we can think B means playing basketball, C means eating cereal. So we have the exact same distribution. Then for this continuous table, we use lift to compute it. The lift is defined as this: B and C are two item sets. For rule B implies C, that confidence if its divided by C support, we get lift. Or we can say if BC this lose support divided by B support times C support. So for this lift, the general rule is if the lift is one, then these two items are independent. If it's greater than one, they are positively correlated. If it is less than one, they are negative correlated. For our example data set, we will calculate a lift of B and C and B and not C. We divide 0.89 and 1.33. Then from those data sets and the rules we've broken C, B and C should be negative correlated because the lift is less than one. B and not C are positive co-related because the lift is greater than one. This actually fix our problem because we know B and C should be negative correlated, B and not C should be positive correlated. So this looks very nice. Let's look at another measure popularly used in statistics as well called chi square. In chi square, the definition, we need to calculate the expected value. How to calculate the expected value? If we can see this 400 is a real value, it's observed value. But expect value is just based on the distribution. For example C and not C the distribution is 700 over 250. This is three to one and all 600 students with three to one you get 450 versus 150. In that case, we probably can't see, we still can't use the popular, the rules like if chi square is zero, they are independent. It's greater than zero, they are correlated either positively or negatively. So we need additional test to see whether they are positively or negatively correlated. Now for our example, we can easy calculate chi square should be almost 76. So B and C should be correlated. Further, we can say they are negatively correlated because the expected value is 450. The observed value is only 400. It's less. So these teams can solve the problem as well. But the problem becomes whether lift and chi square are good in all the cases. Let's examine some interesting case. In this case, you probably can see this not B not C actually is quite big. There are 100000. These actually called null transactions because the transactions contain neither B nor C. And if we just look at a B and C relationship, we first see B and C should be negative correlated because it's not easy to get B and C together. B and not C is far bigger. C and not B is also far bigger. But if we use a lift, we compute a lift B and C, we will get this 8.44 which is far bigger than one. That shows B and C should be strongly positive correlated. This seems not right. Either we tried to use this same contingency table. We add the expected value. We do the computation. We will find chi square is bigger than zero. In the meantime, you observed value is far bigger than the expected value. So we also should say B and C are strongly positively correlated. This seems to be wrong. What's the problem? Actually, there are too many null transactions. That may make things distorted. We need to fix it.
We already seen Lift and a chi-square may not be very good measures, examining the transaction data that contain lots of null transactions. So, what we may like to see is, what are good measures? They do not influence much by number of null transactions. Let's look at those different measures. Some measure, they have the property called null-invariance, that means their values may not change with a number of null transactions. Let's see what measures are null-invariant, what measures are not null-invariant. We already know chi-square and a Lift, they are not null-invariant. Their value change with number on null transactions. But, people have found that the folding five measures, if you check their formula, their definition, they are actually null-invariant measures. So, you probably know Jaccard coefficient and cosine measure quite well. These two measures are popularly used, they're null-invariant. But all confidence which actually take the smaller value among A and B as the denominator, the numerator is just the transaction support of the rule. So, the max confidence is try to find the maximum one of them. These two actually proposed in the study of measuring association rules. Kulczynski measure was proposed around 2007 by us, by our group. We originally called these as balanced measure, but later, the reviewer actually point out that this measure was actually proposed by a Polish mathematician called Kulczynski in 1927. So, we changed the name of this measure to Kulczynski measure. Let's first look at null-invariance, why they are very important. That means why in analysis massive transaction data, the null-invariance is so critical, because in many many transactions, the transaction set contain particular sets of item. The chance actually is very rare like a warm up transactions. They may contain neither milk nor coffee. We will try to analyze milk and coffee using the following contingency table. So, this m_c means the number of transactions that contain both milk and coffee. This not m nor c means the number of transactions that contain neither milk nor coffee. So, this not m nor c is the number of no transactions. Then, we see Lift and chi-square, they are not null-invariant. So they are not good at evaluating data that contain too many or too few null transactions. For example, we just look at this, for this dataset D1, m_c means number of transactions that contain both milk and coffee; not m_c means the number of transactions contain no milk but coffee; m no c means that number of transactions that contains milk but not coffee; not m nor c means the number of null transactions, they contain neither me nor coffee. So, you'll probably go to like a Walmart, it's kind of shopping market, you probably see there could be the cases you get 10,000 transactions that contain milk and a coffee, but 1,000 contain not milk but coffee, 1,000 contain milk but not coffee. In that case, you probably say actually likely if people buy milk, they would buy coffee as well because there are 10,000 such cases. Buy only one of them, there's only 1,000. But, if you have a lot of a null transactions, this value could be quite positive. If you have very few null transactions, it turn out they are independent. You probably look at the value they are not independent. On the other hand, if you see there's only 100 cases, you're got a milk and coffee together. There are many more cases, they buy it alone. But once you have many null transactions, it turns out to be very positive. The number is quite big. So, no matter you get a very many null transactions, so very few null transactions, something may go wrong. So, we do need to analyze such data using some null-invariance measures. We'll examine this in more detail.
[SOUND] Now, we come down to compare these null invariant measures. So, which one is better? We know, we can sense not all those null-invariant measures are created equal so, we want to see which one's better in all the cases. Let's examine the two variable contingency table of the transactions containing milk and coffee. Let's look at the case in this data set. So the first, you look at D1 and D2, D1 and D2, you probably can see the difference is only on the number of null transactions. But you also can see very likely, milk and coffee should get together, they should be positive. In that sense, you can see all of those five, null-invariant measures, they give equal value. In this sense, no matter how many transactions on the null part. Okay, they do not change their value. And also they are very close to one, in the sense, these are possibly getting together. When you look at D3, D3 means mc getting together is quite rare because they get along more frequent. In that sense, all their values are very close to zero. Then you look at the D4, D4 says mc getting together or mc alone, they are all like 1,000 and 1,000 and 1,000 cases. No matter how many null transactions, they actually got things right in the middle this 0.5, 0.5. Only Jaccard's 0.33, actually just in Jaccard, this one means its balance is right in the middle. Then we look at cases, it could be D5 and D6. D5, if you see this is 1,110 solved cases. So what you probably can see, is from coffee point of view, like a coffee guy may say, mc are likely getting together, because they get along as 100 cases buying coffee but not milk. 1000 cases we're buying both coffee and milk. But for milk guy, they probably say, they are very unlikely getting together, because I got 10,000 cases buying milk but not coffee. But only 1000 case buying milk and coffee. So, in that case you look at different measures. It's interesting to see, All Confidence and Jaccard they also say it's closer to zero, unlikely getting together. But Max Confidence says it's close to one, they are very likely getting together. Then we look at a Kulczynski said, I'm right in the middle, because the tug of the war on each side is ten to one. Then Cosine said, I'm a little prone to unlikely getting together. Now, we change this one even more. This is 1,000 to ten, or 1,000 to 100,000. The coffee guys said they are very, very likely getting together. But the milk guy said, they are very unlikely getting together. Now in this case, you probably can see All Confidence and Jaccard drop down to 0.01, and even cosine dropped down to 0.1. But if Max Confidence says, I am very confident they are very close to one, because they are very likely getting together. But in Kulcyzynski said, I'm still in the neutral because this is 100 to one, the other is one to 100, they have the equal ratio. So, which one do you like? So, probably we can see D4 to D6. The real case is, that differentiate that five null-invariant measures. But we probably can see, Kulcyzynski measure holds firm when in these very imbalanced cases. But the ratio is balanced on both sides, and it holds firm at 0.5. That looks interesting. But on the other hand, we also know those cases, some are very imbalanced, we may want to introduce another measure called imbalance ratio. The imbalanced ratio is introduced in the sense, the support of item set A and support of item set B, their differences play important role in this imbalance ratio computation. Then you proceed for the same cases in the last three, the Kulcyzynski vector holds firms at 0.5. But the imbalance ratio, okay, D sub four cases is zero, because they are already balanced. And D sub five cases become 0.89, they are rather imbalanced. And D sub six cases, it is very imbalanced. So imbalance ratio, really can show you how balanced the two sides are. So we feel Kulcyzynski plus imbalance ratio, these two things getting together will present a clear picture for all the three data sets, D4 through D6. Because D4 is neutral and balanced, D5 is neutral but imbalanced, and D6 is neutral but very imbalanced. Finally, we're going to show you some real data sets like a DBLP data sets, we want to look at co-author relationships. So, let's look at this table. This table we got around year 2007. We study the recent database conferences, we look at those authors, they publish papers and they co-author papers in database conferences. But you can probably see, the interesting thing is, for example you look at Hans-Peter Kreigel and Martin Pfeifle. Martin Pfeifle got 18 papers, but all of them are with with Hans-Peter Kriegel. The Hans-Peter Krieger got 146 papers, 18 was with Martin Pfeifle. Well, okay, you can see in that cases Kulcyzynski shows pretty strong value that simply says, these two authors are closely tied together in some way. But they are imbalanced as well. We can see the case of imbalanced ratio, you can either calculate the imbalance ratio is really high. So in that case, you probably can easily judge Hans-Peter, Kriegel likely to be the adviser of Martin Pfeifle. So using Kulcyzynski and imbalance ratio, we can easily see advisor-advisee relationships, and close collaborators. In one research paper on finding advisor-advisees, we are really using those measures to find them with reasonably high accuracy. So finally, we will show you a bunch of papers. These are the papers quite representative on how to judge the correlation relationship, the different measures, including their interest in measure, the non-variant ones and the measure we discussed on the Kulcyzynski. Thank you. [MUSIC]
[SOUND] We first discuss how to mine Multi-level Association Rules. Multi-level Association rules come down to a very natural setting. For example, to your boss if you say, now I find milk and bread sold together, probably everybody thinks this is common sense. But if you find 2% milk with a brand Dairyland sold together with Wonder Wheat Bread, probably it becomes something more interesting. But if you see this Dairyland 2% milk actually sitting in the hierarchy, the top level could be milk, then go down to 2% milk then go down to Dairyland 2% milk. So, it is interesting to mine Multi-level Association Rule patterns. Then the interesting thing becomes how to set a min-support threshold? While we set up uniform min-support threshold across all the levels. But there's one problem, if you set it very high because they naturally have lower support, the low-level patterns will not show up. But if you set it very low the high level you get too many interesting patterns because everything may show up. So a reasonable way is set Level-reduced min-support. That means items at higher level use higher level min-support like 5%, where you go down to the lower level, you may adopt lower level min-support like 1%. To that extent, the skim milk will show up. But at high levels, some you know peanuts or some other things may not show up if they are not interesting, they are not frequent at all. So then the problem is if we set a multi-level minimum support thresholds associated with different levels then how can we use one scan in one shot we mine all the different levels? The interesting thing could be we can use shared multi-level mining. We can use the lowest min-support to let the high level pass down to the low level. But in the meantime when we analyze rules and analyze patterns, we can fill out the higher level rules using higher level support threshold. So another problem for mining Multi-level Association Rules is redundancy. Because the rules may have some hidden relationships. For example, suppose 2% milk sold is about 1/4 of total milk sold in gallons. Then if you see these two rules, one and two, the Rule (1) says, milk implies wheat bread which is supports is 8% and the confidence, 70%. The Rule (2), if you drop down a little down to from milk to 2% milk. In the meantime the support also dropped down correspondingly, for example from 8% to 2%, Un that case, people can see. Rule (2) is a redundant because we can derive such things from Rule (1). That means, if the rule can be derived from the higher level rules that lower level rules are redundant, we shall remove them. Another interesting thing is different kinds of items. Inherently, many different support thresholds. For example, you go to Walmart, you may see diamond, watch, or some expensive things that more valuable but they sold maybe less frequent. But milk and bread probably sold very frequent. So, if we set min-support for all kinds of items using the same minimum support threshold. Then the valuable items may be easily feared out, so, to that extent it is necessary to have customized minimum support settings for different kinds of items. Instead of taking each item try to decide ib minimum support, we can use group-based, individualized, min-support. For example, we can group, diamond, watch, or some expensive things set a lower min-support. Take a milk and bread, those frequent things, and set up higher min-support threshold. Then the question becomes how to mine such patterns efficiently. Actually, if we take our previous study, the scalable pattern mining methods, we can easily extend them by adding different minimum support threshold. I would not discuss the detail, but I think it could be a good exercise. [MUSIC]
Now we come down to another interesting issue, it's mining multi-dimensional association rules. Sometimes the items or the things we want to mine are actually sitting on the multi-dimensions. The single dimension rule we studied, for example, X buys milk, then X buys bread. Milk and bread, both are from the same kind of dimension called product but sometimes we may get a multi-dimensional association rules. For example, for this inter-dimensional association rule, like if X age is 18 to 25 and if X occupation is a student then likely X going to buy a Coke. That was actually H, occupation, and items to buy are three different dimensions. That kind of rule we call inter-dimensional association rule. Then even we look at this sometimes we see the Hybrid cases, like if X age is 18 to 25, if X buys popcorn likely X is going to buy coke. That kind of rule you have H as one dimension but these two buys are seeing the same damages. Okay. So if you see this kind of cases, they likely these three different cases may slightly need different kind of algorithms. Another thing is that attributes can be categorical, can be numerical. For example, the categorical case could be product like beer and diapers and you may have a profession like a student or a professor. So, for these categorical attributes we can for multi-dimensional, inter-dimensional association rules, we can construct a data cube to mine such things or just use data cube structure to mine such association efficiently. Then, another case could be quantitive attributes or we can say those attributes are numerically. For example, H [inaudible] or some other things. For this case it's quite often we may use this criterization method, clustering method, some kind of gradient approaches we are going to discuss this part in the quantitative association rule of mining. Thank you.
[SOUND] Now we come down to study another interesting issue called mining quantitative associations. What is quantitative association? Quantitative association means some attributes after a numerical data like age and a salary. So how to mine such rules? There's one way is we can do static discretization. The reason we need to do static discretization is if you do not discretize them, you're trying to parallelized every possible age and a salary. You will not be able to find any interesting or sensitive rules with sufficient support. But if we try to say, we partition age every ten years, or partition income every $10,000 using some predefined concept of hierarchy, we are going to be able to construct a data cube and we're going to be able to generate some interesting association. But this fixed, predefined concept hierarchy may not fit your data distribution. For example, in the university, likely you may want to partition the age for students. You may say it's 18 to 20, 20 to 22, or something like that. But for income, you may say $10,000 is one partition or low and high. But if you go to hospital, their age distribution you may like to say middle age or old or young. So another way is we do clustering based on data. That means we take every dimension, we study their distribution of the age and income, we perform certain clustering algorithm, generate a few clusters, and then we find the parallelized frequent pattern of each such cluster of pairs. Then finally there's also popular ways to do deviation analysis. That means instead of doing fixed interval, we may do based on certain condition like gender is female. We may find their mean or a median or something, some statistic measure, you will find if the wage mean is substantially deviated from the overall mean, then this could be an interesting rule. Let's go a little further to see how to find some such deviation. We also call extraordinary or interesting phenomena. Usually for this we may say the left-hand side is a subset of the population and the right-hand side is some kind of extraordinary behavior expressed using some statistical measure which could deviate from the overall. Then the rule, whether is true rule or is just a very exceptional case, we need to do some statistic test, like a Z-test, to confirm whether such kind of rule is of high confidence. Further, in many cases, you may even want to go deeper to get a subset of the population, for example, not only look at the gender as female, but look at the location as south. You may get the wage could further deviate from the overall mean, or even from the bigger rule like a gender is female. So that subrule could become a extraordinary subrule associated with its super rule. For example, sometimes you do not have the left-hand side as a subset of population, but based on numerical data you can group them into certain intervals or clusters. For example, the left-hand side could be if the education has pretty many years, like 14 to 18 years, and you will find the mean wage actually is substantially higher than the mean, so that may form another interesting quantitative rule. Efficient methods actually have been developed to mine such extraordinary rules. For example, one research paper published by Aumann and Lindell@KDD'99 is a very interesting case study of a very interesting algorithm. [MUSIC]
[MUSIC] Now, we study another interesting issue called mining negative correlations. So we first need to distinguish rare patterns and negative patterns. What is a rare pattern? Rare pattern usually means there are some rare occurring items, they have very low support but they are interesting. We want to catch such patterns. For example, buying Rolex watches. How to mine such patterns? We previously already discussed this. For different item sets like for those rare items, we should be able to set some individualized group based and minimum supports threshold. That means for rare patterns for just those items, we should set a rather low minimum support threshold, then we'll be able to capture such patterns. But negative patterns could be another very different one. Negative patterns is those patterns that are negatively correlated. That means they are unlikely happen together. So for example, if you find some customer, the same customer, who buys Ford Expedition, which is a SUV car, and also a Ford Fusion, a hybrid car, together. So they are unlikely to happen together, so we called these patterns negative correlated patterns. The problem becomes how to define such patterns? We may have one support-based definition like this. We say, if the itemsets A and B getting together their support is far less than sup(A) x sup(B), that means a chance to get together is far less than random, okay? Then we can say A and B are negatively correlated. Is this a good definition? Actually, this definition may remind us the definition of lift. Then we may see whether they work well for large transaction data sets. Let's look at one example. Suppose a store sold two needle packages, A and B 100 times each, but only one transaction containing both A and B. Then we will see these two needle packages A and B are likely negatively correlated. But when there are in total only 200 transactions in your datasets, you may see s(A U B) getting together, because they got only one time, so 1 over 200, you get this number. This is pretty small number. But then he look at s(A) which is 100 over 200 transaction so it's 0.5. Same as s(B). So their product should be 0.25. So this number is far bigger than this. That means s(A U B) getting together is far less than s(A) x s(B). So we can easily say A and B are negatively correlated, they are negatively correlated patterns. Okay, but when this store, so in total 10 to the power of 5, that means 100,000 transactions. Then suppose all the others does not contain package of A nor B. Then the situation could be completely different, because s(A U B) together is 1 over 10 to the power of 5. But s(A) now is 100 over 100,000, so you get 1 over 1000. s(B) is also 1 over 1000, when they time together you get 1 over 10 to the power of 6. This number is even smaller than A and B getting together. You may say, A and B getting together is very frequent or it's passive correlated, actually it's not. What's the problem? The problem actually is null transactions. Because there are so many transactions that contain neither A nor B, they are null transactions. So we probably can see a good definition of negative correlation should take care of the null invariance problem. That means, when two itemsets A and B are identical related, they should not be influenced. Okay, whether they are an negative correlated or not, they should not be influenced by the number of null transactions. Okay, now we give you another interesting definition, which is a Kulczynzki measure-based definition. That means if we want to say A and B whether they are negative correlated, what we need to see is A and B are frequent. But the condition the probability of A under condition of B and the probability of B under condition of A, their average should be less than epsilon. Where epsilon is a small negative pattern support threshold. Then we probably can see A and B negative correlated can be justified for our needle package problem. We can see no matter there are in total 200 transactions, or 100,000 transactions, if we say epsilon is 0.01, we probably can see this Kulczynski measure based judgement, we can easily see the average of the conditional probability should be less than epsilon, so they are negative correlated. So this seems to be very interesting and a good definition. And how to mine them, actually these are the method similar to our previously discussed pattern mining method. We will not discuss it further. [MUSIC]
[SOUND] Now we are going to discuss another interesting problem, mining compressed patterns. We know frequent pattern mining may often generate many many patterns. But in many of such patterns may share some similarity. Maybe there is scattered, but not so meaningful if you generate all those patterns. Let's look at such an example. Suppose we finally get five pattern IDs. So you perceive these five patterns are like P1 and P2. They are very similar and their supports are also very similar. But P2 and P3, they are similar in item sets but their supports are rather different. Can we compress them? When we first examine closed patterns, actually for closed pattern, there's nothing you can compress. The reason is closed pattern require the supports are identical, then you can compress them. But none of them have identical support counts. So nothing can be compressed. What about max pattern? Of course we can use max pattern, that's P3, to represent all the patterns. But on the other hand you probably see P3, this support, is rather different from all the others. To use P3 you may lose a lot of information on the support of other patterns. So that desired output actually is P2 P3 and P4. The reason you probably can see is P1 and P2, they share a similar item sets and also they share very similar support counts. So we may need a good measure to see what things can be compressed. A good measure could be pattern distance measure. We use this definition to define the pattern distance. You probably can look at a P1 and P2 when we look at this. P1 and P2, their transaction, intersection of their transaction ID. What should be the count? The number of transactions they can intersect, actually it's a smaller one here, because every transaction containing P2 must also contain P1. So their support counts intersection should be this number, okay? What about the unit? The unit actually says all those transactions, because we know all the transactions, P1 must also containing the transactions of P2. So their unit number should be this number, okay? In that case,we perceive these two numbers very close to one. Then the distance 1 minus this very close to 1 number, you get something very close to 0. That means their pattern distance is rather small. Based on this, we probably can see we may be able to define a theta clustering or theta cover. That means if we can see the pattern P, the theta cover of pattern P is finding those patterns, all the pattern can be expressed by this, the distance is within delta. Actually, you probably see, P2 is a good one, because P2 essentially can cover P1, because P1 just have a little less item sets, but their support is so close. To that extent we may say they are within this theta cover. So for data clustering, we will be able to cluster P1 P2 together using P2 to represent the pattern. So that means if we do this data clustering, then all the patterns in the cluster can be represented by one pattern, P. So the problem becomes whether we should mine all the patterns then compress them, or should directly mine these compressed patterns. Actually there's a efficient method which can directly mine those compressed patterns. I'm not going to get into the detail but you may refer to this interesting paper. Okay, then another interesting thinking is Redundancy-Aware Top-k Patterns. That means we want to get a desired pattern which is similar to the compressed one, because want to get high significance and low redundancy. These kind of of set up patterns, okay. Let's look at this a, b, c, d, four different kind of compression. Actually a is a set of original patterns. There are cluster shields, their pattern distance, and the color the darker shows is more significant, the lighter shows is less significant. Okay, in that case you probably can see in this bigger cluster, there are three patterns. They are quite significant. If you just do the top-k pattern mining, that means you take it as a support count, or other significant measure, you would only find these three patterns. Suppose we wanted only find top three, then all the remaining patterns like here in the other cluster is completely missing. But if you say I just do the summarization, try to find no clusters. And within each cluster try to find their centers. Then you'll pretty well find those less significant patterns, so this may not be a good balance. Actually better balance is you take care of both significance and the redundancy. Simply says, you look at this one, there is something very significant. And that they are also in the cluster center you may want to show these patterns. In the meantime, suppose you can only show three, you may show these are significant and less redundant. This one is significant and also it represents this cluster. So the problem becomes how to develop efficient and effective method finding such redundancy aware top-k patterns. There's an interesting study which uses the max marginal significance to measure the combined significance of a pattern and develop efficient methods to mine such patterns. We are not going to get into detail of this method. Interested readers we made read the paper we pointed out. Thank you. [MUSIC]
[SOUND] First, we are going to discuss Sequential Pattern and Sequential Pattern Mining, the concept. So the first thing is we should say, sequential pattern mining is very useful, has very broad applications. One application could be in costumer shopping sequences. For example, you get a loyalty card for your shops, you may want to see. Maybe one costumer likely going to first buy a laptop, then a digital camera, then a smartphone within 6 months. If this forms a pattern, you may be able to try to do some kind of advertisement to other similar customers or serving some new incentive for this customer. Like a medical treatment form sequences, natural disasters like earthquake happening it may have some sequences of natural and also human phenomenon. Science engineering a lot of things are processes. They evolve along with time. Similarly, stocks markets they have some kind of duration of sequences. Weblog click streams, calling patterns for telephones and other things forming sequences. Even for software engineering, the program execution from sequential patterns. The biological sequences very, very useful for analysis like DNA sequences, protein sequences. So we see trying to get sequential patterns out of those very big vast applications, could be very useful and important. Actually, we can distinguish transaction databases, usually may not be important to look at their time effect. Sequence databases, they have a time stamp attached with it. Time-series databases, usually the time, things happened actually along the even or equivalent time intervals. Sometimes, it's very consecutive. Then for sequential patterns, actually there are two kinds. One is gapped, another is non-gapped. The gapped pattern means, you do allow to have gaps within those patterns. The non-gapped patterns means, you will not allow these patterns, the sequence, everything is important. The concept of this important if you have gapped and you have to trade them very seriously. For example, for shopping transactions. Probably, you don't care customer in the middle buying some other things, so it's not important to study the gaps. Click streams sometimes you may say, some click streams, you may care about gaps. Some, you probably do not care of gaps that much. For biological sequences, in many case you do carry gaps, so the protein sequence or DNA sequences, if you insert many things, in the middle of the two DNA sets, sometimes you may completely change the function. So let's look at the customer shopping sequence as a major example to study how to do sequential pattern mining. Sequential pattern mining essentially is, you give me a set of sequences. The algorithm is trying to find the complete set of frequent sub-sequences satisfying a certain minimum support threshold. Let's look at this example. We have a sequence database containing four customer shopping sequences. Okay, what's the meaning of this? We look at this particular sequence. This sequence, the parenthesis means this one is within the same shopping basket. Then after that, you get another one ab that means this ab following ef. But ab is getting together at the same time. Similarly, df getting together but following ab, and then c, then b, okay. That means each one of these you can think is a element. It may contain a set of items or you call events. Then this one event may follow another one. The items within the event, the order is not important because they are in the same shopping basket but for our convenience, we can sort them alphabetically. Then what is subsequence? Actually any sub-strings within this line probably can see here the subsequence you may have a gap. For example, you say, you can have a, you have bc, bc actually you chop this a. You can chop complete ac, then you get d. You can chop one f, you can get it c. So, this one is a subsequence of this longer sequence. Then, sequential pattern mining, the sequential pattern essentially is if you set a support, like a minimum support is 2, that means, at least 2 sequences contain the subsequence. You find those subsequence, this is a sequential pattern. For example, ab getting together then c, in this sequence database, this is a pattern of support 2. So sequential pattern mining algorithm is, you try to develop algorithms which are efficient, scalable and these algorithms should find the complete set of frequent subsequences. What we call sequential patterns. And also should be able to incorporate various kinds of user-defined constraints. For sequential pattern mining, actually Apriori property, the property we have used in frequent pattern mining still holds. For example, if we say a subsequence s sub 1 is infrequent, then any of this supersequence cannot be frequent. So that's almost the same idea as Apriori. So based on this idea, we actually can develop lots of algorithms. One, represented the algorithm called GSP, Generalized Sequential Pattern mining, developed in 1996. Another way is a Vertical format-base mining called SPADE, developed in year 2000. The third one we're going to introduce is Pattern-growth methods called PrefixSpan developed in year 2001. And then we are going to study mining closed sequential patterns called CloSpan. Finally, we're going to discuss constraint-based sequential pattern mining. [MUSIC]
[SOUND] Now, I'm going to introduce an interesting algorithm called GSP, that's Apriori-Based Sequential Pattern Mining mass search. This mass search is pretty simple. You start from a sequence database. Then you first try to get the singleton sequences like the first one appears. That means you scan database once, you count every single item that occurred in every sequence. Then you can see a occurs three times, occurs five times. So if you said min_sup = 2, likely g and h will be gone because their support is only 1. Then you find frequent lens one subsequent a, b, c, d, e, f. With this you can combine them as candidate sub-sequences, you may have aa, ab, ac, ba, bb, bc remember, aa is still important means, you first buy aa then, buy another a. Then, for the, for the shopping basket, you may get ab together. That's why you may have ab is one event, one element, ac is another element. You may get these kind of subsequences. In total, you look at six by six, then you get it no six by five divided by two, you will get this number of candidate size two sequence. But without Apriori, without this pruning, you'll get much more. So even this minimal pruning still can reduce search space substantially. So in general, we can work out the algorithm like this. You put this one into a loop. At the very beginning, you can scan to find length k, which is length one, frequent sequence. Then, based on the Apriori you can generate the length two or length k plus one, candidate sequence. then you can go back to check and find the real frequent sequence, then you go back to the Apriori based candidate generation. So generation test, you can go into the loop until no frequent sequence or not candidates can be found. So if you look at the execution sequence for this particular dataset, you will find, at the very beginning you get a, b, c, d, e, f. And g, h, will be gone. They are painted in blue. So then you can, from here you can go up, get a frequent length-2 sub-sequences. Then you can generate lengths three, length four, length five, and here, you cannot go along anymore. So, this is the GSP algorithm [MUSIC]
[SOUND]. Now I am going to introduce you another algorithm called SPADE, which is sequential pattern mining based on vertical data format. You probably still remember the vertical data format based frequent pattern mining algorithm called ECLAD. Here for the same set of authors, they actually develop an interesting algorithm for sequential pattern mining. Okay. The idea is pretty simple. If you take the Sequence, you do a little detailed study, you get a Sequence ID, Element ID, and set of Items. So, what you can see is, for the first Sequence ID, 1. Element ID 1, you find item a. Element ID 2, you find a, b, c so on, okay. Then we can transform this into vertical format. That means we just look at the, where a occurs and where the b occurs. So the a occurs, you probably can see, it happens in Sequence 1, ElementID 1. In Sequence 1, Element 2, and also Sequence 1, Element 3. So you get this one the same. You can get Sequence ID 2, 3, 4. Similarly for b, you can find where the b comes, is Sequence 1, Element 2. So you get 1, 2. Then how we can combine them into frequent sequences, like a then b, or b then a. If you say a then b, you, you will be requiring a is in front of b, or a's Element ID is in front b's Element ID. That means for the same Sequence ID 1, if EID 1 is smaller than EID 2 of b, then you get 1 Sequence E ab. Similarly, for b then a, what you get is these Element ID of b should be smaller than Element ID of a. For the same reason you can get all of them for the lines 2, okay. So for lines 3 what you need is, you just get lines 2 frequent 1s. Then you do drawing. How do you do drawing? You probably can see these Element ID Sequence ID should be the same. The Element ID, this b shared with this b, they are both 2. So you can join them together, you get 1, 2, 3 here. And you join the other together, you can get a 1, 3, 4 because it, Element 2, you get a, you get a b, you can get a again. So, the two lines and you actually can find all of them, okay. So that's a reason you can use Apriori based principle to find all the frequent subsequences. This algorithm was developed by Zaki in 2001 called SPADE. It's Sequential Pattern Discovery using Equivalent Class. [MUSIC]
[SOUND]. We have introduced [INAUDIBLE] based, sequential pattern mining algorithm, GSP. We have introduced vertical format based sequential pattern mining [INAUDIBLE]. Now we come down to see pattern-growth space algorithm, called PrefixSpan. PrefixSpan is a slow spaced mining algorithm, okay? To examine the sequential pattern in more detail, we need to introduce the concept of prefix and a suffix. The prefix means anything in the front, if it's frequent you want to capture them, as a frequent prefix. Like a, a, a, ab. Then their projection becomes a suffix. Remember if you get a a, a, what you see is you got a position holder for the next one is b. So that's why we use underscore as a position holder. Okay. The similar thing, you get an ab, the position holder will shift to c. So that means, given a sequence, you will find a set of prefixes and a set of suffixes, or you can say prefix-based projection, okay. Then for this prediction, what we will find is first find length-1 sequential pattern. If they are frequent, we call them length-1 sequential pattern. Then we can do divide and conquer, that means we divide the search space to mine each projected database. We have a projected database, b projected database, c projected database, up to f projected database. This mining method, methodology, called PrefixSpan, or prefix-projected sequential pattern mining. So let's examine a little detail. For this sequence database, if we find length-1's sequential pattern like this, then we can actually get length-2 sequential pattern by first doing projective database. Then find length-2 sequential patterns. That means, if they are frequently in this projected database, they will form length-2 sequential pattern. And then we can do length-2 sequential pattern-based projection from aa project database, af project database. Okay? And we can keep this one ongoing. Okay? The major strengths, or advantages. There's no candidate subsequence to be generated. And the projected database keeps shrinking. So, let's look at some implementation tricks. So if you do the projection, like you really taking the sequence to do [INAUDIBLE] a, [INAUDIBLE] ab. You will get largely redundant subsequences, or you see postfix but they are largely redundant with the original string. However, we do not need to do the real physical prediction. What do we need is we call pseudo-projection. That means you just say, what is a's predictability base? It's the same sequence, but the position is number two. What is ab's project database is the same sequence as s, but the position is number four. So if you register the next position to be scanned, it will essentially register the projected sequence, or suffix. Okay? So if the database can be held in main memory, this pseudo-projecting is very effective. Because there's no physical copying or suffix. You only need pointer to the sequence. You just get offset. You get suffix. But if it does not fit in the memory, you use pointer. You may involve lots of disk accesses. So you may really want to do physical projection because once you projected it quickly this set can be fit in the main memory. That means we can integrate physical and pseudo-projection any time when the data fits in the main memory, you can use pseudo-projection. [MUSIC]
[MUSIC] Now we study another interesting issue called Mining Closed Sequential Patterns. The algorithm mining this is called CloSpan, so what is closed sequential patterns, similar to closed frequent item sets? The closed sequential pattern s means, if there exists no superpattern s prime and this s prime and s have the same support. Then s is a closed sequential pattern, in another word, closed pattern means for the same support, you will find the longest one. That's the closed pattern, the other ones probably does not really matter, okay, for example, let's look at this example. Supposed we find three sequences, abc with support 20, abcd with support 20 and the abcde with support of 15. Which ones are closed, abcd is closed in the sense for support 20, abcd is a longest one. Then abcde is also closed because the support at 15, there is no longer 1 and 15 which is a super sequence of abcde. So there are two ways to mine closed sequential patterns, one way is you first mine all the sequential patterns. Then you find which one is closed to like this abc, you can knock it down, then you'll get set of closed sequential patterns. But this is not a very efficient, what we want is directly mine closed sequential patterns. This will reduce a number of redundant patterns to be generated in the middle. But it will attain the same expressive power because it's last it's compression, so there's a one interesting property like this. If s is the superset of s1, s is closed if and only if two projected database have the same size. Let's look at this, for example, for this sequence database, if you find f the project database and another sequence is af. This is the project database, if these project database have exactly same size, then this actually means af is closed. Essentially, you only need to mine one, that means you can merge them, so you will be able to develop two kinds of pruning. One called backward subpattern pruning, this is a subpattern, you can prune it, another is backward superpattern pruning, so you probably see. That you can use superpattern to chop up the subpattern you will get into this, okay? So with this spirit we can develop an efficient algorithm called CloSpan. It greatly enhance the processing efficiency, I'm not going to discuss very detailed. But details you can read this paper, it gives you all the detail on the CloSpan. [MUSIC]
When first studying Mining Spatial Associations. Spatial association or spatial frequent patterns share some commonalities as a general association and frequent patterns. For example, the association rules are also in the form of A in plus B, with certain support and a confidence. In this context, A and B could be sets of spatial or non--spatial predicates. The spatial predicates may indicate topological relations or spatial orientations or distance information like close to, within certain distance. And it measures support and confidence are very similar to the general ones. The rules with him find it could be like if x is a large town then x intersect with the highway, then x is likely to be adjacent to water. Like lakes and the rivers and ocean so that with certain support and a confidence. In spatial rail mining, quite often we would like to explore spatial autocorrelation. That means spatial data tends to be highly self-correlated, nearby things are more related than the remote things. For example when we study neighborhood, study temperature, likely would pay more attention to find interesting relationship in the nearby objects. In spatial association mining, there's a interesting heuristic called progressive refinement. The general philosophy of this for a spatial relationship, there are some rough ones like close to which is generalization of some more refined one like nearby, touch, intersect, contain, they are somewhat or close to. Just give you an example. Like here, if we can see, near a highway intersection, you may be able to find shopping centers and gas stations. But how close to, whether they are very nearby or they are almost touch the highway intersection. So in that sense, those are detail refine relationships. However, if we first find those close to there frequent together, okay. Then we, if we want to find the more refine, how close to are they really to the intersection. So we can say if the close to is a frequent pattern, then we are going to study the more refined watch. To that extent, this is the philosophy of progressive refinement. That means, we first search for rough relationships, and then refine for, to study more refined relationships. The general philosophy is, if the rough relationship is not frequent. So there's no need to study the very final because they are not frequent as well. So to that extent, we can do two step mining of spatial association. The first step, as we use and we shape or low cost algorithm. Like minimum bounding rectangle or R-trees for some rough pattern mining. That means we first compute the rough spatial frequent patterns. Then we know if the rough one is frequent, then we are going to get into refinement process. That means we may study a more detailed algorithm using more refined data structure. So this principle can save a lot of mining cost. Because we first using rough lines, we get a big filter. A lot of unnecessary pairs because they are not frequent already. We don't have to refine it. We don't have to use a refine measure to study more refined patterns. Close to, you can find
We will discuss Mining Spatial Colocation Patterns. What is spatial colocation pattern? Usually, a group of spatial features of events can be frequently colocated in the same region. For example, West Nile Virus may often occur in the regions with poor mosquito control and the presence of birds. So if we can find such patterns, we may have better way to control the spread of the virus. Let's look at the example on the right. So we have A, B, C, D as four types of objects. You can think A could be West Nile Virus, spread in region. B could be some mosquito region. Or C could be in some cities, D could be the presence of birds, something. Then we use the address, means they are colocated. For example, 3, 6, 17 are co-located, we used edge to link them together. And a 4, 7, 10, 16 are co-located together. We use the edge, linking them together. Okay, like 2, 9 could be collocated, but on the other hand a 2, 14, 18, 11, and 15, these are collocated regions, okay? Then we may be able to find rowset. The rule set is this, okay? If we want to find A, B, C, D, how many things are, how many objects are colocated together as A, B, C, D for types. When I be able to find four, seven, 10, 16 are colocated. And 2, 11, 14, 15 are co-located or 8, 11, 14, 15 are co-located. That means A, B, C, D we may find two, actually three instances they are co-located. That's they are all set. On the other hand, A, B may be a little more. For example, 5,13 is co-located, 7, 10 is co-located, so we may find more instances. Then we may get a colocation rule like this. If pattern A is colocating, whether pattern B may also colocating. We may find this conditional probability of the colocation patterns. That means if we want to find AB may imply CD, that means if AB are colocated, likely CD will be colocated with certain probability. Then, we need to compute the pattern A, at what condition, in how many cases, when A is there, and AB is also there. That means Actually AB, the cases is usually is the case. You can find A and in the row AB set, rowset. Okay, essentially what we need to calculate is A, B, C, D is a rowset. You are probably going to see there are three such cases A, B, C, D together. But there are four cases A,B together. So the condition, if you find the A, B then you find the C, D, colocated. The condition equals 3 over 4 is 75%. The conditional probability for this rule is 75%. Then the interesting thing is, can we find such colocation patterns and rules automatically and efficiently? Okay. So let's see how we can derive an efficient algorithm to find them. To find efficient algorithms, we first introduce another concept called participation ratio. That means that the feature f is participating in this pattern c. What's the probability? Okay. So for example A participating in A, B, C, D, you're broken in C. A is the red one. We have one, two, three, four, five. Five cases you'll find A. But every time we find A We can find a, b, c, d. There are only two such cases like 7 and 14. That's why we get a 2 over 5 is the participation ratio. Similarly, if you want to find D participating in A, B, C, D, d It actually is the blue circle. You can see there are only two blue circles, but both of them are completely partitioned, participating in this ABCD pattern. This simply says you get cases two over two is 100 percent. Then we may derive a interesting property called monotonicity property of participation ratio, okay? That means suppose we have c and c prime as two core location patterns. But c prime is a subset of c. Then for every feature f, if feature is participating in C prime, okay? Then the probability will be higher than participating in a superset. This is quite easily, can be an either and or [INAUDIBLE] because you can think about this, if A participating in A, B then the chance is higher than A participating into, in A, B, C, D. Because every time when A, B occurs. ABCD also will be. When A, B, C D occurs, a, b will also be there. That's why when f participating in c for sure the probability or the participation ratio will be higher then participating supersect. So based on the monotonicity, we can work out an Apriori-like algorithm to efficient mine colocation patterns. Let's give such an example. Suppose minimum feature support is riveting a sigma, and a minimum participation ratio is rewritten as roe. Then we start from a set of single feature patterns. If they are frequent, that means if their support is no less than sigma. Okay. Then, we can keep using upper array like a principle to find their pairs. That mean from the single feature pattern try to find two feature patterns, three feature patterns grow up to size k. Anytime we can stop if the pattern is frequent to anyone Because once it's not frequent, it's super patterned will not be frequent. This is simply based on the Apriori principle. Then based on these participation in ratio monotonicy we will be able to find all the super patterns of a single feature or the multiple features. Suppose we got a feature Is p, we want to find it's super feature, okay nor super-pattern. And we know we can get a little bigger super-pattern to see whether it's greater in row. Anytime if it's not greater in row, it's super-pattern would not need to be examined because they cannot be bigger than row anymore. So based on these Apriori principle, we can easily work out efficient algorithms. So that's the trick or that's the interestingness if we can find a monotonicity pattern, we will be able to work out, Apriori-like efficient processing algorithms.
In this unit, we're going to study Mining and Aggregating Patterns over Multiple Trajectories. When we study spatial and temporal patterns, an important pattern is trajectory pattern. That means you look at the points, the objects, the moving along the spatial map, along with time. So we call these the trajectories. What we want to find are trajectory patterns. One interesting trajectory pattern mining method called partition-based trajectory pattern mining. They are mining T-patterns, T means trajectory. This is a work done by a group of Italian researchers. They published in KDD 2007 called Trajectory Pattern Mining. Their mining method essentially is a really good study like busy traffic in the city. You can think a city can't be partitioned into equal width grids to obtain regions of interest. For example, one grid may represent the museum, the other grid may represent the railway or campus. Then when you study the busy traffic, they go along the route. You will be able to find either what hours or how much time you will find a very busy traffic and how long it may take to reach the other side. So, what we can do is we can transform each input's trajectory into a time annotated symbolic sequence. For example, you may transform one location is a railway station. Another location is Castle Square. The third location could be museum. And based on a time, you may say from the railway station, it takes about 15 minutes to reach Castle Square and takes 2 hours and 15 minutes to reach museum. And we can use a constraint-based sequential pattern mining because each grid is matched into a symbolic sequence. So the finer you can use the symbolic sequence to represent the whole trajectory and try to find trajectory pattern is trying to use constraint-based sequential pattern mining. The constraints can be the range of time delay. Then, the constraint-based sequential pattern mining results can be mapped onto the map to show how much time it may take from one location going to the other location. That kind of matching T-pattern will be something like (x0, y0) after alpha time delay, you will reach (x1, y1) points. This will get into explicit representation of the pattern. Then another interesting group of studies are detecting moving object clusters. That means, you may think trucks. You may think animals. They are moving together, you may want to find their moving object clusters in this sense. The first one, definition is flock. Both flock and a convoy require k consecutive time stamp in order to find patterns. The flock essentially is they require at least m entities are within the circular region of radius r, and they are moving together in the same direction. Then we call these the flock pattern. But a flock pattern is a little too rigid in the sense they require at least m entities moving in k consecutive time stamps. And it's within their movement are within the circular region of radius r, they are relative distances. And sometimes this radius r is too rigid. The convoy definition is using density-based clustering. They don't have to be within radius r. They can be tighter, or they can be a little looser. As long as they form a density-based clustering, you'll be able to find those m entities, and in the k consecutive time stamps, they move together, you can think they are convoy patterns. However, such movement constraints are still very rigid in the sense both require k consecutive time stamps. We may think like animal movements as certain time stamp, the animal may not be so closely clustered together. They may spread around to graze or to do other things. In that sense, we may relax this k consecutive time stamps to allow at certain time stamps, they probably are quite far apart. But on the other time stamps, you will be able to find they are very closely moving together. In that sense, we define such pattern as swarm. Swarm means the moving objects may not need to be close together, all the consecutive time stamps. Of course, to find such movement pattern, it will be more costly than finding flocks and convoys because the pattern is more relaxed. Some efficient algorithm has been developed to mine such pattern. The paper was published in VLDB 2010. Now we look at another trajectory pattern is during clustering try to find them. We call trajectory clustering. This could be useful in, for example, try to find land, try to forecast a hurricane landfall. If you overlay many years of hurricane together, you may find they may form very close clusters. However, if you try to take the whole hurricane paths as inseparable, you will not be able to find such patterns, just because at certain points of time, these hurricanes may already nicely. But at other time, they may become more spreading because they were influenced by different flow of the air. So try to find such patterns. We will propose a partitioning and a grouping approach. Partitioning means you will first chop these trajectories, for each trajectory, you will chop them into a sequence of segments, okay? Then after chopping these into sequence of segments, you will be able to find for certain fragments, they are moving in the same direction. They may form nice patterns. You can group them together as trajectory clusters. Then how we can nicely find such patterns? For partitioning, you can use minimum description lens principle called MDL. The MDL, the general philosophy is, you try to use minimum number of points, but maximally reflect the real trajectory paths. That means instead of thinking you use many, many small fragments, which are too costly, you try to use less of them. But you don't want to use too few, because you will distort the picture. You will try to maximally approximate the real trajectory pass. In that sense you may say I use minimum description length principle when the trajectory started turning in the sharper angle, I would try to say these should be separate points. Then you may find smaller number of fragments, but maximum preserve the shape of the trajectory. So this is interesting algorithm published in SIGMOD 2007 called Trajectory Clustering: A Partition-and-Group Framework. [MUSIC] [SOUND]
Now, we study another interesting movement pattern called Mining Semantics-Rich Movement Patterns. We'll first see what is Frequent Movement Pattern? Frequent Movement Pattern is a movement sequence, frequently appears in the input trajectory database, but we care more on the semantics here. For example, from home to Starbucks to office. So maybe people, the home maybe different or office maybe different location, but this pattern maybe interesting, even we can consider the home and office could be just the same semantic units in different locations. So comparing to Frequent Movement Pattern versus Frequent Sequential Pattern, we can say, they both are at finding frequent subsequences from input sequence database. However, for mining frequent movement patterns, we would like to think of similar places may need to be grouped together collectively to form frequent subsequences from the semantic point of view. Now, we look at Mining Semantics-Rich Movement Patterns. The semantics-rich patterns means in addition to know how people move from one region to another, we want to also understand the functions of the regions. That means we want them see people moving from office to restaurant or from home to gym. So, we promote a two-step top-down mining approach. The first step is try to find a set of a coarse patterns that reflects people semantic level transitions. That means as long as a reaching carry the similar semantic functions, we may one to consider they are the same function or same entity. Then in the step two, we split each coarse pattern into several fine-grained ones by grouping similar movement snippets together. This time, we'll consider spatial, temporal, besides the semantics. We're going to consider spatial and temporal features more seriously. A recent study published in VLDB 2014 called Splitter, Mining Fine-Grained Sequential Patterns in Semantic Trajectories is an interesting study in this direction. [MUSIC]
Now we study another interesting issue called Mining Periodic Movement Patterns. Usually the sensor may register movement data, but in many cases you may have very sparse data. Just giving you a very interesting example. For example, animal scientists or bird scientists, they would like to study animal movements and bird movements. They put censors on the body of those animals. But remember the sensor usually is pretty small, on the other hand it will last, it will need to be last for a long time. Like, for a study of bird migration, a sensor on the body of the bird is expected to last for a year. So, how could a sensor be lasting for a year without charging the battery? The only thing you can think is we will get very sparse data, like 24 hours we may only register one GPS location although 24 hours the birds may fly very far away. Then if we want to find the periodicity, the interesting thing could be it's very hard to find the periodicity if there's not overlap, just because the data is so sparse. Then just to look at how to, thinking about bird flying patterns, we just think the bird, suppose this place is the nest of the bird, okay. Then, if you just look at the bird, how they exactly fly with a very sparse sensor data, you'll not be able to find any meaningful periodic patterns of bird flying. However, you may find every night, every evening the birds were flying back to the nest. [COUGH] In the morning it will fly out from the nest. So, from the nest point of view or from the cluster center point of view it does have some periodicity. Then we may take such location as our reference spots. That means the reference spots can be detected using density-based method because they come back quite frequently. Then with this, if we do just based on the nest, we say when they're in the nest, when they're out of the nest, so we will be able to find a nice and interesting periodic patterns. That means the periodicity is more obvious when we look at the binary sequence like in and out of the nest. So that's interesting [INAUDIBLE] can see if we only think of a nest the periodicity will be able to be detected for each reference spot using Fourier Transform and auto-correlation. Now we look at an interesting example on mining periodic patterns with sparse data. This is a real data sets about birds flying in North America. So this actually accumulate three-year bird migration data, they are very, very sparse. However, if you based on the dense points, you take those dense regions as reference spots, you will be able to find a periodicity because in the winter they're going to fly down to the South. In the summer they may fly back to the North. In the middle they may fly some places to have some other activities, so you will find a few interesting reference spots. Once we detect periodicity based on those reference points, we will be able to summarize such patterns in a periodic way because we find a period and we find the nice periodic patterns. So this is an interesting study, based on the very sparse data we will be able to mine periodic behaviors for moving objects. Then the previous study we will assume the periodicity can be detected because the period is sort of knowing, okay. Then, in many cases, the time related data can be scattered and sparse. For example, phone calls at a location, they could be very sparse. And then for such sparse data, if we look at the time, we will be able to map the calling time in the sequence based on the time you're say 5, 13, 26, 29. If you map this into the time sequence, it's very hard to find a periodicity. Then what about if we can guess a right period, like t. If t is 20, if we guess it right, you probably can easily see they form a very dense cluster. However, if you got a wrong guess on the period, likely this mapping will be scattered along the whole period. So that means the projection on the true period, it will show highly skewed (clustered) distribution. Based on this observation, some in-depth study and algorithm have been developed to detect the true pattern in a mining event periodicity from incomplete observations. So in this whole session, we discussed a few interesting issues. We studied Mining Spatial Associations, Mining Spatial Colocation Patterns, Mining and Aggregating Patterns over Multiple Trajectories, Mining Semantics-Rich Movement Patterns and Mining Periodic Movement Patterns. In this session we actually used the following research papers. If you would like to know more, we strongly encourage you to read those research papers. Thank you. [MUSIC]
Now we are going to discuss another interesting pattern mining applications called Mining Quality Phrases from Text Data. We encounter massive unstructured text data. To get knowledge out of text data, a very important thing is mining quality phrases. We will discuss how we do it from mining frequent patterns to mining phrases. We'll reintroduce some previous phrase pattern mining methods, then we are going to discuss two interesting new algorithms developed by those. Many of my students, they are ToPMine and SegPhrase. ToPMine is mining phrases without training data, and SegPhrases by introducing tiny training data sets you can further enhance the phrase mining quality. Let's first study why we want to get from frequent pattern mining to phrase mining. The first question you may ask is why we want to do phrase mining. Actually, if we think every word is a unigram, and then phrases actually are a set of consecutive words made from some meaningful semantic unit. We probably can see, for single words, or we call unigrams, they're often ambiguous. For example, when you say united, people often do not know you mean United States or United Airlines or United Parcel Service? However, once you present them as a phrase like United States, or United Airlines, these kind of phrases are actually natural meaningful and unambiguous semantic units. That means if we want automatically transform text data to some meaningful sentences or to some meaningful analysis structures, the first important thing is mining semantically meaningful phrases? This would transform text data analysis from word granularity to phrase granularity. This will enhance the power and efficiency and manipulating unstructured data. However why we want to start with frequent pattern mining? The general principle is we want to explore information redundancy and use data-driven criteria to determine phrase boundaries and the salience instead of using laws of curation and training data set. The general methodology is to explore three ideas. One is frequent pattern mining and a colocation analysis. The second one is explore phrasal segmentation in documents. The third one is we will have some quality phrase assessment criteria. Using this, we will be able to work out efficient and effective phrase mining methods, okay. We are going to introduce two interesting methods developed just recently. One is ToPMine, is mining quality phrases without training date. This one was developed by Ahmed El-Kishky and his collaborators. The second one was SegPhrase, is mining quality phrases with tiny training sets has been developed by Jialu Liu and his collaborators.
Now, we first introduced a few previous phrase mining methods. Phrase mining actually originally came from the natural language processing community called chunking or noun phrase chunking. Essentially is a remodel of phrase as a sequence of labeling problem. For example, you can squeeze a label word at the beginning of this noun phrase, then you see another word that could be inside of noun phrases, and you may have something outside of this phrase. This will need a lot of annotation and a training effort. For example, you may ask domain experts to annotate hundreds of documents as training data. Then we can use the software, use the measures to train a supervised model based on part-of-speech features like part of speech tracking. Okay, the recent trend is also try to use web n-grams. Those web n-grams,we can get some statistics, we'll be able to use this distributional features to generate good phrases. This one was found by Bergsma and others in 2010. The state-of-the-art performance using this approach is about 95% accuracy and about 88% phrase-level F-score. However, this method suffers from the high annotation cost. And then it will not be scalable to a new language for example, new labor English, it may not be used for French or a new domain like you labor the news may not be able to use for computer science or new genre, okay? So it may not fit domain-specific, dynamic, emerging applications. For example, there are lots of new articles in scientific domain, computer science domain or like medical domains. Or the machine may generate a lot of query logs. Our social media may contain Yelp and Twitter data. This kind of data set is so domain specific and dynamic, so it's hard to assign or change every subset of such data using annotation. So people want to develop unsupervised phrase mining method. Actually, many unsupervised phrase mining studies are close linked to as topic modeling. Topic modeling essentially is a new recent developed method. Represent topics by many, many phrase words. You get every topic is written by a word distribution, and documents a set of documents actually may contain many topics with different proportions. This method does not require any prior annotations or labeling of the documents. It relies on statistical topic modeling algorithm. One of the most popular ones, LDA, or we call Latent Dirichlet Allocation developed by David Blei and others in 2003. Actually to do phrase mining using topic modelling there are three different methods. One is we simultaneously infer the topics and the tokens, the phrases at the same time using some kind of generative model. The second method is first using generative model to generate topic models and using topic model, try to group different words into n-grams, into phrases to visualize such topics. The third one, the newest developed one is first we phrases mining before we do topic model. That means, we mine phrases and impose such phrases on the bag-of-words modelling process. So let's look at the first method. We call strategy 1 is simultaneously inferring phrases and topics. There studies the first one in 2006 was called bigram topic model. That means we use some kind of probabilistic generating model to look at the previous words in the topic to see whether the next word should belong to the same topic should be a phrase. And it's generalization from this Bigram Topic Model is TNG or called Topical N-Grams model. It was developed in 2007. It is a probabilistic model. Generates words in textual order, then create n-grams by concatenating successive bigrams. Then the source that is by Lindsey and others into some twelve called PDLDA or Phrase-Discovering LDA. For this measure, we first view each sentence as a time-series of words. Then using PDLDA, and assume that generative parameter like those topic may change periodically, then each word is drawn based on the previous m words, based on the previous context. And the current phrase topic, so we can generate whether this word should be there or not. Okay, this method, due to its high model complexity, it tends to overfitting. That means it overfitting to a particular topic. Sometimes it may not generate very good quality phrases. Another why it has high inference cost, because of this complex generative process, it runs pretty slow. The second strategy is post topic-modelling phrase construction. That means refers topic modeling, then we do phrase construction. The first that work called TurboTopics. TurboTopics was developed in 2009 by David Blei and John Lafferty. The general idea is first to Latent Dirichlet Allocation to do topic modeling, then based on topic modeling we do post-processing to construct the phrases. Okay, that means we first perform LDA on corpus to assign each token a topic model. This is topic modelling. Then, we'll merge adjacent unigrams with the same topic model by a distribution-free permutation test on arbitrary-length back-off model. For example, you can see this. If you see phase, transition, and you see phase transition are in same topic number 11. And they are consecutive, likely they could form a phrase. You can recursively merging generating such phrases. We will end recursive merging when all significant adjacent unigrams have been merged, so then you will be able to generate some good phrases. Another method called KERT is also first do topic modelling them do phrase construction. It was developed by Marina Danilevsky and others in 2014. It take first do some LDA, and then to post-processing for phrase construction. The first one the same is turbo topics, then the interesting thing is it performs frequent pattern mining to extract candidate phrases within each topic. Then perform phrase ranking, based on four different criteria. One criteria called popularity. For example, if you see information retrieval is more popular, then cross-language information retrieval, then information retrieval as a phrase will rank higher than the cross-language information retrieval. The second criteria is concordance. For example, if you see strong tea quite often rather you see powerful tea, or you see active learning quite often rather than you see learning and the classification getting together as a sequence, then we will say strong tea and active learning is higher concordance. The third one for informativeness essentially is, whether the phrase generator is more informative, it contains retrieved information, discriminative. For example, if you see this paper, it's almost every research paper, they have a lot of this paper, so this paper is more like a stock words here is more like a stock phras,e it is not informative. It rank low, like it should be removed. The first criteria is completeness. For example, you see vector machine, almost every time you see this phrase, you'll also see support vector machine. That means vector machine may not be complete, and support vector machine is a complete phase. Then not only you can compare phrase quality with the same lens, in many cases you can compare phrase quality of mixed lens. That means you compare vector machine vs support vector machine. Rather than using the same lens, you compare them. With this process, frequent pattern mining plus phrase ranking, this message generate better quality phrases.
Now we introduce an interesting phrase mining method called ToPMine which mines quality phrases without training data. This method actually belongs to strategy three as we discussed. We first do phrase mining, then do topic modeling. Why we want to do that way? We can see if we use Strategy 2, we first do topic modelling, then try to do phrase modeling, we may encounter some problems. For example, let's look at this example. Suppose we first do topic modeling in this one. But remember, the topic modeling you will regenerate a larger number of topics. Suppose knowledge discovery, you see knowledge is in the red topic and discovery is in the blue topic. Okay. The same thing as support vector machine, you may get a support as in one topic vector is another and the machine is in the topic the same as support. If that's the case, we'll never be able to generate knowledge discovery also for a vector machine is quality phrases because they belong to different topics. How can we fix this problem? If we switch order between phrase mining and topic modeling That means we first two phrase mining if we can find knowledge, discovery is a quality phrase. A least a squares is another quality phrase and support vector machines assert quality phrase. Then, we will be able to generate the very high quality topic models for example knowledge discovery is one topic, least a squares is another topic Support vector machine is another topic. Can we do that? Okay, so the trick is we can first do phrase mining. Then we do document segmentation. Then we do phrase ranking. After that, we take those phrase as constraints. Then we do topic modeling inference then we will be able to get those high quality phrases and high quality topic models. Let's look at this method top mine method. The method was developed by Ahmed El-Kishky's and his team, In publishing VLDB 2015. At first do phrase mining then do phrase-based topic modelling. The phrase mining is first mine frequent contiguous patterns that only can extract these candidate phrases and their counts. This method is very much like a typical frequent pattern mining methods. Then we can agglomerative merging those adjacent unigrams guided by a significance score. We are going to ensure you see all later, okay? Then we can do documents segmentation. That means we will work out to the base of raw frequency based going back to the document to see which one we count which we get rectified true frequency. For example, just give you a simple example, suppose we have support vector machine as a phrase. It occurs 90 times. Then vector machine, as a phrase it occurs 95 times. Support vector occurs even 100 times. However you go back to the documents you will find if you count support vector machines it's a phrase. You will not recount vector machine as independent phrase or support vectors, independent phrase. Actually you'll decrease their count substantially. That the count is we call the rectified frequency. Analysis can be based on rectified frequency instead of raw frequency. Now we can rank those phrases based on popularity, concordance, informativeness and the completeness as we just discussed in KERT. After this we can do phrase space topic modelling. That means we can, taking those mined bag of phrases. We pass them PhraseLDA which is the extension of LDA. This method will constrains all the words in a phrase. So, each sharing the same latent topic. In that way, we'll be able to divide better quality topic models. This matter essentially doing collocation mining, then is, we were to find a sequence of the words that occur more frequently than expected, then we would say they likely should be a phrase, okay? For example, if you see "strong tea", okay? You see it happens frequently together. Then you look at a strong, how many times happens tea, how many times it happens. Then you see strong tea happen together more frequently than expected frequency. That means strong tea should be a phrase. How can we measure such things? We can measure this based on different measures. For example, the mutual information, okay. And this essentially is paralyzed mutual information. And we can do t-test, can do z-test, can do chi-squared test, can do likelihood ratio, actually using all the different measures we can confirm, the phrase would generate a truly good phrases. Usually use any of the good measures, little work. That means many of these measure can be used to guide the agglomerative phrase segmentation. Let's look at the example. Suppose we have a bunch of documents, one says Markov blanket feature selection for support vector machines and all the others. How can we know without training to see feature selection should be a phrase, support vector machine should be a phrase, Markov blanket should be a phrase. Actually the trick is very simple, if we think about the normal distribution, the Gaussian distribution, by basic statistics we know, if you've got a Gaussian distribution, you get a one standard deviation away. Actually it's about the chance to get this is one minus 68.2%. But, you get three standard deviations away. The chance is very, very small. It simply says you only got 0.3% of chance to have three standard deviation away. And then how do you explain it? Likely they should get together, as an exception, but not as a random distribution. They should be a phrase. Okay. We can use some significant score like a church developing in 1991 right on this former. This is very much in the same spirit as the Z score. You know how many standard of deviation away. Suppose in these documents we found a support vector getting together is about 12 standard iterations away, okay? That means that the support vector coming together so tight, so unusual they should be a phrase. Even you merge support vector as a phrase, then you still calculate this machine you find there are still eight standard deviation away. Simply says Support Vector Machine getting together should be a phrase. The interesting case could be you see feature selection some people see selection for which when should be a good phrase. Actually if you check the whole document you'll see features selection likely should be a good phrase. Because they're getting together is more than expected. Okay, then you team up with feature selection as a phrase. In that case it's selection for would not be a candidate in this case. If we do phraser sequestration, features selection will have a count. Selection for, in this particular sentence, will now contribute to a count to selection for, because feature selection is already there. Okay so if we do the good ratification of the phrase frequency, we'll be able to make a very good judgement. So we did some good experiments you probably see. If we use DBLP which is compromise research publication bibliographic data base. We actually can see the unigram generated and the n-gram generated has already better quality than purely topic modelling. However you see this n-gram generated very informative. For example, almost anybody working on natural language processing, you probably can see natural language, speech recognition, language model, natural language processing, machine translation. All these become very, very good top rank of phrases for natural language processing. That implies this phrase mining not only helps in generating quality phrases but also generally better quality topic models. That means ToPMine is efficient and generates high-quality topics and phrases without any training data. Similar experiments on Yelp. Yelp actually is a social media data sets. For Yelp reviews, people writing something not a quite formerly but still we probably can see the phrase generated are actually pretty high quality. And for example if you look at this the Mexican food, the chips and salsa, hot dog, rice and beans you will see these are the very typical Mexican food you can generate using this and phrase mining. First, do phrase mining, then do topic mining. One thing we should note is, for example, food was good generate here. Food was good actually is a sentence. It's a short sentence, it's not a phrase. If we add a little more, for example, part-of-speech tagging, we'll be able to rectify it. Carried out this food was good as a short sentence, it's not a phrase. The remaining ones are pretty good. So, that implies top mine, and it can't further integrate it with some natural language processing features. And we will be able to generate pretty high quality phrases for the subsequent process.
Now we introduce another interesting phrase mining algorithm called segPhrase. This is phrase mining with tiny training sets. We have introduced ToPMine. ToPMining is mining phrases without any training data. However if we have a small set of training data, it may substantially enhance the quality of phrase mining. This was done by Jalo Liu. He and his collaborators working on Mining Quality Phrases from Massive Text Corporate Data, and they published their work in SIGMOD 2015, is as follows. If we have a raw corpus, if we use a small set of training labels by human. Of course, later they extended to get a general knowledge base like Freebase or Wikipedia. But in this algorithm we first just study, suppose we got a small set of labels by human, like 300 labels. Then we feed them into a mining process. We may be able to get a quality phrases and also get a segmented corpus. And a quality Phrases and segmented Corpus can mutually enhance each other by phrase or segmentation process. So finally, we'll be able to get a high quality phrases generated. So we look at the overall framework. And the overall framework, the first thing they introduced comparing with ToPMine is first to frequent pattern mining, feature extraction. But in the meantime, they introduce a classification process. The reason they introduced the classification process is because they have the labels. The label can, based on the feature extracted, the classifier will be able to say we can put more weights on the positive labels. And then put some negative labels as a back phrases. We will be able to work out a classifier. Okay, that means SegPhrase itself use a classifier, okay, and small labeled datasets. This datasets could be provided by experts or by a distant supervised knowledge base, like Wikipedia or DBPedia, okay. Then If we look at the SegPhrase is doing phrasal segmentation and phrase quality estimation. After that, they will go back and feed into this classifier again. To get one more round to enhance mined quality phrases. Okay, so that's why they call this one more round after phrasal segmentation and phrase quality estimation, to feed back to the classifier again as SegPhrase+ process. Let's look at how they do it. The first thing they did is doing pattern mining for candidate sets, that means using frequent pattern mining algorithm to get a candidate phrase sets. That means you mine frequent k-grams. This essentially is frequent phrases, but k is typically small, for example 6 grams. Of course, for some other applications that I have, some medical applications, k could be even bigger. The popularity measure is by raw frequent words and a phrase mined from the corpus, this popularity. Then doing feature extraction based on the concordance. That means whether you partition the phrase into two parts and check whether the co-occurrence is significantly higher than the pure random. Then another feature extraction process Is informativeness. That, essentially, is you assume the quality phrase typical start end with a non-stopword. For example, you say, machine learning is, machine learning. Likely machine learning is a good phrase. Machine learning is, this is stop. Where it starting, I'm ending with a stop words. Like this actually is a non-stopword but is a stopword, so you basically say, this is, is not part of the phrase. Another method or heuristic is using average IDF. IDF is inverted document frequency over the words in the phrase to measure the semantics. Simply says if this phrase is frequent almost in all the documents, across all the documents in the corpus, then this IDF is too high. And it may not be very good at quality. And if this phrase is, actually, occur more frequent in some documents but less frequent in other documents, it becomes more interesting, more distinctive. Then another heuristic, the algorithm users is considered the probability of quality phrase in quotes, brackets, or connected by dash or hyphen should be higher. For example, like state-of-the-art actually is, it should be a phrase rather than syncing these several different things. Then SegPhrase uses a classification process. It explores a tiny set of training data. Usually four gigabytes corpus. You just need it to have your labels. But what kind of label it is? For example, you may say, support vector machine is high quality phrase. The experiment shows is not a good phrase. You just based on the quality of one or zero. Then we can construct a models to distinguish quality phrases from the poor ones. And this classifier [COUGH] actually in it use Random Forest algorithm to boost different datasets with limited labels. So you do not using very small number of labels. Then you'll miss coverage of the others. So the Random Forest usually gives you less bias. Then phrasal segmentation can tell which phrase is more appropriate. For example, you look at this one, you say a standard feature vector machine learning setup is used to describe something. Actually, instead of thinking this, like if you say feature vector here, it's a good phrase. In this particular one, the vector machine should be not be combined, but machine learning should be combined. So that simply says when you rectify it, you will not say vector machine actually got as the real count. But the feature vector is a good count, and machine learning is a good count. Then we can partition a sequence of the words by maximizing the likelihood. And consider length penalty. That means you get too long, maybe it's not a very good phrase. Filter out phrases with low rectified frequency. Then if the process is a classification, then do phrasal segmentation. This part is the SegPhrase the first one. And for putting plus, actually it's you feed this phrasal segmentation process into classification again, then you do phrasal segmentation again. This actually derive even better results. We call this one SegPhrase +. Now we probably can see the experimental results. The data actually used was DBLP and Yelp was in millions of documents and tens of millions or hundreds of millions of words. But every time, we just used this 300 human label phrases for training. And then fo r the phrase evaluation, we use Wiki Phrase and sampled 500 times 7 Wiki-uncovered phrases with evaluated by reviewers. Then you probably can see the readers comparing with other algorithms. And you can see comparing to a few comparative algorithms. Of course, it may not be a very fair comparison if you think TopMine does not use any training at all. Well, anyway we put it down there to give you a relatively where TopMine is if we do not use any training data. So this is a precision recall curve for DBLP data using Wiki Phrase as a label to check it. And the second one is based on DBpedia, not using Wiki Phrase but using now Wiki Phrases. For the Wiki phrases, you probably can see the difference is pretty high. Now, Wiki phrases, the phrase read out. Actually, it's, you probably can see it's overall, this method generate a very high quality comparing to the others. And also Segphrase algorithm is efficient, is linearly scalable. So we look at some real results people can see. To my, from the titles and abstract of ACMCKDD, the KDD proceedings, you probably can see, we just need a title and abstract by using SegPhrase and comparing with chunking method using TF-IDF and C value. And these red ones are phrases only generated in SegPhrase+ but not in chunking, these red ones. And these blue ones, those generated only in chunking but not in SegPhrase. But you probably can see in that case, you'll see like for KDD, time series, gene expression data, frequent subgraph, categorical attribute. These are meaningful good phrases for KDD. But if you look at a important problem, effective ways, small sets, these are the phrases almost appear anywhere, not just in KDD. So that means chunking methods may not generate as high quality phrases as like SegPhrase+. And the interesting thing is both ToPMine and SegPhrase+ are extensible to mining quality phrases in multiple languages. For example, this is a Chinese language from the Chinese Wikipedia. And we probably can see, these Chinese phrases are pretty high quality is generated using SegPhrase. And it can, my Arabic language, even without the pre-processing, you probably can see the results TopMine can generate those phrases without any training and pre-processing, and generate some quality phrases as well. So in summary, we have studied pattern mining applications, mining quality phrases from text data. We studied why frequent pattern mining could be useful for phrase mining. We discussed previous phrase pattern mining methods. We introduced two new algorithms. One is ToPMine, one is SegPhrase. And this is a set of references, those published papers, we have been cited and compared in our studies. Thank you.
[SOUND] So, let's first discuss frequent pattern mining in Data Streams. We know in current big data era, besides we get a huge amount of data stored in database systems, in file system, on the web, but also we have internet of things or internet of sensors. So, in those kind of scenarios, there are lots of stream data. The stream data, the feature usually is the come in and go continuously. They are ordered in the sequence but they are changing dynamically. They come and go very fast but in very huge volume. This is very different from traditional finite persistent data sets stored in the file systems, in database management system or on the web. So, now let's look at the major characteristics of data streams. First, data streams are usually repeated huge volumes of continuous data. They could be potentially infinite like the online sensors, they have no ending at this part. So they are fast changing, they may also require faster rear time response like anomalies or emergencies. And then for data stream usually captured nicely of our data processing needs of today because even in data systems, the data could be so huge. You may only want to scan them in a sequential manner live data streams or you don't want to keep on repeating, getting them back again. Okay, so to that extent that even in the large data stored you may also process them. In the data stream manner, not to say you really have to enter large amount of sensor data. So, in general, we say for processing data streams, random access is expensive. Because of the data come and go, you don't want to fetch back, or you can not fast forward as well. So we usually call these algorithms developing data streams, called single scan algorithm. That means for any particular data set, particular page, you can only have one look. So another interesting thing is the data stream in many cases you may like store some kind of sketch or store a summary of the data seen so far. Instead of storing the detailed data because you don't have such volume or such processing power. Then another important challenge is most data streams actually are coming at the very low level like the particular sensors. And they also come in the multi-dimensional feature, for example, even for the data they may come. Some part would be the temperature, some part it could be video, some part it could be audio, some could be the text data. It's multi-dimensional in nature but for the information processing needs, usually you want summarize or process the data and pattern in a multi-level and in multi-dimensional way. So we can see there lots of research challenges data stream processing. If we look at the Architecture of data stream processing system usually we called this as Stream Data Management System. What we have is we have a stream query processor. You'll have multiple data stream streaming into this processor at the same time in a continuous way. But even for User/Application programs they usually post Continuous Query rather than ad hop words. It's like a [INAUDIBLE]. They won't see anything abnormal. They want to summarize the data in a multi-dimensional space. Then for this Stream Query Processor usually you have some scratch space that could be a big main memory, or even plus some disks. These markable streams coming down here, where be summarized the processed, especially answer your continuous query. The results will be streaming back to the user's application programs. So this is a typical way to process streaming data online in the real time, okay. The problem becomes for frequent pattern mining. How can we find the frequent patterns in these data streams? Actually, it's pretty challenging. When we look at the major differences between stream mining versus stream querying. The first thing is stream mining and steam querying, they share many common difficulties. For example, you can only do single-scan, you need a fast response, you have to handle dynamic, noisy data. But for stream mining, usually you want to see the global picture, you want to see patterns, you want to find the clusters. It also requires less precision than the stream querying because stream querying one the ping pong to a particular point. You may need to perform join, grouping, sorting which actually in streams join and sorting those algorithm is actually pretty difficult. Because you cannot see the pass data, you cannot predicted the future of incoming data as well but patterns are hidden. They are more general than querying, so their processing is different. Actually, for stream data mining, there are lots of research and development activities already. One branch actually studied pattern mining in data streams. Another is doing the multi-dimensional on-line summary of data streams and also data stream can be clustered dynamically. Can do dynamic classification, can find outliers and anomalies. So there are lots of research in data mining, all these frontiers. So what we are studying in this lecture, we will only touch pattern mining in data streams. So one important thing is for mining frequent patterns. In stream data it is unrealistic to try to find a precise frequent patterns. Simply says the past precising may already been gone, it's pretty hard to capture them. The future one may not be coming yet. So it's pretty hard to predict them and even further why you can hold just because they are so big you cannot store them even in a compressed form like FPtree. So what we expect is trying to find approximate answers but in many cases, approximizer may be sufficient for our analysis purpose. For example, you may find a router could be interested in finding the flows. Those flow in the network you may find whose frequency is at least 1% of the entire traffic stream seen so far. If you identify those patterns, those frequent occurring flows you could be pretty happy. On the other hand you may say for this sigma if you give me the count is probably a little less like over 1/10 or you would say the error rate is 0.1%. You probably feel is very comfortable, you think you're doing good thing already. In that case, we may develop some very efficient algorithm to mine such patterns with good approximation like this. Then in this lecture I'm going to only introduce one algorithm around this which is called Lossy Counting Algorithm developed by Manku and Motwani in 2002. The major idea is not to keep all the items especially not to keep the items with very low support count. That means if they are very low they unlikely will reach the frequency [INAUDIBLE] you do not even keep them. Okay, the advantage is you can guarantee some error bound. That means I [INAUDIBLE] find all the frequent items but of course you need still to keep really large set of traces, your buffer size should be really big. Let's look at Lossy Counting Algorithm but we only introduce frequency single items. They did study multiple item sets for the simple explanation of the idea refers to the frequent single item counting, okay. Suppose you get a very, very huge data stream, you may divide this stream into buckets. But the bucket size could be one over epsom, because if your epsom is 0.1% error bound. Then the bucket size could be 1 over epsilon means, 1 over 0.1% is 1,000, means each bucket should have 1,000 items, okay. Then this one item we assume that the main memory is size is big enough, you can easily hold this one bucket. Then at the very beginning, when the first bucket comes, the main memory, at the very beginning you can assume for this part is empty. This summary is empty but then you get the first bucket you get 1,000 items. Then you start counting them for example, there are four red ones, there are two yellow ones, one green one, one blue one, black and so on. So at the end of this bucket boundary we will decrease all the counters by 1, you probably can't see. There are many if they only appear once like the green one, a blue one, the black one. They all gone, because it decrease a counter by one, a counter is zero, they all gone. Even for red ones you get four, now the counter becomes three. You get two yellow ones, the counter actually is only one. So that means you your counter actually get a less than the real one. When the next bucket of the stream coming, you'll probably empty a lot of space because the counter was not existing there too. There are too few items in this color. Then, you may get additional red ones and yellow ones or even black ones. So you can see at the end of this bucket, then you will do the same. You decrease all the counters by one. So those, not frequent, they are gone again. So the yellow why you can see, originally you got a one. You get one more but you decrease by one and so you keep one. The black one is one, the red one you get a little more because the red one is so frequent. After you know youre thinking bucket by bucket you get many, many buckets you still keep this counter 1,000 counters. Because you get 1,000 buckets at most you have 1,000 counters you only get a number so the space is actually quite limited. Then finally we want to output the frequent patterns. The interesting thing is whether we will be able to output all the frequent patterns if they are frequent in these data streams, but maybe we have some reduced count. So, actually, suppose the support threshold is sigma, the error threshold is epsilon, and the stream length is N so far. Then, the output are those items with frequency counts exceeding sigma- epsilon times N. Simply says if for any item you find it is conquered, was this number or bigger your output is a frequent item. So the key is because end of each bucket we decreased the counter by 1, so we were undercount something. So the question becomes, how much do we undercount? So if the stream length seen so far as N, the bucket size is 1/epsilon. We can easily determine the frequency count error should be no more than number of buckets, why? Because for each item for every bucket you decrease the count by what? If some item come, even later, at the very beginning they did not come, you did not decrease them. So to that extent you'd decrease less than number of buckets, so if they come even at the very beginning, the first bucket is they are on the way to create the count by number of buckets. What is the number of buckets you see? Because you have seen N elements and the bucket size is one over it's epsilon. So N atom is over bucket size is number of buckets which is N over 1 over epsilon. So you get epsilon times N, that means that the frequency count error at most is epsilon times N. We know epsilon is pretty small, so the error count error is not that big. Then we look at the counting. They have some interesting property we call approximation guarantee. The first thing is there's no false negatives. Simply says if the item is frequent, it will be captured an output, why? If we can see, is if your item is frequent, that means your total support threshold should be sigma times N or more. Then, because we output a frequent item exceeding sigma-epsilon times N, we know we already undercount this part at most. That means actually since you output all these you will output every item whose support count is no less than sigma times N so there's no false negatives. But there could be false positives where the false positive comes just because probably in sometimes they come later like you may have one color okay suppose orange. They come at a very late stage. Okay so they actually the previous increment by one actually it did not tax them. So now they really have this support, sigma minus epsilon time N. You actually offer this, say this one could be a frequent item. Yep, okay but this orange one. Actually, you, the real support is this. They do not suffer the decrement or only suffer one. So then your support count, this one, actually in principle is not frequent in rear. But you think they are frequently just output as a frequent item. So the false positive have this two frequency a list of this so that means the orange wire have the two frequency of this one. You still count them as a frequent item so you do have some false positive. But a frequency count underestimated by at most epsilon times N, because we already have this. So, you have some nice guarantee for, finally you have just this bucket size as your main memory. You actually can return all the frequent items and it may contain some false positives. It may have some underestimate of your frequency count. But still it is a nice, elegant algorithm with very limited resource you can handle data streams. Of course, after this there are lots of studies by improving, for example, improving lossy counting algorithm. One interesting study is by Metwally in 2005, they do space-saving computation of frequent and top-k elements. The general philosophy is, they may tried to use more memory space if you really have bigger memory. You don't have to restricted down to one over epsilon as your bucket size but that we are not going to discuss this in more detail. Another interesting thing is in this lecture we only discuss frequent one item sets. That means frequent item. We discuss how to mine the approximate one. Actually in Manku and Motwani paper, they also discussed frequent k-itemsets. How to maintain counter, how to do Lossy counting. But due to limited time we will not introduce this algorithm. You may like to read the paper by yourself. Then there are also some subsequent studies on how to mine sequential patterns in data streams. It is a pretty challenging task as well. So I'm going to only introduce you two papers, one is Manku and Motwani's paper, another is Metwally's paper. You may want to see how to my streams streams in a very interesting way. Thank you. [MUSIC]
[SOUND] Now we examine another interesting line of pattern discovery applications. There's pattern discovery for software bug mining. We use software all the time. But we also know software may contain bugs. But software usually could be quite big, quite long, and its running data could be even larger and more complex. So automatic debugging, finding software bugs, it's a very challenging issue because often, there's no clear specification or property we can rely on. We have to invest substantial human effort first to analyze data, analyze the source code, and analyze bugs. So, if we can have certain automated tool to do software reliability analysis to find bugs, it will save a lot of people's effort. So for bug detection, there is one category called static bug detection, that means you check the source code and try to find bugs. Another code dynamic bug detection or testing is you run the code, you try to detect where could be the bugs. The debugging usually is you give me symptoms or failures, I try to pinpoint the bug locations in the code. If we can do good pattern mining because the code are running sequences that may contain some hidden patterns. If we can do pattern mining, we will be able to find some common patterns and some anomalies. Usually the common patterns likely are specifications or properties, but the violations are anomalies comparing to the common patterns, likely bugs. So if we can do pattern mining, we mine the patterns, it may narrow down the scope of inspection. For example, usually the code locations or predicates that happen more in the failing runs. That means every time it fails, it likely sees somewhere you can see it happens more for certain predicates or happens more for code locations, this kind of patterns. But it happens less in the passing runs. And likely, those code locations or predicates are suspicious bug locations. That may narrow down the scope for the bug inspection. So the typical software bug detection methods contain their mining one, could be mining rules from source code, or may find deviant behavior. Those are bugs by a statistic analysis, you may find some source coded substantially deviate from the others, or you may mine some programming rules. For example, you first do a variable value assignment, then you do the condition testing. So if you use frequent itemset mining, you may be able to identify those typical program practice or rules. Or you also can mine function precedence protocols. Simply says, if you find some functions always precede some other functions, so you can use frequent subsequence mining to find such patterns. And also, you may reveal neglected conditions you base on the different condition whether it would be tested or never touched. You use frequent itemset or subgraph mining, you'll be able to find such patterns. Another interesting practice is mining rules from revision histories. You'll look at sequence of the program coded revisions. You'll use frequent itemset pack mining. You may find certain general practice or certain anomalies. Actually we are going to focus on one interesting bug. We study, try to find copy and paste bugs. So we try to mine copy-paste patterns from the source code, then we try to find the copy-paste bugs. This one actually was one interesting piece of work published in 2004, called CP-Miner by YY Zhou's group. And we are going to just show this, this interesting idea how to extend pattern mining to find copy and paste bugs. So copy and paste bugs is very common. For example, some statistics show they are 12% of the code in Linux file system are copy and paste code, and 19% of the code in X Window system, also copy and paste. But a copy and paste may introduce errors. For example, the typical one called forget-to-change bugs, that means something like this code, okay. If you see the first part, the upper part, the for loop, okay, this part and the down part, another for loop, these two actually, if you see many other things are just the same, but there are something changed. So you likely, this part, the second part, was copy and paste from the first part, and then do some changes. But the problem is when you do global change, sometimes the programmer may forget to change something like here, okay. It means all the total in the front has changed to taken, but the last total and the second occurrence in this statement forget to change to taken. So this is a very common practice because the carelessness of some programmer. But how can we find this kind of bug, actually this bug has really happened in Linux 2.6.6? Okay, then we will see how to automatically find such bugs. The general sequence, the general way to solve this problem we will be detail later. As we may build a sequence database from the source code, mining such sequential patterns. Then when we find some mismatched ID names, then the bugs as well, okay. This one was done by YY Zhou's group that time she was at USC, so it's very interesting study using pattern mining. So we first see how to build sequence database from source code. So the general philosophy is for each statement, we may match the statement into a number. But how we can match the number, for example, this two are 16, the other two are also 16, how we can find such similar sequences. So the methodology is we tokenize each component, like just give you an example, like old is assigned to 3. And the variable new is also assigned to 3. How we can? We based it for different operators, constants, or key words, remain mapping to different tokens. But for same type of IDs, we map to the same token. For example, old and new are both same type of ID, these are both old new. We do not care they are different IDs, we just map them into the same token 5. But it's assigned to, these two map to 61. Then the value like 3 we map into another token like 20. Then these two statements, if we take the same sequence, we give them a hash value. These two statements should have the same hash value, simply say, these are very similar statements. So if we do this, either you'll see this total[i].adr and the other one is total[i].bytes, they essentially map to the same thing, they got the same sequence. So, then a program is mapped to a long sequence, we can further cut the long sequence by blocks. For example, we can find at least, fragments of the program is matched to a sequence in the same block, you probably can see this is the map into sequence database. Then we can do sequential pattern mining to detect forget-to-change bugs. How to do sequential pattern mining? Remember the previous sequential pattern mining allow arbitrary number of gaps in the middle. This is derived from the customer shopping sequences. But, for programming, for finding a sequence in the copy and paste bugs, you want to have exactly one because you allow people to insert some programming statements right in the middle, you count these as same kind of sequences. But you don't want them to insert too many statements in the middle, because they likely they could be complete different things. So we can constrain the maximum gap, like the small number of statements in the middle. Then we will still say these two sequence of the code are the same sequential pattern, okay. Then we can compose larger copy and paste segments because they have many things in common. We can combine the neighbor copy and paste segments repeatedly. Then we may try to find conflicts. Essentially, we try to identify those names that cannot be mapped to the corresponding ones. For example, if you see here, you may see the concrete identifier, F mapped to F1, this F also mapped to F1. But the other F is mapped to F2, so there's a conflict. So we try to calculate the ratio. Suppose if we find a 1 out of 4 total is unchanged, so the unchanged ratio is only 0.25. If you set an unchanged ratio, if it's greater than 0, it means something did change. But unchanged ratio is less than specified threshold, then we can say this likely could be a copy and paste bug, okay. So this one actually was done by YY Zhou's group. They work at CP-Miner. They mined the common source code, like Linux, Apache, Postgres. So they actually got to find many copy and paste bugs out of millions of lines of code. And you probably can see this sequential pattern mining is very efficient. That means that within a short time, you can browse through millions lines of code, finding such copy and paste bugs. And based on their human identification, these are the real bugs. So it's very interesting to see this pattern mining can help finding software bugs. [MUSIC]
[SOUND] In this session, I'm going to introduce you another interesting applications of Pattern Discovery called Pattern Discovery for Image Analysis. We know image analysis or computer vision, there have been lots of technology developed in this field. Then for, there's one interesting recent line of work is using visual pattern discovery to do image analysis. We can take this picture as a example. For this image, we may find there are many interest points. These interest points we call visual primitives. Every visual primitives can be described by a set of visual features or you can think is a high dimension feature vector. And then, each image is a collection of visual primitives. Then, we will study how to turn those visual primitives into patterns and how to do pattern minning on large number of images. So that means we want to study visual pattern discovery problem. For visual pattern discovery, we can think about it from this images. We can use feature extraction. We can extract lots of high dimension features, they form visual primitives. Those visual primitives can be clustered, based on their space they may cluster into visual items, okay? Those visual items, if they are similar, then they belong to the same item. You can map them into different item ID, for example. You can map this feature into W, and maybe another picture I have of similar one, they actually also called W, okay? Then each visual primitive, they can have, based on that k-nearest-neighbor, they can form small clusters, and those we usually called them a transaction. For example, you may find these three visual primitives can form a transaction like CBA as a transaction. Then an image may have many transactions, okay, in every image. Then if there are many, many images, you may, based on their transactions, you do the frequent pattern mining, you will be able to find some visual patterns. For example, in these pictures, you may find a BA and CA, actually are quite frequent together in those transactions. So you can say, B and CA or CBA are frequent patterns, are the frequent visual patterns in those images. So the simple example could be this, suppose we have four pictures. Each picture you may find note, certain primitives, visual primitives from items. Those items based on the k-nearest-neighbor, they form transactions. Then for this car, you may find the two transactions here, G12 and G13, okay? Then they contain a set of items, EDABC, could be the transactions containing those items. Each item actually is a set of visual primitives from the items, okay? But if you do frequent pattern mining, you may find among those images, you can get an AB as frequent item set and actually these AB wrapped in something related to the wheels of the car. So that's a reason those frequent pattern mining can help identify certain unique features in different images. So then the interesting thing could be, because images are spatial data, then spatial configuration does matter. Then because sometimes those transactions spatially very close, they may overlap each other. Then they may have over counting problem. For example, this AB actually in this image, there is only one such AB pattern. But since based on your feature image there, clustered together, you get two transactions, you actually count AB twice in this picture, okay? So, there's a over counting problem. Another problem encountered in this visual pattern discovery is uncertainty problem. Because you may have some, like those wheels can be partially occluded by other objects or mixed with other objects, we have lots of noise, okay? Some feature you may not be able to detect. Another is visual synonym and polysemy problem that essentially is sometimes the same feature actually may erupt in different objects or one object can be represented by different kinds of feature sets, okay? So we need to revise our frequent pattern discovery algorithm to handle these over counting problem, uncertainty problems, synonym and the polysemy problems. Actually, there are interesting studies in computer vision community, they developed some interesting visual pattern discovery methods for images and video data. So if you are interest in this, I would recommend you study those research papers to gain your knowledge to certain depths. [MUSIC]
[SOUND] In this session, we're going to discuss another important issue. &nbsp;Pattern Mining and Society. Especially, we will be focusing on the privacy issue in Data Mining. We know, we are living in a data rich or big data society. This of course create, you know, create opportunity for a successful data mining. But on the other hand, people do have concerns whether pattern mining may violate people's privacy or security. Okay. Actually, data mining may naturally have potential adverse side effect is compromise the privacy. The privacy and accuracy are typically contradictory in nature. That means, if you want your pattern mining very accurate. In many cases, the privacy may be compromised. But if you want to protect your privacy in a very tight manner, then the accuracy of the pattern mining may suffer. Okay. So the problem is we need a new technology to privacy preserving data mining. We ensure the data mining will achieve good results. But in the mean time, we're going to protect people's privacy. There are lots of research working on this issue. We call privacy preserving data mining. Okay. The study generally focused on three categories on privacy. One, we call input privacy or you can think about data privacy. Essentially, how do we hide data? We can distort data, we can hide data to prevent data miners from reliable extracting confidential or private information. Another category of privacy is output privacy or we say, knowledge privacy, knowledge hiding problem. That means, where the input data may be okay, but once we find patterns, certain pattern or knowledge could be sensitive. You may not want to disclose it. So how we can guarantee? We do the mining, we can ensure the output privacy. Okay. This is another interesting issue. The third issue people studied called owner privacy. That means the people, any party may host certain data. For example, different hospitals may hold different kinds of patient information. You may want to study, for example, AIDS or heart problems or lung cancer or those problems. Okay. But you may not want to see where the data come from. That means, you actually want to hide the data owners or the hospitals hide the source of the data, then you can do more, you know, study. This we call owner privacy problem. So, in this short session, we are going to focus on how to ensure input privacy or you can say, data privacy problem. Okay. Of course, one simple approach could be you trust service provider. They are going to anonymize their data especially the use of private information. They're going to anonymize them and they are going to ship those anonymized data to data miners. This model we call business-to-business environment or service provider-to-data miner environment. That means you have to trust both parties. But the problems, do you really trust them? And most people may say, no. Okay. So that's the one, of course, this could be quite popular in, in the regular practice, but you want more. Okay. So the second approach we call data anonymization or perturbation. That means, we ensure data privacy and owners cited data source site. Usually, this anonymization even you may not trust the service providers. You actually, will try to ask a third party vendor to anonymize the data before they can release to anybody. Okay. This one we call business-to-customer environment. And the typical method develop, like a data perturbation, data transformation, data hiding. That means, we know that hides some sensitive value or sensitive attributes. Let's look at this table, this is a, suppose we have a hospital, it contains certain problem of, like the data is patient ID, you get zip code, you get age, you get the disease. People may not be comfortable on this kind of data, because even you anonymize patient name, but you may got to, some people may know the patient's age, and zip code. They can easily say, oh, this person got a heart problem. Okay. So then the message could be, what about we, we make it a little more generalized. We hide the last two digit, digits of zip code, then we hide the last digit of their age. That means you only know is add a zero, one, eight, something, zip code, and in the 40s, got a heart problem. But then you see for the same, you know zip code and same 40s, there are lots of other disease. So you will have no way to tell this person whether they got a heart problem, cancer, flu or anything else. So in that sense, you may feel more comfortable. If you got three records, you may still not feel very comfortable, but what about you get 100? You probably say that's okay. So this actually records k-anonymity problem. That means every equivalent class, that means they have the same zip code and h, they at least have case records. If this k is really large, you'll feel probably much more comfortable. Because within this 100 different disease, nobody can guess what you have. Okay. So, but this may not still be sufficient. For example, what about all these age, the, the zip code and in the thirties, everybody got a diabetes. Then, you know, then you agree to use it, then you can see you do get diabetes. Okay. So that means, this record even this k is not sufficient, you want ensure this disease is sensitive attribute got at least the same values, so we call this l diversity. But even this l diversity, what about the majority of people who have diabetes you still feel not the, not comfortable. Okay. So there are other studies, further studies for example t-closeness, differentially privacy. So there are lots of research I'm doing to see how to insure people have no way to guess what you have. Of course even for error diversity, if the majority supposed is HIV negative, probably this, this majority will not really have concern, because, you know, HIV negative is not a concern. Okay. So that's the thing the approach can see there are, there are many issues to study to ensure privacy in this way. Okay. So, I'll not get into detail, but there are many good research papers I list at the end some typical papers, you may like to go deeper by reading them. Now, I'm going to discuss a little data perturbation methods for Privacy-Preserving Pattern Mining. Data peturb, perturbation usually people use statistic approach. That means using some randomization algorithms to distort the data. One way you can do is we call independent attribute perturbation. That means, you take each attribute. You perturb the, the values independent of the other attributes. Okay. You don't have to think about their other attributes existing. Like you perturb the zip code or you perturbed the h. So, it doesn't matter. Okay. But for pattern mining, you will find the correlations. So sometimes, you may like to see dependent attribute perturbation, because, because the purer independent you do perturb these correlation may be lost. That means, if you want to take care of the correlation across attributes, your may concern to develop some dependent attribute perturbation method. Okay. And for this data perturbation, there are some interesting studies. One method called MASK. The, that method was if you think your shopping transaction contain lots of items. Okay. Usually, one transaction may only, that means one shopping basket may only contain ten or fifteen items. But if you go to Walmart, there are so many items, you can think all these items is a very long vector. Okay. In your own shopping basket, only small number of these factor turns to one. Okay. But you want to hide what exactly you, you, you are buying. Okay. So, in that sense, you may turn the other items from zero to one and some of your one items will turn from one to zero with certain probability. Okay. With this, if you're carefully tuning this p, you may achieve both good average privacy, because you insert a lot of randomly, particular items. They were will not figure out what you bought, but you still will get a good accuracy. The reason is in general those pattern, once you randomize a lot of different items they, with this randomization and likely, they will become frequent. Okay. So the pattern may real frequent pattern mix you up. But this method actually may turn, because of the majority of things are zeros. You may turn many things into one with certain probability, so that may increase your number of items in each transaction quite a lot, make your database really big. Okay. Either you can find patterns it could be substantially slow. There are stu, studies now how to, you know, ensure both, you know, efficiency and accuracy. Okay. Another method studied called cut and paste operator. This essentially is using uniform randomization. The method is you get a real transaction, you don't increase items in this real transaction. But you take the existing item, you have a probability to randomly replace some existing item with a new item. Okay. Not presenting your original transaction. So as long as you do not make your probability too big to replace them, you still get a good patterns show up. So but sometimes, if you replace too much, you go quite worst case. You know, on the accuracy or you will pay is too little, you may get worst case privacy. So their masters studied how to balance both. How to select items in an organized way. Okay. To improve the worst case privacy, but still keep good accuracy. Actually, the experiments show, if you want to mine, you know, short transactions, short patterns, short itemsets. Okay. So these cut and paste randomize the database you mine it. You may correctly identifies 80 to 90% short patterns, like lens, less or equal to three. But if you want to mine long patterns, because such perturbation, a lot of the long pattern may be destroyed. Okay. So how to effectively mine such long patterns, still remain good privacy. Its an open problem. Okay. So privacy preserving data mining is still a very active research field. There are lots of research papers published. For example, R Agrawal, they got the first paper published in year 2000 in Sigmod and there are books. And they, I, also intentionally put a privacy, differential privacy like t-closeness, l-diversity, those papers here, because I did not really lecture this to any detail. Okay. So interested reader, you may like to read those if you want, you are interested in, you know, further push forward the frontier of privacy preserving and data mining. Thank you. [MUSIC]
Now we come down to the very last session of this lecture which is also the very last session of this short course, Looking Forward. We have studied a lot of pattern discovery methods, some advanced concepts, some algorithms and lots of applications. We'll probably see there are lots of interesting issue to be studied from the first paper in this field, like 1993 up to now year 2015, we introduced various kinds of research discoveries and methods, even last until today. So this is still interesting research and application domain in data mining. So what we want to emphasize is how we further build up a new applications using invisible pattern mining approach. What is invisible pattern mining? It's you build this pattern mining measures into various kinds of functions, like a search function, ranking function, recommendation, or other mining functions. Sometimes when people search the web, the may not know they implicitly are using pattern mining results. For example, we haven't seen we're studying classification, there are so many classification methods. But definitely, frequent, and discriminative-pattern mining could be an interesting contribution to the different repositories of the methods on classification. Then we also studied a graph indexing and a similarity search. When you do indexing you do search, you do various kind of retrieval, you may use index constructive by pattern analysis. Then even for pattern mining we may first mine frequent patterns and doing topic modeling then we can get lots of interesting premises and features for effective text mining. When people doing software analysis, you may use pattern mining methods to discuss lots of bugs or software specification regularities and patterns, which may help us to find bugs in software. For spatiotemporal trajectory or moment pattern mining, you may see the frequent pattern analysis can be an interesting method added to this set of weapons. Then for image mining, multimedia data mining, like a video-audio analysis, the pattern analysis could be an implicit approach method built inside. Even for biological and chemical data analysis, like analyze DNA sequences or biological graphs, lots of pattern mining may be used as primitives. Even in clustering for example we are going to study clustering in data mining in this course. I'm going to introduce you subspace clustering, which actually frequent pattern mining will become another interesting technique for subspace clustering. Even people search the web or make a recommendation of your products, or study click streams to enhance your search. The pattern mining may play an important role in this. So, you probably can see pattern mining can be applied in many, many different domains. So, this invisible pattern mining could be very important taking this as a pre-processing of some component in your whole analysis package. So we are facing very big data, we're facing lots of challenges but the pattern mining surely is a tool. So I hope by learning this course, you will become part of research and development force to push pattern mining into new era on post research and applications. Hopefully in the future in your work, you may find using the material learned from this course. You're going to develop some interesting functions and push the technology into application into the new frontier. Thank you. [MUSIC]
