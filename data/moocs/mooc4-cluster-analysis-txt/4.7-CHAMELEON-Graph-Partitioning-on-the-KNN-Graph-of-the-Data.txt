[SOUND] Hi,
in this session we are going to introduce an interesting hierarchical clustering algorithm extension called CHAMELEON which is a graph partitioning on the KNN, means Kenya's neighbor graph of the data. So, this graph partitioning
based approach was developed in 1999 by a group of researchers
at the University of Minnesota. CHAMELEON actually has
some interesting points. It measures the similarity
based on a dynamic model. Essentially two clusters are merged. Only if the interconnectivity and
the closeness between the two clusters are high relative to their internal
interconnectivity of the clusters. And a closeness of items
within the clusters. CHAMELEON is a graph-based,
two-phase algorithm. In the first phase,
it uses a graph partitioning algorithm. Essentially, it cluster objects into a large number of relatively
small sub-clusters. Also, we call it graphlets. Then, in the second phase,
it uses an agglomerative hierarchical clustering algorithm
to find the genuine clusters by repeatedly combining these
sub-clusters or these graphlets. Let's look at the overall
framework of CHAMELEON. Suppose we have a large data set. Then we can use K-NN graph to merge
them into sparse graph like this. What is K-NN graph? Essentially we can see that two points,
P and Q, are connected. If Q is among the top k
closest neighbors of p, then based on this k nearest neighbor,
you construct this graph. Then we'll repartition this graph into a good number of smaller subclusters,
or called graphlets. After this partitioning, we are going to
merge them into high quality clusters. How we merge those graphlets
back to the bigger clusters? Essentially the,
the merge is based on these two measures. One we call relative interconnectivity,
another we call relative closeness. Relatively interconnectivity means
the connectivity of C sub 1 and C sub 2 over their respective
internal connectivity. Okay. Then the relative closeness is defined as
closeness between clusters C sub 1 and C sub 2 over their respective
internal closeness. Let's look at the,
the messert in some detail. The first is KNN graph. What is KNN graph? Let's look at the example. This is the original two
dimensional data set. So the first nearest neighbor,
one nearest neighbor graph is for each point, each you know, object. You just find the top
one closest neighbor. For example,
this ones closest neighbor could be here, this ones close neighbor could be here. Then you construct a subgraph
based on these you get a one, you get a one, one nearest neighbor graph. For two nearest neighbor graph is you,
for each point, you look their two, top two nearest
neighbor, construct a graph like this. Three nearest neighbor graph is for
each point, you look at their top three nearest neighbors in
a constructed graph like this. Okay? Then, we look at their absolute
interconnectivity between C sub i and C sub j. Okay. These absolute in,
interconnectivity is, essentially, the sum of the weight of the edges. That connects vertices in C
sub i to vertices in C sub j. Essentially you look at
the weighted sum of these edges. Then the internal connectivity
of the cluster C sub i essentially the size of
its min-cut bisector, the weighted sum of these edges partition
the graph into two roughly equal parts. Then you look at the weighted sum
of the edges of these partition. Then the relative interconnectivity,
RI, essentially is for C sub i and C sub j is defined by their ab,
absolute interconnectivity divided by or you can say normalized by the average
of their respective interconnectivity. Then we look at the relative closeness and
the merge of sub-clusters. Relative closeness between
a pair of clusters C sub i and C sub j is defined as follows. The first is what is absolute
closeness between C sub i and C sub j. The absolute closeness essentially is
the average weight of the edges connected these, the vertices between
these two clusters. Then their internal essentially
the internal closeness like this internal closeness of C sub
i internal closeness of C sub j, is the average weight of the edges
that belong to the min-cut bisector of clusters C sub i and
C sub j, respectively. That means you look at what
is min-cut bisector cut this cluster, or cutting this cluster. Then you'll find,
their average weights of the edges. Okay. Then you probably can see the relativeness
is, is the absolute closeness normalized with their inproportionally
defined internal closeness. Then how do we merge sub-clusters? Essentially is we will only
merge those pairs of clusters whose relative interconnectivity and relative closeness are both above
some user specified threshold. That means we only merge those maximizing
the function that combines relative interconnectivity and relative closeness. That means even we find
those pair are mergeable, we still want to find these merge,
we're maximizing the function. Then based on the implementation and
experiments we properly can see, CHAMELEON actually can cluster
complex objects like this. Okay. This actually, these are all the different
points with different density different shapes,
as well as lots of noise points. Even with different shapes,
different density and different shapes and different noises, we still can see CHAMELEON actually
generates pretty high quality clusters. Okay. That simply says CHAMELEON
using this graph partioning and emerging methods is pretty robust and
generate a pretty high quality clusters. Even it is in the spirit
of hierarchical clustering, it does do a lot of good with
this graph-based method. [MUSIC]